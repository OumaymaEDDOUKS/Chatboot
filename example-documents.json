[{"filename": "1.pdf", "page_number": 1, "content": "WINOGRANDE : An Adversarial Winograd Schema Challenge at Scale Keisuke Sakaguchi\u0003, Ronan Le Bras\u0003, Chandra Bhagavatula\u0003, Yejin Choi\u0003y \u0003Allen Institute for Arti\ufb01cial IntelligenceyUniversity of Washington fkeisukes, ronanlb, chandrab, yejinc g@allenai.org Abstract The Winograd Schema Challenge (WSC) (Levesque, Davis, and Morgenstern 2011), a benchmark for commonsense rea- soning, is a set of 273expert-crafted pronoun resolution prob- lems originally designed to be unsolvable for statistical models that rely on selectional preferences or word associations. How- ever, recent advances in neural language models have already reached around 90% accuracy on variants of WSC. This raises an important question whether these models have truly ac- quired robust commonsense capabilities or whether they rely on spurious biases in the datasets that lead to an overestimation of the true capabilities of machine commonsense. To investigate this question, we introduce WINOGRANDE , a large-scale dataset of"}, {"filename": "1.pdf", "page_number": 1, "content": "44k problems, inspired by the origi- nal WSC design, but adjusted to improve both the scale and the hardness of the dataset. The key steps of the dataset con- struction consist of (1) a carefully designed crowdsourcing procedure, followed by (2) systematic bias reduction using a novel AFLITEalgorithm that generalizes human-detectable word associations to machine-detectable embedding associ- ations . The best state-of-the-art methods on WINOGRANDE achieve 59.4 \u2013 79.1%, which are \u001815-35% (absolute) below human performance of 94.0%, depending on the amount of the training data allowed (2% \u2013 100% respectively). Furthermore, we establish new state-of-the-art results on \ufb01ve related benchmarks \u2014 WSC ( !90.1% ), DPR (!93.1% ), COPA(!90.6% ), KnowRef (!85.6% ), and Winogender (!97.1% ). These results have dual implications: on one hand, they demonstrate the effectiveness of WINOGRANDE when used as a resource for transfer learning. On the other hand, they raise a concern that we are likely to be"}, {"filename": "1.pdf", "page_number": 1, "content": "overestimating the true capabilities of machine commonsense across all these benchmarks. We emphasize the importance of algorithmic bias reduction in existing and future benchmarks to mitigate such overestimation. 1 Introduction The Winograd Schema Challenge (WSC) (Levesque, Davis, and Morgenstern 2011), proposed as an alternative to the Turing Test (Turing 1950), has been used as a benchmark for evaluating commonsense reasoning. WSC are designed to be pronoun resolution problems (see examples in Table 1) that are trivial for humans but hard for machines that merelyrely on statistical patterns without true capabilities of com- monsense reasoning. However, recent advances in neural language models have already reported around 90% accu- racy on a variant of WSC dataset.1This raises an important question: Have neural language models successfully acquired com- monsense or are we overestimating the true capabilities of machine commonsense ? This question about the potential overestimation"}, {"filename": "1.pdf", "page_number": 1, "content": "leads to an- other crucial question regarding potential unwanted biases that the large-scale neural language models might be exploit- ing, essentially solving the problems right , but for wrong rea- sons. While WSC questions are expert-crafted, recent studies have shown that they are nevertheless prone to incidental bi- ases. Trichelair et al .(2018) have reported word-association (13.5% of the cases, see Table 1 for examples) as well as other types of dataset-speci\ufb01c biases. While such biases and annotation artifacts are not apparent for individual instances, they get introduced in the dataset as problem authors subcon- sciously repeat similar problem-crafting strategies. To investigate this question about the true estimation of the machine commonsense capabilities, we introduce WINO- GRANDE , a new dataset with 44k problems that are inspired by the original design of WSC, but modi\ufb01ed to improve both the scale and hardness of the problems. The key steps in WINOGRANDE construction"}, {"filename": "1.pdf", "page_number": 1, "content": "consist of (1) a carefully de- signed crowdsourcing procedure, followed by (2) a novel algorithm AFLITEthat generalizes human-detectable biases based on word occurrences to machine-detectable biases based on embedding occurrences. The key motivation of our approach is that it is dif\ufb01cult for humans to write problems without accidentally inserting unwanted biases. While humans \ufb01nd WINOGRANDE problems trivial with 94% accuracy, best state-of-the-art results, including those from RoBERTa (Liu et al .2019) are considerably lower, ranging between 59.4% - 79.1% depending on the amount of training data provided (from 800 to 41k instances), which falls 15 - 35% (absolute) below the human-level performance. 1https://github.com/pytorch/fairseq/tree/master/examples/ roberta. We note that this variant aggregates the original WSC, PDP (Morgenstern, Davis, and Ortiz 2016) and additional PDP-style examples, and recasts them into True/False binary problems.arXiv:1907.10641v2  [cs.CL]  21 Nov 2019"}, {"filename": "1.pdf", "page_number": 2, "content": "Twin sentences Options ( answer ) 3(1)a The trophy doesn\u2019t \ufb01t into the brown suitcase because it\u2019s too large . trophy / suitcase b The trophy doesn\u2019t \ufb01t into the brown suitcase because it\u2019s too small . trophy / suitcase 3(2)a Ann asked Mary what time the library closes, because shehad forgotten. Ann / Mary b Ann asked Mary what time the library closes, butshehad forgotten. Ann / Mary 7(3)a The tree fell down and crashed through the roof of my house. Now, I have to get itremoved .tree / roof b The tree fell down and crashed through the roof of my house. Now, I have to get itrepaired . tree / roof 7(4)a The lions ate the zebras because they arepredators . lions / zebras b The lions ate the zebras because they aremeaty . lions / zebras Table 1: WSC problems are constructed as pairs (called twin) of nearly identical questions with two answer choices. The questions include a trigger word that \ufb02ips the correct answer choice between the questions. Examples (1)-(3) are drawn from WSC"}, {"filename": "1.pdf", "page_number": 2, "content": "(Levesque, Davis, and Morgenstern 2011) and (4) from DPR (Rahman and Ng 2012)). Examples marked with 7have language-based bias that current language models can easily detect. Example (4) is undesirable since the word \u201cpredators\u201d is more often associated with the word \u201clions\u201d, compared to \u201czebras\u201d Furthermore, we also demonstrate that WINOGRANDE provides transfer learning to other existing WSC and related benchmarks, achieving new SOTA performances on \ufb01veof them, including the original WSC (Levesque, Davis, and Morgenstern 2011) ( !90.1% ), DPR (Rahman and Ng 2012) (!93.1% ), COPA (Roemmele, Bejan, and Gordon 2011) (!90.6% ), KnowRef (Emami et al .2019) (!85.6% ), and Winogender (Rudinger et al. 2018) ( !97.1% ). Although the improvements of SOTA over multiple chal- lenging benchmarks are exciting, we cautiously note that these positive results must be taken with a grain of salt. The result might also indicate the extent to which spurious ef- fects are prevalent in existing datasets,"}, {"filename": "1.pdf", "page_number": 2, "content": "which runs the risk of overestimating the true capabilities of machine intelligence on commonsense reasoning. More generally, human-crafted problems and tasks (regardless of whether they are crowd- sourced or by experts) contains annotation artifacts in many cases, and algorithmic bias reduction such as AFLITEis essential to mitigate such dataset-speci\ufb01c bias. 2 Crowdsourcing W INOGRANDE at Scale WSC problems have been considered challenging to craft by crowdsourcing due to the structural constraints of twins and the requirement of linguistic knowledge (Table 1). Neverthe- less, we present an effective approach to creating a large-scale dataset ( WINOGRANDE ) of WSC problems while maintain- ing its original properties \u2013 i.e. trivial for humans but hard for AI systems. Our approach consists of a carefully designed crowdsourcing task followed by a novel adversarial \ufb01ltering algorithm (x3) that systematically removes biases in the data. Enhancing Crowd Creativity Creating twin sentences"}, {"filename": "1.pdf", "page_number": 2, "content": "from scratch puts a high cognitive load on crowd workers who thereby subconsciously resort to writing pairs that are lexically and stylistically repetitive. To encourage creativity and reduce their cognitive load, we employed creativity from constraints (Stokes 2005) \u2013 a psychological notion which suggests that appropriate constraints can help structure and drive creativity. In practice, crowd workers are primed by a randomly chosen topic as a suggestive context (details be- low), while they are asked to follow precise guidelines on the structure of the curated data.Crowdsourcing Task We collect WINOGRANDE problems via crowdsourcing on Amazon Mechanical Turk (AMT).2 Workers are asked to write twins sentences (as shown in Ta- ble 1) that meet the requirements for WSC problems (e.g., avoiding word association, non-zero but small edit distance). To avoid repeating the same topics, workers were instructed to randomly pick an anchor word(s) from a randomly as- signed WikiHow article3and to"}, {"filename": "1.pdf", "page_number": 2, "content": "ensure that the twin sen- tences contain the anchor word. The anchor word does not have to be a trigger word, but we ensured that it is not a function word such as the, it, he, of . In our pilot experiments, we found that this constraint drastically improves worker\u2019s creativity and diversity of topics. Additionally, workers were instructed to keep twin sentence length in between 15and30 words while maintaining at least 70% word overlap between a pair of twins.4Following the original WSC problems, we aimed to collect twins in two different domains \u2013 (i) social commonsense: a situation involving two same gender people with contrasting attributes, emotions, social roles, etc., and (ii) physical commonsense: a context involving two physical objects with contrasting properties, usage, locations, etc. In total, we collected 77k questions (i.e., 38k twins). Data Validation We validate each collected question through a distinct set of three crowd workers. Following a rig- orous process, a"}, {"filename": "1.pdf", "page_number": 2, "content": "question is deemed valid if (1) the majority of the three workers chooses the correct answer option, (2) they agree that the two answer options are unambiguous (one option is clearly more plausible than the other) and (3) the question cannot be answered simply by word association in which local context around the target pronoun is given (e.g., \u201cbecause itwas going so fast.\u201d ( race car / school bus)).5As a result, 68% of the questions (53k) were deemed valid and we discarded the invalid questions. While our crowdsourcing procedure addresses some amount of instance-level biases like word association, it is 2Our datasets, crowdsourcing interface, and models are available at http://winogrande.allenai.org. 3https://www.wikihow.com/Special:Randomizer 4The workers met minimum quali\ufb01cation in AMT: 99% ap- proval rate, 5kapprovals. The reward was $0:4per twin sentences. 5For each sentence validation, workers were paid $0.03."}, {"filename": "1.pdf", "page_number": 3, "content": "still possible that the constructed dataset has dataset-speci\ufb01c biases \u2013 especially after it has been scaled up. To address this challenge, we propose a method for systematic bias reduc- tion. 3 Algorithmic Data Bias Reduction Several recent studies (Gururangan et al .2018; Poliak et al . 2018; Tsuchiya 2018; Niven and Kao 2019; Geva, Goldberg, and Berant 2019) have reported the presence of annotation artifacts in large-scale datasets. Annotation artifacts are unin- tentional patterns in the data that leak information about the target label in an undesired way. State-of-the-art neural mod- els are highly effective at exploiting such artifacts to solve problems correctly , but for incorrect reasons. To tackle this persistent challenge with dataset biases, we propose AFLITE\u2013 a novel algorithm that can systematically reduce biases using state-of-the-art contextual representation of words. Light-weight adversarial \ufb01ltering Our approach builds upon the adversarial \ufb01ltering (AF) algorithm"}, {"filename": "1.pdf", "page_number": 3, "content": "proposed by Zellers et al .(2018), but makes two key improvements: (1) AFLITEis much more broadly applicable (by not requiring over generation of data instances) and (2) it is considerably more lightweight (not requiring re-training a model at each it- eration of AF). Overgenerating machine text from a language model to use in test instances runs the risk of distributional bias where a discriminator can learn to distinguish between machine generated instances and human-generated ones. In addition, AF depends on training a model at each iteration, which comes at extremely high computation cost when being adversarial to a model like BERT (Devlin et al. 2018).6 Instead of manually identi\ufb01ed lexical features, we adopt a dense representation of instances using their pre- computed neural network embeddings. In this work, we use RoBERTa (Liu et al .2019) \ufb01ne-tuned on a small subset of the dataset. Concretely, we use 6k instances (5k for training and 1k for validation) from the dataset"}, {"filename": "1.pdf", "page_number": 3, "content": "(containing 53k instances in total) to \ufb01ne-tune RoBERTa (referred to as RoBERTa embed ). We use RoBERTa embed to pre-compute the embeddings for the rest of the instances (47k) as the input for AFLITE. We discard the 6k instances from the \ufb01nal dataset. Next, we use an ensemble of linear classi\ufb01ers (logistic re- gressions) trained on random subsets of the data to determine whether the representation used in RoBERTa embed is strongly indicative of the correct answer option. If so, we discard the corresponding instances and proceed iteratively. Algorithm 1 provides the implementation of AFLITE. The algorithm takes as input the pre-computed embeddings X and labels y, along with the size nof the ensemble, the train- ing sizemfor the classi\ufb01ers in the ensemble, the size kof the \ufb01ltering cutoff, and the \ufb01ltering threshold \u001c. At each \ufb01ltering phase, we train nlinear classi\ufb01ers on different ran- dom partitions of the data and we collect their predictions 6AFLITEis designed for \ufb01ltering instances"}, {"filename": "1.pdf", "page_number": 3, "content": "so that the resulting dataset is less biased, whereas the original AF algorithm (Zellers et al.2018) is designed for \u201cgenerating and modifying\u201d individual instances, such as by creating better distractors. AFLITEand AF are therefore different in their goals and hence dif\ufb01cult to compare directly.Algorithm 1: AFLITE Input: datasetD= (X;y), ensemble size n, training set size m, cutoff sizek, \ufb01ltering threshold \u001c Output: datasetD0 1D0=D 2whilejD0j>m do // Filtering phase 3 foralle2D0do 4 Initialize the ensemble predictions E(e) =; 5 foriterationi: 1::ndo 6 Random partition (Ti;Vi) ofD0s.t.jTij=m 7 Train a linear classi\ufb01er LonTi 8 foralle= (x;y)2Vido 9 AddL(x)toE(e) 10 foralle= (x;y)2D0do 11score (e) =jfp2E(e)s.t.p=ygj jE(e)j 12 Select the top- kelementsSinD0s.t.score (e)\u0015\u001c 13D0=D0nS 14 ifjSj<k then 15 break 16returnD0 on their corresponding validation set. For each instance, we compute its score as the ratio of correct predictions over the total number of predictions. We rank the"}, {"filename": "1.pdf", "page_number": 3, "content": "instances ac- cording to their score and remove the top- kinstances whose score is above threshold \u001c. We repeat this process until we remove fewer than kinstances in a \ufb01ltering phase or there are fewer thanmremaining instances. When applying AFLITE toWINOGRANDE , we setm= 10;000,n= 64 ,k= 500 , and\u001c= 0:75. This approach is also reminiscent of recent work in NLP on adversarial learning (Chen and Cardie 2018; Belinkov et al.2019; Elazar and Goldberg 2018). Belinkov et al .(2019) propose an adversarial removal technique for NLI which encourages models to learn representations that are free of hypothesis-only biases. When proposing a new benchmark, however, we cannot enforce that any future model will pur- posefully avoid learning spurious correlations in the data. In addition, while the hypothesis-only bias is an insightful bias in NLI, we make no assumption about the possible sources of bias in WINOGRANDE . Instead, we adopt a more proactive form of bias reduction by relying on state-of-"}, {"filename": "1.pdf", "page_number": 3, "content": "the-art (statisti- cal) methods to uncover undesirable dataset shortcuts. Assessment of A FLITE We assess the impact of AFLITE relative to two baselines: random data reduction and PMI- based \ufb01ltering. In random data reduction, we randomly sub- sample the dataset to evaluate how a decrease in dataset size affects the bias. In PMI-based \ufb01ltering, we compute the dif- ference (f) of PMIs for each twin ( t) as follows: f(t1;t2) =X w2t1PMI(y= 1;w)\u0000X w2t2PMI(y= 1;w): Technically, we \ufb01rst pre-computed PMI between a word and the labely= 1 for each word in the dataset, following a method proposed by Gururangan et al .(2018). The sum of PMI value of each token in a given sentence indicates the"}, {"filename": "1.pdf", "page_number": 4, "content": "d1d2WinoGrandeall(44k) d1d2Random (12k) d1d2PMI-\ufb01ltering (12k) d1d2WinoGrandedebiased(12k) d101000200030004000KL-divergence=2.53 d102505007501000KL-divergence=2.51 d102505007501000 KL-divergence=2.42 d1050100150KL-divergence=0.12Figure 1: The effect of debiasing by AFLITE. RoBERTa pre-computed embeddings (applied PCA for dimension reduction) are shown in two-dimensional space ( top row ) and histograms regarding d1(bottom row ) with the bin size being 100. Data points are colored depending on the label (i.e., the answer yis option 1 (blue) or 2 (red)). In the histograms, we show the KL-divergence betweenp(d1;y=1)andq(d1;y=2). Twin sentences Options ( answer ) 7The monkey loved to play with the balls but ignored the blocks because he found them exciting. balls / blocks The monkey loved to play with the balls but ignored the blocks because he found them dull. balls / blocks 7William could only climb begginner walls while Jason climbed advanced ones because hewas very weak . William /"}, {"filename": "1.pdf", "page_number": 4, "content": "Jason William could only climb begginner walls while Jason climbed advanced ones because hewas very strong . William / Jason 3Robert woke up at 9:00am while Samuel woke up at 6:00am, so hehadlesstime to get ready for school. Robert / Samuel Robert woke up at 9:00am while Samuel woke up at 6:00am, so hehadmore time to get ready for school. Robert / Samuel 3The child was screaming after the baby bottle and toy fell. Since the child was hungry ,itstopped his crying. baby bottle / toy The child was screaming after the baby bottle and toy fell. Since the child was full,itstopped his crying. baby bottle / toy Table 2: Examples that have dataset-speci\ufb01c bias detected by AFLITE(marked with 7). The words that include (dataset-speci\ufb01c) polarity bias (x3) are highlighted ( positive and negative). For comparison, we show examples selected from WINOGRANDE debiased (marked with 3). likelihood of the label y= 1for the sentence. We only retain twins that have a small difference in their PMI values as"}, {"filename": "1.pdf", "page_number": 4, "content": "it corresponds to twins that are hard to discriminate.7 Figure 1 plots RoBERTa pre-computed embeddings whose dimension is reduced to 2D ( top) and 1D ( bottom ) using Prin- cipal Component Analysis (PCA). We observe that WINO- GRANDE alland the two baselines exhibit distinct compo- nents between the two correct answer options (i.e., y21;2), whereas such distinction becomes less salient in WINO- GRANDE debiased , which implies that AFLITEsuccessfully reduces the spurious correlation in the dataset (between in- stances and labels). To quantify the effect, we compute the KL divergence between the samples with answer options. We \ufb01nd that the random data reduction does not reduce the KL diver- 7We also evaluated other variations of PMI- \ufb01ltering such as the absolute difference ( jfj), max-PMI (max(max w2t1PMI(y;w);max w2t2PMI(y;w))), and token- pairwised PMI( y;w1;w22t), but we did not observe a signi\ufb01cant difference among them.gence ( 2:53!2:51). It is interesting to see that PMI-\ufb01ltering"}, {"filename": "1.pdf", "page_number": 4, "content": "marginally reduces the KL divergence ( !2:42), although the principal component analysis on the PMI-\ufb01ltered subset still leads to a signi\ufb01cant separation between the labels. On the other hand, in WINOGRANDE debiased ,AFLITEreduces the KL divergence dramatically ( !0:12) which suggests that this debiased dataset should be challenging for statistical models that solely rely on spurious correlation. What bias has been actually detected by A FLITE?Is the bias really spurious and undesirable according to the original WSC\u2019s goal? Table 2 presents examples that AFLITEhas detected as a dataset-speci\ufb01c bias. We see a structural pattern in the \ufb01rst two twins, where the sentiment between the answer option and the target pronoun are highly correlated. In other words, these problems can be easily answered by simply exploiting the pattern of the polarity (positive or negative). Importantly, this dataset-speci\ufb01c bias is structural rather than at the token level, contrasting with the biases that have"}, {"filename": "1.pdf", "page_number": 4, "content": "been"}, {"filename": "1.pdf", "page_number": 5, "content": "identi\ufb01ed in the NLI literature (Gururangan et al .2018; Poliak et al .2018), and it is hard to detect these biases using heuristics such as lexical PMI-\ufb01ltering. Instead of depending on such heuristics, AFLITEis able to detect samples that potentially have such biases algorithmically. After applying the AFLITEalgorithm, we obtain a debi- ased dataset of 12,282 instances split into training (9,248), development (1,267), and test (1,767) sets. We also release 31k problems that are \ufb01ltered out by AFLITEfor additional training set (x4) and resource (x5), resulting in a total number of problems in WINOGRANDE allto be 43,972 (40,938 for training, 1,267 for development, and 1,767 for test). 3.1 W INOGRANDE V .S. the Original WSC While WINOGRANDE is inspired by the original WSC, we make a few design choices that deviate from the original design guidelines of WSC in order to scale up the dataset considerably while ensuring the hardness of the dataset. First, WINOGRANDE is formatted as a \ufb01ll-"}, {"filename": "1.pdf", "page_number": 5, "content": "in-the-blank problem where the blank corresponds to the mention of one of the two names in the context, following the same modi\ufb01cation made by other recent WSC variants such as Trinh and Le (2018).8In contrast, the original WSC explicitly places a pronoun (instead of a blank). From the modeling stand point, the use of blanks instead of explicit pronouns do not make the problem any easier. Second, while we originally collected all problems in twins, the \ufb01nal questions in the \ufb01ltered WINOGRANDE debiased are not always twins because it is possible that AFLITE\ufb01lters out only one of the twin sentences. In WINOGRANDE debiased , about 1/3 of questions are not twins. We also release WINO- GRANDE all(training set) that all consists of twins. Third, unlike the original WSC problems that were com- posed by just a few linguistics experts, WINOGRANDE is authored by crowdworkers. Thus, the language used in WINO- GRANDE re\ufb02ects the more diverse and noisy language used by crowds. Importantly, laymen"}, {"filename": "1.pdf", "page_number": 5, "content": "still \ufb01nd WINOGRANDE problems easy to solve, with 94% accuracy ( x4). 4 Experimental Results 4.1 Baseline Models We evaluate the WINOGRANDE debiased (dev and test) on meth- ods/models that have been effective on the original WSC. Wino Knowledge Hunting Wino Knowledge Hunting (WKH) by Emami et al .(2018) is based on an informa- tion retrieval approach, where the sentence is parsed into a set of queries and then the model looks for evidence for each answer candidate from the search result snippets. This IR-oriented approach comes from a line of work in corefer- ence resolution (Kobdani et al .2011; Ratinov and Roth 2012; Bansal and Klein 2012; Zheng et al .2013; Peng, Khashabi, and Roth 2015). Ensemble Neural LMs Trinh and Le (2018) is one of the \ufb01rst attempts to apply a neural language model which is 8https://github.com/tensor\ufb02ow/models/tree/master/research/ lmcommonsensepre-trained on a very large corpora (including LM-1-Billion, CommonCrawl, SQuAD, and Gutenberg Books). In this ap-"}, {"filename": "1.pdf", "page_number": 5, "content": "proach, the task is treated as \ufb01ll-in-the-blank question with binary choice. The target pronoun in the sentence is replaced by each answer candidate and the neural language model provides the likelihood of the two resulting sentences. This simple yet effective approach outperforms previous IR-based methods. BERT BERT (Devlin et al .2018) is another pre-trained neural language model which has bidirectional paths and consecutive sentence representations in hidden layers. We \ufb01netune BERT with splitting the input sentence into context and option using the candidate answer as delimiter. The in- put format becomes [CLS] context [SEP] option [SEP] ; e.g., The trophy doesn\u2019t \ufb01t into the brown suitcase because the [SEP] is too large. [SEP] (The blank is \ufb01lled with either option 1 or 2), and the [CLS] token em- bedding is used to classify which answer option is correct. We used grid-search for hyper-parameter tuning: learning ratef1e\u00005;3e\u00005;5e\u00005g, number of epochs f3;4;5;8g, batch-sizef8;16gwith"}, {"filename": "1.pdf", "page_number": 5, "content": "three different random seeds. RoBERTa RoBERTa (Liu et al .2019) is an improved vari- ant of BERT that adds more training data with larger batch sizes and training time, as well as other re\ufb01nements such as dynamic masking. RoBERTa performs consistently better than BERT across many benchmark datasets. Word association baseline Using BERT and RoBERTa, we also run the word association baseline ( local-context- only) to check if the dataset can be solved by language-based bias. In this baseline, the model is trained with only local contexts (wt\u00002:EOS) surrounding the blank to be \ufb01lled ( wt) (e.g., because the [SEP] is too large. [SEP ]). This is analogous to the hypothesis-only baseline in NLI (Poliak et al.2018), where the task (dataset) does not require the full context to achieve high performance. Finetuning on DPR dataset DPR (De\ufb01nite Pronoun Reso- lusiton Dataset), collected by Rahman and Ng (2012), con- sists of 1,886 WSC style problems written by 30 undergradu- ate students. Kocijan"}, {"filename": "1.pdf", "page_number": 5, "content": "et al .(2019) have recently shown that BERT \ufb01netuned with DPR boosts the performance on WCS (72.2% accuracy). As additional baselines, we \ufb01netune BERT and RoBERTa with DPR and evaluate on WINOGRANDE . This allows us to compare the dif\ufb01culty of WSC and WINO- GRANDE empirically. Human evaluation In addition to the methods described above, we compute human performance as the majority vote of three crowd workers for each question. 4.2 Results Table 3 shows the results. Two baselines, WKH and En- semble LMs, only achieve chance-level performance (50%). The best model, RoBERTa, achieves 79:1%test-set accu- racy9, whereas human performance achieve 94:0%, indicat- ing that the WINOGRANDE debiased is still easy for humans to 9When we use the debiased training set (9,248), both BERT and RoBERTa showed only chance level performance."}, {"filename": "1.pdf", "page_number": 6, "content": "Methods dev acc. (%) test acc.(%) WKH 49.4 49.6 Ensemble LMs 53.0 50.9 BERT 65.8 64.9 RoBERTa 79.3 79.1 BERT (local context) 52.5 51.9 RoBERTa (local context) 52.1 50.0 BERT-DPR?50.2 51.0 RoBERTa-DPR?59.4 58.9 Human Perf. 94.1 94.0 Table 3: Performance of several baseline systems on WINO- GRANDE debiased (dev and test). The star ( ?) denotes that it is zero-shot setting (e.g., BERT-DPR?is a BERT model \ufb01ne-tuned with the DPR dataset and evaluated on WINO- GRANDE debiased .) Training size dev acc. (%) test acc.(%) XS (160) 51.5 50.4 S (640) 58.6 58.6 M (2,558) 66.9 67.6 L (10,234) 75.8 74.7 XL (40,938) 79.3 79.1 Table 4: Performance of RoBERTa with different training sizes. answer as desired. Regarding the word association (i.e., local context) baselines, both BERT and RoBERTa achieve close to chance-level performance, illustrating that most WINO- GRANDE debiased problems cannot be answered by local con- text only. Finally, BERT and RoBERTa \ufb01netuned with DPR achieve chance-level to below"}, {"filename": "1.pdf", "page_number": 6, "content": "60% accuracy, which con- trast with the performance boosts on WSC (72% by BERT (Kocijan et al .(2019)) and 83% in RoBERTa) and other exist- ing WSC-style problems (shown in x5.3). This indicates that WINOGRANDE debiased consists of more challenging problems than WSC and existing variants. Learning Curve In order to see the effect of training size, Table 4 shows the performance by RoBERTa trained on dif- ferent training sizes from 160 to 40k questions. Figure 2 shows the learning curve of the best model, RoBERTa, on theWINOGRANDE debiased dev set. RoBERTa\u2019s performance ranges from 59% to 79% when the size of training data is varied from 800 (2% of the training data) to 41K (100% of the training data) instances. To achieve human-level perfor- mance, current state-of-the-art models would need over 118K training instances. Importantly, the lower end of the available training data (\u0018800) in the learning curve roughly matches the size of the training data made available in previous variants"}, {"filename": "1.pdf", "page_number": 6, "content": "of WSC (see Table 5). For most of these datasets, state-of-the-art already reaches around 90% ( x5). In contrast, when we control for the training set size in WINOGRANDE , RoBERTa\u2019s performance is considerably lower (59%) \u2013 demonstrating that our dataset Accuracy on Dev Set (%)405060708090100 No. of Training Examples100100010000100000 y = 4.8463ln(x) + 26.21594Figure 2: Learning curve on the dev set of WINOGRANDE . Each point on the plot is the best performance for a given number of randomly selected training examples, computed over ten random seeds. construction method is able to compose WSC problems that are collectively considerably harder than previous datasets. 5 Transfer Learning from W INOGRANDE WINOGRANDE contains a large number of WSC style ques- tions. In addition to serving as a benchmark dataset, we use WINOGRANDE as a resource \u2013 we apply transfer learning by \ufb01rst \ufb01ne-tuning a model on our dataset and evaluating its per- formance on related datasets: WSC, PDP, SuperGLUE-"}, {"filename": "1.pdf", "page_number": 6, "content": "WSC, DPR, KnowRef, KnowRef, and Winogender). We establish state-of-the-art results across several of these existing bench- mark datasets. 5.1 Existing WSC and Related Datasets We brie\ufb02y describe existing WSC variants and other related datasets. Table 5 provides their summary statistics. WSC (Levesque, Davis, and Morgenstern 2011) This is the original Winograd Schema Challenge dataset, which con- sists of 273 problems. The problems are manually crafted by the authors to avoid word association bias as much as possible, although Trichelair et al .(2018) later report that 13.5% of the questions may still have word-association bias. PDP (Morgenstern, Davis, and Ortiz 2016) PDP (Pro- noun Disambiguation Problems) dataset is closely related to the original WSC, and used in the 2016 running of the Winograd Schema Challenge. The dataset consists of 80pro- noun disambiguation problems. It is formulated as a multiple choice task, in which a pronoun must be resolved to one of up to 5(but mostly"}, {"filename": "1.pdf", "page_number": 6, "content": "binary) possible antecedents. SuperGLUE-WSC (Wang et al. 2019) SuperGLUE con- tains multiple datasets including a modi\ufb01ed version of WSC, which we will refer to as SuperGLUE-WSC. This dataset aggregates the original WSC, PDP and additional PDP-style examples, and recasts them into True/False binary problems (e.g., \u201cPete envies Martin because heis very successful.\u201d Q:"}, {"filename": "1.pdf", "page_number": 7, "content": "Dataset #Probs Avg Len #V ocab WSC 273 19.1 919 PDP 80 39.5 594 SuperGLUE-WSC 804 28.4 1,711 DPR 1,886 15.9 4,127 KnowRef 1,269 19.3 5,310 COPA 1,000 13.3 3,369 Winogender 720 15.6 523 WINOGRANDE debiased 12,282 21.1 11,408 WINOGRANDE all 43,972 20.6 16,469 Table 5: Statistics on WSC and related datasets ( x5.1). Does herefer to Martin ? A: True). Therefore, the number of problems are roughly doubled from WSC and PDP, although the size is still relatively small (804 in total). We converted WinoGrande to the True/False binary problems. DPR (Rahman and Ng 2012) DPR (De\ufb01nite Pronoun Resolution Dataset) introduces 1,886 additional WSC prob- lems authored by 30 undergraduate students. Trichelair et al . (2018) point out that this dataset is overall less challenging than the original WSC due to an increased level of language- based or dataset-speci\ufb01c biases. We split the original training set (1,332) into training (1,200) and development (122) sets, DPR does not have an of\ufb01cial split for it."}, {"filename": "1.pdf", "page_number": 7, "content": "KnowRef (Emami et al. 2019) KnowRef provides over 8k WSC-style coreference resolution problems that are ex- tracted and \ufb01ltered with heuristic rules from 100 million web sentences (Reddit, Wikipedia, and OpenSubtitles). We report results on the publicly available testset (1.2k problems). COPA (Roemmele, Bejan, and Gordon 2011) This dataset introduces 1,000 problems that aim to test com- monsense reasoning focusing on script knowledge, formu- lated as a binary choice about causes andeffects of given premises. Since COPA does not provide a training set, we split the original development set (500) into training (400) and development (100) sets in the same way as SuperGLUE- COPA (Wang et al. 2019). Winogender (Rudinger et al. 2018) This dataset intro- duces 720problems focusing on pronouns resolution with respect to people, with distinct goal of measuring gender bias in coreference resolution systems. 5.2 Experimental Setup Our model is based on RoBERTa \ufb01netuned with WINO- GRANDE (train"}, {"filename": "1.pdf", "page_number": 7, "content": "and dev sets). To compare different corpora used as a resource, we also \ufb01netune RoBERTa on DPR (train and test sets). For hyper parameter search, we use the same grid search strategy as in x4. Additional Human Evaluation We also report human per- formance for WSC, PDP, and DPR to calibrate the quality of our crowd worker pool as well as to support previous \ufb01nd- ings. To our knowledge, this is the \ufb01rst work to report human performance on the DPR dataset.WSC (Levesque, Davis, and Morgenstern 2011) Liu et al. (2016) 52.8 WKH (Emami et al. 2018) 57.1 Ensemble LMs (Trinh and Le 2018) 63.8 GPT2 (Radford et al. 2019) 70.7 BERT-DPR?(Kocijan et al. 2019) 72.2 HNN (He et al. 2019) 75.1y RoBERTa-DPR?(This work) 83.1 RoBERTa-WinoGrande?(This work) 90.1 Humans (Bender 2015) 92.1 Humans (This work) 96.5 PDP (Morgenstern, Davis, and Ortiz 2016) Liu et al. (2016) 61.7 Trinh and Le (2018) 70.0 RoBERTa-DPR?(This work) 86.3 RoBERTa-WinoGrande?(This work) 87.5 HNN (He et al. 2019) 90.0y Humans (Davis,"}, {"filename": "1.pdf", "page_number": 7, "content": "Morgenstern, and Ortiz 2016) 90.9 Humans (This work) 92.5 SuperGLUE-WSC (Wang et al. 2019) Majority baseline 65.1 RoBERTa-DPR-ft (This work) 83.6 RoBERTa-WinoGrande-ft (This work) 85.6 RoBERTa-ensemble (Liu et al. 2019) 89.0 Humans (Wang et al. 2019) 100 DPR (Rahman and Ng 2012) Rahman and Ng (2012) 73.0 Peng, Khashabi, and Roth (2015) 76.4 BERT-WinoGrande?(This work) 84.9 RoBERTa-ft (This work) 91.7 RoBERTa-WinoGrande?(This work) 92.5 RoBERTa-WinoGrande-ft (This work) 93.1 Humans (This work) 95.2 KnowRef (Emami et al. 2019) Emami et al. (2019) 65.0 RoBERTa-DPR?(This work) 84.2 RoBERTa-WinoGrande?(This work) 85.6 Humans (Emami et al. 2019) 92.0 COPA (Roemmele, Bejan, and Gordon 2011) Gordon, Bejan, and Sagae (2011) 65.4 Sasaki et al. (2017) 76.4 RoBERTa-WinoGrande?(This work) 84.4 RoBERTa-ft (This work) 86.4z RoBERTa-WinoGrande-ft (This work) 90.6 Humans (Gordon, Kozareva, and Roemmele 2012) 99.0 Table 6: Accuracy ( %) on existing WSC-related tasks (test set). The star ( ?) denotes"}, {"filename": "1.pdf", "page_number": 7, "content": "that it is zero-shot setting. \u2018-ft\u2019 indicates \ufb01ne-tuning on the targeted dataset (train and dev). RoBERTa-X-ft denotes sequential \ufb01ne-tuning with dataset X followed by the targeted dataset. The daggers ( y) indicate that the evaluation data is not exactly the same from ours. The double dagger ( z) denotes that we could not reproduce the same number as in SuperGLUE leaderboard (Wang et al . 2019)."}, {"filename": "1.pdf", "page_number": 8, "content": "Winogender (Rudinger et al. 2018) Gotcha Female Male j\u0001Fj j\u0001Mj RULENo 38.3 51.728.3 14.2Yes 10.0 37.5 STATSNo 50.8 61.75.0 21.7Yes 45.8 40.0 NEURALNo 50.8 49.214.1 2.5Yes 36.7 46.7 RoBERTa-DPR No 98.3 96.71.6 0.9(This work) Yes 96.7 95.8 RoBERTa-WG No 97.5 96.70.8 0.8(This work) Yes 96.7 97.5 Table 7: Accuracy ( %) and gender bias on Winogender dataset. \u201cGotcha\u201d indicates whether the target gender pro- noun (e.g., she) is minority in the correct answer option (e.g., doctor).j\u0001Fjandj\u0001Mjshow the system performance gap be- tween \u201cGotcha\u201d and \u201cnon-Gotcha\u201d for each gender (lower the better). The \ufb01rst three baselines are adopted from Rudinger et al.(2018); RULE is Lee et al .(2011), STATS is Durrett and Klein (2013), and N EURAL is Clark and Manning (2016). 5.3 Experimental Results Tables 6 and 7 show results of applying transfer learning from WINOGRANDE to other WSC variants. Overall, RoBERTa \ufb01ne-tuned on WINOGRANDE helps improve the accuracy on all the related tasks (Table 6), and performs"}, {"filename": "1.pdf", "page_number": 8, "content": "consistently better than when RoBERTa is \ufb01ne-tuned on DPR. While improvements on some related datasets (particularly WSC, PDP, and DPR) might seem expected, the signi\ufb01cant improvement on COPA is not so. The COPA task \u2013 identify- ing causes and effects \u2013 is very different from that in WINO- GRANDE . This signi\ufb01cant improvement on an unrelated task indicates that WINOGRANDE can serve as a resource for commonsense knowledge transfer. Important Implications We consider that while these pos- itive results over multiple challenging benchmarks are highly encouraging, they may need to be taken with a grain of salt. In particular, these results might also indicate the extent to which spurious dataset biases are prevalent in existing datasets, which runs the risk of overestimating the true capa- bilities of machine intelligence on commonsense reasoning. Our results and analysis indicate the importance of con- tinued research on debiasing benchmarks and the increasing need for algorithmic"}, {"filename": "1.pdf", "page_number": 8, "content": "approaches for systematic bias reduc- tion, which allows for the benchmarks to evolve together with evolving state of the art. We leave it as a future research ques- tion to further investigate how much of our improvements are due to dataset biases of the existing benchmarks as opposed to true strides in improving commonsense intelligence. 5.4 Diagnostics for Gender Bias Winogender is designed as diagnostics for checking whether a model (and/or training corpora) suffers from gender bias. The bias is measured by the difference in accuracy between the cases where the pronoun gender matches the occupation\u2019smajority gender (called \u201cnon-gotcha\u201d) or not (\u201cgotcha\u201d). For- mally, it is computed as follows : \u0001F=Acc (Female, Non-gotcha)\u0000Acc (Female, Gotcha) \u0001M=Acc (Male, Non-gotcha)\u0000Acc (Male, Gotcha) for female and male cases, respectively. Large values of \u0001For\u0001Mindicate that the model is highly gender-biased, whereas j\u0001Fj=j\u0001Mj= 0 (along with high accuracy) is the ideal scenario. In addition, if"}, {"filename": "1.pdf", "page_number": 8, "content": "\u0001F or\u0001Mis largely negative , it implies that the model is biased in the other way around. The result of the gender-bias diagnostics is shown in Ta- ble 7. While we \ufb01nd that the RoBERTa models \ufb01netuned onWINOGRANDE and DPR both demonstrate very high ac- curacy, the gender gap in RoBERTa-WinoGrande is smaller than RoBERTa-DPR. 6 Conclusions We introduce WINOGRANDE , a new collection of 44k WSC- inspired problems that is signi\ufb01cantly larger than existing variants of the WSC dataset. To create a dataset that is ro- bust against spurious dataset-speci\ufb01c bias, we also present AFLITE\u2013 a novel light-weight adversarial \ufb01ltering algorithm for systematic bias reduction. The resulting dataset is consid- erably more challenging for existing state-of-the-art models while still being trivially easy for humans. In addition, using WINOGRANDE as a resource, we demonstrate effective trans- fer learning and achieve state-of-the-art results on several related benchmarks. In parallel, we also emphasize the"}, {"filename": "1.pdf", "page_number": 8, "content": "potential risk of overesti- mating the performance of the state-of-the-art methods on the existing commonsense benchmarks; these models might be solving the problems right for the wrong reasons, by relying on spurious statistical patterns (annotation artifacts). Our work suggests a new perspective for designing bench- marks for measuring progress in AI. Unlike past decades where the community constructed a static benchmark dataset to work on for many years to come, we now need AI algo- rithms to compose challenges that are hard enough for AI, which requires dynamic datasets that evolve together with the evolving state-of-the-art. Acknowledgments We thank the anonymous reviewers, Dan Weld, Noah Smith, Luke Zettlemoyer, Hannaneh Hajishirzi, Oren Etzioni, Leora Morgenstern, Ernest Davis, Gary Marcus, and Yuling Gu for their thoughtful feedback. This research was supported in part by NSF (IIS-1524371, IIS-1714566), DARPA under the CwC program through the ARO (W911NF-15-1-0543), and DARPA"}, {"filename": "1.pdf", "page_number": 8, "content": "under the MCS program through NIWC Paci\ufb01c (N66001-19-2-4031). References Bansal, M., and Klein, D. 2012. Coreference semantics from web features. ACL. Belinkov, Y .; Poliak, A.; Shieber, S.; Van Durme, B.; and Rush, A. 2019. On adversarial removal of hypothesis-only bias in natural language inference. *SEM ."}, {"filename": "1.pdf", "page_number": 9, "content": "Bender, D. 2015. Establishing a human baseline for the winograd schema challenge. MAICS . Chen, X., and Cardie, C. 2018. Multinomial adversarial networks for multi-domain text classi\ufb01cation. NAACL . Clark, K., and Manning, C. D. 2016. Deep reinforcement learning for mention-ranking coreference models. EMNLP . Davis, E.; Morgenstern, L.; and Ortiz, C. 2016. Human tests of materials for the winograd schema challenge 2016. Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv:1810.04805 . Durrett, G., and Klein, D. 2013. Easy victories and uphill battles in coreference resolution. EMNLP . Elazar, Y ., and Goldberg, Y . 2018. Adversarial removal of demographic attributes from text data. EMNLP . Emami, A.; Trischler, A.; Suleman, K.; and Cheung, J. C. K. 2018. A generalized knowledge hunting framework for the winograd schema challenge. NAACL: SRW . Emami, A.; Trichelair, P.; Trischler, A.; Suleman,"}, {"filename": "1.pdf", "page_number": 9, "content": "K.; Schulz, H.; and Cheung, J. C. K. 2019. The KnowRef corefer- ence corpus: Removing gender and number cues for dif\ufb01cult pronominal anaphora resolution. ACL. Geva, M.; Goldberg, Y .; and Berant, J. 2019. Are we modeling the task or the annotator? an investigation of annotator bias in natural language understanding datasets. arXiv:1908.07898 . Gordon, A. S.; Bejan, C. A.; and Sagae, K. 2011. Com- monsense causal reasoning using millions of personal stories. AAAI . Gordon, A.; Kozareva, Z.; and Roemmele, M. 2012. SemEval-2012 task 7: Choice of plausible alternatives: An evaluation of commonsense causal reasoning. *SEM . Gururangan, S.; Swayamdipta, S.; Levy, O.; Schwartz, R.; Bowman, S.; and Smith, N. A. 2018. Annotation artifacts in natural language inference data. NAACL . He, P.; Liu, X.; Chen, W.; and Gao, J. 2019. A hy- brid neural network model for commonsense reasoning. arXiv:1907.11983 . Kobdani, H.; Schuetze, H.; Schiehlen, M.; and Kamp, H. 2011. Bootstrapping coreference"}, {"filename": "1.pdf", "page_number": 9, "content": "resolution using word asso- ciations. ACL. Kocijan, V .; Cretu, A.-M.; Camburu, O.-M.; Yordanov, Y .; and Lukasiewicz, T. 2019. A surprisingly robust trick for the winograd schema challenge. ACL. Lee, H.; Peirsman, Y .; Chang, A.; Chambers, N.; Surdeanu, M.; and Jurafsky, D. 2011. Stanford\u2019s multi-pass sieve coreference resolution system at the CoNLL-2011 shared task. CoNLL: Shared Task . Levesque, H. J.; Davis, E.; and Morgenstern, L. 2011. The winograd schema challenge. AAAI Spring Symposium: Logi- cal Formalizations of Commonsense Reasoning . Liu, Q.; Jiang, H.; Ling, Z.-H.; Zhu, X.; Wei, S.; and Hu, Y . 2016. Commonsense knowledge enhanced embeddings for solving pronoun disambiguation problems in winograd schema challenge. arXiv:1611.04146 .Liu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M. S.; Chen, D.; Levy, O.; Lewis, M.; Zettlemoyer, L. S.; and Stoyanov, V . 2019. Roberta: A robustly optimized bert pretraining ap- proach. ArXiv abs/1907.11692. Morgenstern, L.; Davis, E.; and"}, {"filename": "1.pdf", "page_number": 9, "content": "Ortiz, C. L. 2016. Planning, executing, and evaluating the winograd schema challenge. AI Magazine 37(1):50\u201354. Niven, T., and Kao, H.-Y . 2019. Probing neural network comprehension of natural language arguments. ACL. Peng, H.; Khashabi, D.; and Roth, D. 2015. Solving hard coreference problems. NAACL . Poliak, A.; Naradowsky, J.; Haldar, A.; Rudinger, R.; and Van Durme, B. 2018. Hypothesis only baselines in natural language inference. *SEM . Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and Sutskever, I. 2019. Language models are unsupervised multitask learners. OpenAI Blog . Rahman, A., and Ng, V . 2012. Resolving complex cases of de\ufb01nite pronouns: The winograd schema challenge. EMNLP- CoNLL . Ratinov, L., and Roth, D. 2012. Learning-based multi-sieve co-reference resolution with knowledge. EMNLP-CoNLL . Roemmele, M.; Bejan, C. A.; and Gordon, A. S. 2011. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. AAAI Spring Symposium: Logical Formal-"}, {"filename": "1.pdf", "page_number": 9, "content": "izations of Commonsense Reasoning . Rudinger, R.; Naradowsky, J.; Leonard, B.; and Van Durme, B. 2018. Gender bias in coreference resolution. NAACL . Sasaki, S.; Takase, S.; Inoue, N.; Okazaki, N.; and Inui, K. 2017. Handling multiword expressions in causality estima- tion. IWCS . Stokes, P. D. 2005. Creativity from constraints: The psychol- ogy of breakthrough . Springer Publishing Company. Trichelair, P.; Emami, A.; Cheung, J. C. K.; Trischler, A.; Suleman, K.; and Diaz, F. 2018. On the evaluation of common-sense reasoning in natural language understanding. arXiv:1811.01778 . Trinh, T. H., and Le, Q. V . 2018. A simple method for commonsense reasoning. arXiv:1806.02847 . Tsuchiya, M. 2018. Performance impact caused by hid- den bias of training data for recognizing textual entailment. LREC . Turing, A. M. 1950. Computing machinery and intelligence. Mind . Wang, A.; Pruksachatkun, Y .; Nangia, N.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and Bowman, S. R. 2019. Superglue: A stickier"}, {"filename": "1.pdf", "page_number": 9, "content": "benchmark for general-purpose language understand- ing systems. arXiv:1905.00537 . Zellers, R.; Bisk, Y .; Schwartz, R.; and Choi, Y . 2018. Swag: A large-scale adversarial dataset for grounded commonsense inference. EMNLP . Zheng, J.; Vilnis, L.; Singh, S.; Choi, J. D.; and McCallum, A. 2013. Dynamic knowledge-base alignment for coreference resolution. CoNLL ."},{"filename": "3.pdf", "page_number": 1, "content": "Published as a conference paper at ICLR 2020 REFORMER : THEEFFICIENT TRANSFORMER Nikita Kitaev\u0003 U.C. Berkeley & Google Research kitaev@cs.berkeley.edu\u0141ukasz Kaiser\u0003 Google Research flukaszkaiser,levskaya g@google.comAnselm Levskaya Google Research ABSTRACT Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the ef\ufb01ciency of Transform- ers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O( L2) to O( LlogL), where Lis the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training pro- cess instead of Ntimes, where Nis the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-ef\ufb01cient and"}, {"filename": "3.pdf", "page_number": 1, "content": "much faster on long sequences. 1 I NTRODUCTION The Transformer architecture (Vaswani et al., 2017) is widely used in natural language processing and yields state-of-the-art results on a number of tasks. To obtain these results, researchers have resorted to training ever larger Transformer models. The number of parameters exceeds 0.5B per layer in the largest con\ufb01guration reported in (Shazeer et al., 2018) while the number of layers goes up to 64 in (Al-Rfou et al., 2018). Transformer models are also used on increasingly long sequences. Up to 11 thousand tokens of text in a single example were processed in (Liu et al., 2018) and when processing other modalities, like music (Huang et al., 2018) and images (Parmar et al., 2018), even longer sequences are commonplace. These large-scale long-sequence models yield great results but strain resources to the point where some argue that this trend is breaking NLP research1. Many large Transformer models can only realistically be trained in large"}, {"filename": "3.pdf", "page_number": 1, "content": "industrial research laboratories and such models trained with model parallelism cannot even be \ufb01ne-tuned on a single GPU as their memory requirements demand a multi-accelerator hardware setup even for a single training step. Do large Transformer models fundamentally require such huge resources or are they simply inef\ufb01- cient? Consider the following calculation: the 0.5B parameters used in the largest reported Trans- former layer account for 2GB of memory. Activations for 64K tokens with embedding size 1024 and batch size 8 account for 64K\u00021K\u00028 = 0 :5B \ufb02oats, requiring another 2GB of memory. If our memory use was only per-layer, then we should fairly easily \ufb01t a large Transformer even on sequences of length 64K on a single accelerator. Further, the whole corpus used to train BERT only requires 17GB to store. Why is it then that we cannot even \ufb01ne-tune these models on single machines? The above estimate includes only per-layer memory and input activations cost and does not take into"}, {"filename": "3.pdf", "page_number": 1, "content": "account the following major sources of memory use in the Transformer. \u000fMemory in a model with Nlayers is N-times larger than in a single-layer model due to the fact that activations need to be stored for back-propagation. \u000fSince the depth dffof intermediate feed-forward layers is often much larger than the depth dmodel of attention activations, it accounts for a large fraction of memory use. \u000fAttention on sequences of length Lis O(L2) in both computational and memory complex- ity, so even for a single sequence of 64K tokens can exhaust accelerator memory. \u0003Equal Contribution 1https://hackingsemantics.xyz/2019/leaderboards/ 1arXiv:2001.04451v2  [cs.LG]  18 Feb 2020"}, {"filename": "3.pdf", "page_number": 2, "content": "Published as a conference paper at ICLR 2020 We introduce the Reformer model which solves these problems using the following techniques: \u000fReversible layers, \ufb01rst introduced in Gomez et al. (2017), enable storing only a single copy of activations in the whole model, so the Nfactor disappears. \u000fSplitting activations inside feed-forward layers and processing them in chunks removes the dfffactor and saves memory inside feed-forward layers. \u000fApproximate attention computation based on locality-sensitive hashing replaces the O( L2) factor in attention layers with O( LlogL) and so allows operating on long sequences. We study these techniques and show that they have negligible impact on the training process com- pared to the standard Transformer. Splitting activations in fact only affects the implementation; it is numerically identical to the layers used in the Transformer. Applying reversible residuals instead of the standard ones does change the model but has a negligible effect on training"}, {"filename": "3.pdf", "page_number": 2, "content": "in all con\ufb01gurations we experimented with. Finally, locality-sensitive hashing in attention is a more major change that can in\ufb02uence the training dynamics, depending on the number of concurrent hashes used. We study this parameter and \ufb01nd a value which is both ef\ufb01cient to use and yields results very close to full attention. We experiment on a synthetic task, a text task (enwik8) with sequences of length 64K and an image generation task (imagenet-64 generation) with sequences of length 12K. In both cases we show that Reformer matches the results obtained with full Transformer but runs much faster, especially on the text task, and with orders of magnitude better memory ef\ufb01ciency. 2 L OCALITY -SENSITIVE HASHING ATTENTION Dot-product attention. The standard attention used in the Transformer is the scaled dot-product attention (Vaswani et al., 2017). The input consists of queries and keys of dimension dk, and values of dimension dv. The dot products of the query with all keys are computed,"}, {"filename": "3.pdf", "page_number": 2, "content": "scaled bypdk, and a softmax function is applied to obtain the weights on the values. In practice, the attention function on a set of queries is computed simultaneously, packed together into a matrix Q. Assuming the keys and values are also packed together into matrices KandV, the matrix of outputs is de\ufb01ned as: Attention( Q; K; V ) = softmax(QKT pdk)V (1) Multi-head attention. In the Transformer, instead of performing a single attention function with dmodel -dimensional keys, values and queries, one linearly projects the queries, keys and values h times with different, learned linear projections to dk,dkanddvdimensions, respectively. Attention is applied to each of these projected versions of queries, keys and values in parallel, yielding dv- dimensional output values. These are concatenated and once again projected, resulting in the \ufb01nal values. This mechanism is known as multi-head attention. Memory-ef\ufb01cient attention. To calculate the memory use of the attention mechanism, let us"}, {"filename": "3.pdf", "page_number": 2, "content": "focus on the attention computation from Equation 1. Let us assume that Q, K and V all have the shape [batch size; length; d model ]. The main issue is the term QKT, which has the shape [batch size; length; length ]. In the experimental section we train a model on sequences of length 64K\u2013 in this case, even at batch-size of 1, this is a 64K\u000264Kmatrix, which in 32-bit \ufb02oats would take 16GB of memory. This is impractical and has hindered the use of the Transformer for long sequences. But it is important to note that the QKTmatrix does not need to be fully materialized in memory. The attention can indeed be computed for each query qiseparately, only calculating softmax(qiKT pdk)Vonce in memory, and then re-computing it on the backward pass when needed for gradients. This way of computing attention may be less ef\ufb01cient but it only uses memory propor- tional to length . We use this memory-ef\ufb01cient implementation of attention to run the full-attention baselines presented in the experimental"}, {"filename": "3.pdf", "page_number": 2, "content": "section. Where do Q, K, V come from? The multi-head attention described above operates on keys, queries and values, but usually we are only given a single tensor of activations A of the shape [batch size; length; d model ]\u2013 e.g., coming from embedding the tokens in a sentence into vectors. 2"}, {"filename": "3.pdf", "page_number": 3, "content": "Published as a conference paper at ICLR 2020 Figure 1: An angular locality sensitive hash uses random rotations of spherically projected points to establish buckets by an argmax over signed axes projections. In this highly simpli\ufb01ed 2D depiction, two points xandyare unlikely to share the same hash buckets (above) for the three different angular hashes unless their spherical projections are close to one another (below). To build Q, K and V from A, the Transformer uses 3 different linear layers projecting A into Q, K and V with different parameters. For models with LSH attention, we want queries and keys (Q and K) to be identical. This is easily achieved by using the same linear layer to go from A to Q and K, and a separate one for V . We call a model that behaves like this a shared-QK Transformer. It turns out that sharing QK does not affect the performance of Transformer, even if we additionally normalize the length of the keys K, as we show in the experimental Section 5. Hashing"}, {"filename": "3.pdf", "page_number": 3, "content": "attention. For the LSH attention, we start with two tensors, Q=K and V of the shape [batch size; length; d model ]. We keep the multi-head mechanism intact and focus on the atten- tion computation from Equation 1. As already mentioned, the main issue is the term QKT, which has the shape [batch size; length; length ]. But note that we are actually only interested insoftmax( QKT). Since softmax is dominated by the largest elements, for each query qiwe only need to focus on the keys in K that are closest to qi. For example, if K is of length 64K, for each qi we could only consider a small subset of, say, the 32or64closest keys. That is much more ef\ufb01cient, but how can we \ufb01nd the nearest neighbors among the keys? Locality sensitive hashing. The problem of \ufb01nding nearest neighbors quickly in high-dimensional spaces can be solved by locality-sensitive hashing (LSH). A hashing scheme that assigns each vector xto a hash h(x)is called locality-sensitive if nearby vectors get the same hash with"}, {"filename": "3.pdf", "page_number": 3, "content": "high probability and distant ones do not. In our case, we actually only require that nearby vectors get the same hash with high probability and that hash-buckets are of similar size with high probability. We achieve this by employing random projections as follows (see Figure 1). To get bhashes, we \ufb01rst \ufb01x a random matrix Rof size [dk; b=2]. We then de\ufb01ne h(x) = arg max([ xR;\u0000xR])where [u;v]denotes the concatenation of two vectors. This method is a known LSH scheme (Andoni et al., 2015) and is easy to implement and apply to batches of vectors. LSH attention. Knowing our LSH scheme and the general idea of hashing attention, we will now formalize the LSH attention we use in this paper. We \ufb01rst rewrite the equation for normal attention, (1), for a single query position iat a time: oi=X j2Piexp (qi\u0001kj\u0000z(i;Pi))vj wherePi=fj:i\u0015jg (2) We introduce the notation Pito represent the set that the query at position iattends to, and zto denote the partition function (i.e. the normalizing term in the"}, {"filename": "3.pdf", "page_number": 3, "content": "softmax). For clarity, we also omit scaling bypdk. For batching purposes we typically perform attention over a larger set ePi=f0;1; : : : ; lg\u0013P i while masking out elements not in Pi: oi=X j2ePiexp (qi\u0001kj\u0000m(j;Pi)\u0000z(i;Pi))vjwhere m(j;Pi) =\u001a1 ifj =2Pi 0 otherwise(3) 3"}, {"filename": "3.pdf", "page_number": 4, "content": "Published as a conference paper at ICLR 2020 Figure 2: Simpli\ufb01ed depiction of LSH Attention showing the hash-bucketing, sorting, and chunking steps and the resulting causal attentions. (a-d) Attention matrices for these varieties of attention. Now we turn to LSH attention, which we can think of in terms of restricting the set Piof target items a query position ican attend to, by only allowing attention within a single hash bucket. Pi=fj:h(qi) =h(kj)g (4) Figure 2(a-b) shows a schematic comparison of full-attention with a hashed variant. Part (a) depicts that the attention matrix for full attention is typically sparse, but the computation does not take advantage of this sparsity. In (b), the queries and keys have been sorted according to their hash bucket. Since similar items fall in the same bucket with high probability, the full attention pattern can be approximated by only allowing attention within each bucket. Hash buckets in this formulation tend to be uneven in size, which makes"}, {"filename": "3.pdf", "page_number": 4, "content": "it dif\ufb01cult to batch across buckets. Moreover, the number of queries and the number of keys within a bucket may be unequal \u2013 in fact, it is possible for a bucket to contain many queries but no keys. To alleviate these issues, we \ufb01rst ensure that h(kj) =h(qj)by setting kj=qj kqjk. Next, we sort the queries by bucket number and, within each bucket, by sequence position; this de\ufb01nes a permutation where i7!siafter sorting. In the sorted attention matrix, pairs from the same bucket will cluster near the diagonal (as depicted in Figure 2c). We can follow a batching approach where chunks of mconsecutive queries (after sorting) attend to each other, and one chunk back (Figure 2d). Following our earlier notation, this corresponds to setting: ePi=n j:jsi mk \u00001\u0014jsj mk \u0014jsi mko (5) Ifmax ijPij< m , thenPi\u0012ePi. In practice we set m=2l nbuckets(where lis the sequence length). The average bucket size isl nbuckets, and we assume that the probability of a bucket growing to twice that size is"}, {"filename": "3.pdf", "page_number": 4, "content": "suf\ufb01ciently low. The overall process of LSH attention is summarized in Figure 2. Multi-round LSH attention. With hashing, there is always a small probability that similar items nevertheless fall in different buckets. This probability can be reduced by doing multiple rounds of hashing with nrounds distinct hash functions fh(1); h(2); : : :g, such that: Pi=nrounds[ r=1P(r) i whereP(r) i=n j:h(r)(qi) =h(r)(qj)o (6) The multi-round case essentially involves performing LSH attention nrounds times in parallel; the details of the procedure are described in in Appendix A. Causal masking for shared-QK attention. In a Transformer decoder, masking (denoted by m(j;Pi)in Equation 3) is used to prevent positions from attending into the future. To implement masking in LSH attention, we associate every query/key vector with a position index, re-order the position indices using the same permutations used to sort the query/key vectors, and then use a comparison operation to compute the mask. 4"}, {"filename": "3.pdf", "page_number": 5, "content": "Published as a conference paper at ICLR 2020 Table 1: Memory and time complexity of attention variants. We write lfor length, bfor batch size, nhfor the number of heads, ncfor the number of LSH chunks, nrfor the number of hash repetitions. Attention Type Memory Complexity Time Complexity Scaled Dot-Product max(bnhldk; bnhl2) max( bnhldk; bnhl2) Memory-Ef\ufb01cient max(bnhldk; bnhl2) max( bnhldk; bnhl2) LSH Attention max(bnhldk; bnhlnr(4l=nc)2) max( bnhldk; bnhnrl(4l=nc)2) Table 2: Accuracies on the duplication task of a 1-layer Transformer model with full attention and with locality-sensitive hashing attention using different number of parallel hashes. TrainEvalFull Attention LSH- 8LSH- 4LSH- 2LSH- 1 Full Attention 100% 94.8% 92.5% 76.9% 52.5% LSH- 4 0.8% 100% 99.9% 99.4% 91.9% LSH- 2 0.8% 100% 99.9% 98.1% 86.8% LSH- 1 0.8% 99.9% 99.6% 94.8% 77.9% While attention to the future is not allowed, typical implementations of the Transformer doallow a position to attend to itself . Such behavior"}, {"filename": "3.pdf", "page_number": 5, "content": "is undesirable in a shared-QK formulation because the dot-product of a query vector with itself will almost always be greater than the dot product of a query vector with a vector at another position. We therefore modify the masking to forbid a token from attending to itself, except in situations where a token has no other valid attention targets (e.g. the \ufb01rst token in a sequence). 2.1 A NALYSIS ON A SYNTHETIC TASK To verify the performance of LSH attention and study its behavior, we start with the following synthetic task: duplicate a sequence of symbols. In this task, each training and testing example has the form 0w0wwhere w2f1; : : : ; Ng\u0003is a sequence of symbols ranging from 1toN(we use N= 127 in our experiments). An example with the word wof length 3is given below. Example: 019 113 72 019 113 72 To study LSH attention, we train a language model on examples of the above form where each w is of length 511(so the whole input 0w0wis of length 1024 ). As this is a language modeling"}, {"filename": "3.pdf", "page_number": 5, "content": "task, we always predict the next symbol given all the previous ones, but we mask the loss and accuracy to only consider positions in the second half of the input, i.e., those that can actually be predicted. The above task can be solved perfectly (to accuracy 100% and loss 0) by a 1-layer Transformer model. Note though, that it requires non-local attention lookups, so it cannot be solved by any model relying on sparse attention with a limited span. To make it easy and fast to train but similar to models used in NLP, we use a 1-layer Transformer with dmodel =dff= 256 and4heads. We train it for 150K steps in 4different settings: with full attention, LSH attention with nrounds = 1, nrounds = 2andnrounds = 4. From the results summarized in Table 2 we see that a model trained with full attention can be imme- diately used with LSH attention, but at some loss of accuracy. When trained from scratch with LSH attention, the model trained with 4 hashes achieves almost perfect accuracy as well."}, {"filename": "3.pdf", "page_number": 5, "content": "Interestingly, the accuracy becomes perfect when evaluated with 8 hashes. It goes down when evaluated with 2 or 1 hashes. Models trained with less hashes show worse results but even the model trained with just 1 hash performs almost perfectly when evaluated with 8 hashes. 5"}, {"filename": "3.pdf", "page_number": 6, "content": "Published as a conference paper at ICLR 2020 3 R EVERSIBLE TRANSFORMER As the above section shows, the complexity of attention can be reduced from square in length to linear, provided an approximation is acceptable. But it is clear from Table 1 that each \ufb01eld starts with a b\u0001nh\u0001lterm: the b\u0001nh\u0001l\u0001dk, or alternatively b\u0001l\u0001dmodel cost cannot be avoided. Indeed, the activations before each layer are already of the size b\u0001l\u0001dmodel , so the memory use of the whole model with nllayers is at least b\u0001l\u0001dmodel\u0001nl. Even worse: inside the feed-forward layers of Transformer this goes up to b\u0001l\u0001dff\u0001nl. In a big Transformer it is usual to set dff= 4Kand nl= 16 so with l= 64Kthis again would use an impractical 16GBof memory In this section, we show how to reduce this cost by \ufb01rst dealing with the nlpart of the term using reversible layers and then showing how chunking can allow us to handle the dffproblem. The effects of each of these approaches on memory and time complexity are summarized in Table 3."}, {"filename": "3.pdf", "page_number": 6, "content": "RevNets. Reversible residual networks were introduced by Gomez et al. (2017) where it was shown that they can replace ResNets for image classi\ufb01cation. The main idea is to allow the activations at any given layer to be recovered from the activations at the following layer, using only the model parameters. Rather than having to checkpoint intermediate values for use in the backward pass, layers can be reversed one-by-one as back-propagation proceeds from the output of the network to its input. Whereas a normal residual layer performs a function x7!ythat operates on a single input and produces a single output and has the form y=x+F(x), a reversible layer works on pairs of inputs/outputs: (x1; x2)7!(y1; y2), and follows the equations: y1=x1+F(x2) y2=x2+G(y1) (7) A layer can be reversed by subtracting (rather than adding) the residuals: x2=y2\u0000G(y1) x1=y1\u0000F(x2) (8) Reversible Transformer. We apply the RevNet idea to the Transformer by combining the attention and feed-forward layers inside"}, {"filename": "3.pdf", "page_number": 6, "content": "the revnet block. In the notation above, F becomes an attention layer while G becomes the feed-forward layer. Note that Layer Normalization (Ba et al., 2016) is moved inside the residual blocks. Y1=X1+ Attention( X2) Y2=X2+ FeedForward( Y1) (9) The reversible Transformer does not need to store activations in each layer and so gets rid of the nl term. In Section 5 we show that it performs the same as the normal Transformer when using the same number of parameters; we achieve this by having both x1andx2have size dmodel . Chunking. While reversibility covers the nlterm, the thicker layers can still use a lot of memory. The feed-forward layer in particular can use intermediate vectors of dimensionality dff= 4Kor higher. However, computations in feed-forward layers are completely independent across positions in a sequence, so the computation can be split into cchunks: Y2=h Y(1) 2;: : :;Y(c) 2i =h X(1) 2+ FeedForward( Y(1) 1);: : :;X(c) 2+ FeedForward( Y(c) 1)i (10) This layer is typically"}, {"filename": "3.pdf", "page_number": 6, "content": "batched by performing operations for all positions in parallel, but operating on one chunk at a time can reduce memory. The reverse computation in (8) and the backward pass are also chunked. In addition to the feed-forward layers, for models with large vocabulary (more thandmodel word types) we also chunk the log-probabilities at the output and calculate the loss for sections of the sequence at a time. Chunking, large batches and parameter reuse. With chunking and reversible layers the memory we use for activations in the whole network is independent of the number of layers. The same is not true for parameters though as their number grows with the number of layers. This problem is remedied though because we can swap layer parameters to and from CPU memory when this layer is not computing. In a standard Transformer this would be inef\ufb01cient because memory transfer to CPU is slow. The batch size multiplied by length in Reformer is much larger though and therefore the amount of compute"}, {"filename": "3.pdf", "page_number": 6, "content": "done with the parameters amortizes the cost of their transfer. 6"}, {"filename": "3.pdf", "page_number": 7, "content": "Published as a conference paper at ICLR 2020 Table 3: Memory and time complexity of Transformer variants. We write dmodel anddfffor model depth and assume dff\u0015dmodel ;bstands for batch size, lfor length, nlfor the number of layers. We assume nc=l=32so4l=nc= 128 and we write c= 1282. Model Type Memory Complexity Time Complexity Transformer max(bldff; bnhl2)nl (bldff+bnhl2)nl Reversible Transformer max(bldff; bnhl2) ( bnhldff+bnhl2)nl Chunked Reversible Transformer max(bldmodel; bnhl2) ( bnhldff+bnhl2)nl LSH Transformer max(bldff; bnhlnrc)nl(bldff+bnhnrlc)nl Reformer max(bldmodel; bnhlnrc) (bldff+bnhnrlc)nl 4 R ELATED WORK The Transformer model introduced in (Vaswani et al., 2017) has been used widely in natural lan- guage tasks and further extended to model diverse data such as music scores (Huang et al., 2018), and images (Parmar et al., 2018; Ramachandran et al., 2019). Most notably, this model class has been applied successfully in the self-supervised training of extremely large"}, {"filename": "3.pdf", "page_number": 7, "content": "language models (Devlin et al., 2018; Radford et al., 2019). Given the enormous computational requirements of state of the art sequence models, there has been increasing interest in \ufb01nding methods to reduce the memory footprint and computational require- ments of Transformer models. In addition to standard methods such as precision reduction and gradient checkpointing (Sohoni et al., 2019), more ef\ufb01cient versions of the Transformer model\u2019s self-attention mechanism (Sukhbaatar et al., 2019a;b) have also recently been explored. In particular, leveraging sparsity in the attention layers has proved fruitful. OpenAI introduced the sparse Transformer (Child et al., 2019) which exploits a factorized sparse representation of atten- tion. Using product-key attention to increase the key space has also been used to reduce memory requirements in the feed-forward layers with no loss in performance (Lample et al., 2019). Locality-sensitive hashing (LSH) has, to our knowledge, not been directly"}, {"filename": "3.pdf", "page_number": 7, "content": "applied to Transformer attention layers before. But previous work using external memory with neural networks has dealt with memories of large sizes. The original implementation of memory networks (Weston et al., 2014) and later work on scaling it (Bordes et al., 2015; Chandar et al., 2016) used memory with size in the millions. The cost of doing so is that the memory must be \ufb01xed prior to training. Moreover, since during the beginning of training the model is unlikely to query the memory correctly, strong supervision is used to encourage the model to query memory locations that are useful. These hints are either given as additional supervising information by the task or determined heuristically as in Hill et al. (2015). The requirement that the memory be \ufb01xed before has been removed in Santoro et al. (2016) at the cost of memory size and later alleviated by Rae et al. (2016). The last paper considered memory lookups with approximate nearest neighbors including both LSH and random kd-"}, {"filename": "3.pdf", "page_number": 7, "content": "trees, but only for lookups in external memory. 5 E XPERIMENTS In this section we present experimental results demonstrating the techniques described above. We analyze the techniques one-by-one to make clear which combinations have impact on performance. We start by showing that reversible layers and shared query-key spaces do not impact performance, then proceed to analyze hashing attention and \ufb01nally the full Reformer model. We ran our experiments on the imagenet64 and enwik8-64K tasks, where the latter is a variant of enwik8 that is chunked into subsequences of 216= 64Ktokens. We use 3-layer models for our ablations so as to make it tractable to compare with the regular Transformer, which has high memory usage and performs full O(l2)attention. All experiments have dmodel = 1024 ,dff= 4096 , nheads = 8, and a total batch size of 8 sequences. We used the Adafactor optimizer (Shazeer & Stern, 2018) for training these models. We also evaluate on the WMT 2014 English-to-German"}, {"filename": "3.pdf", "page_number": 7, "content": "translation task, following the hyperparameters of Vaswani et al. (2017). Training for all experiments 7"}, {"filename": "3.pdf", "page_number": 8, "content": "Published as a conference paper at ICLR 2020 Figure 3: Effect of shared query-key space (left) and reversibility (right) on performance on enwik8 and imagenet64 training. The curves show bits per dim on held-out data. Table 4: BLEU scores on newstest2014 for WMT English-German (EnDe). We additionally report detokenized BLEU scores as computed by sacreBLEU (Post, 2018). sacreBLEU Model BLEU Uncased3Cased4 Vaswani et al. (2017), base model 27.3 Vaswani et al. (2017), big 28.4 Ott et al. (2018), big 29.3 Reversible Transformer (base, 100K steps) 27.6 27.4 26.9 Reversible Transformer (base, 500K steps, no weight sharing) 28.0 27.9 27.4 Reversible Transformer (big, 300K steps, no weight sharing) 29.1 28.9 28.4 was parallelized across 8 devices (8 GPUs or 8 TPU v3 cores). Code for training our models is made publicly available.2 Effect of sharing QK. We \ufb01rst consider the effect of shared-QK attention on a regular Transformer model. Shared-QK attention sets kj=qj kqjkand prevents tokens from"}, {"filename": "3.pdf", "page_number": 8, "content": "attending to themselves (except when no other context is available). In the left part of Figure 3, we plot perplexity curves for both regular and shared-QK attention. A shared query-key space does not perform worse than regular attention; in fact, for enwik8 it appears to train slightly faster. In other words, we are not sacri\ufb01cing accuracy by switching to shared-QK attention. Effect of reversible layers. In the two plots on the right in Figure 3, we compare a regular Trans- former per Vaswani et al. (2017) with the reversible one describe in Section 3. The two models have identical parameter counts, and the learning curves likewise appear to be nearly the same. These results show that the memory savings in the reversible Transformer do not come at the expense of accuracy. Reversible layers in machine translation. We also evaluate reversible layers in the context of an encoder-decoder Transformer model for machine translation from English to German. We start by making both the encoder"}, {"filename": "3.pdf", "page_number": 8, "content": "and the decoder fully reversible in the Transformer-base architecture, and 2https://github.com/google/trax/tree/master/trax/models/reformer 3BLEU+case.lc+lang.en-de+numrefs.1+smooth.exp+test.wmt14/full+tok.intl+version.1.4.3 4BLEU+case.mixed+lang.en-de+numrefs.1+smooth.exp+test.wmt14/full+tok.intl+version.1.4.3 8"}, {"filename": "3.pdf", "page_number": 9, "content": "Published as a conference paper at ICLR 2020 Figure 4: LSH attention performance as a function of hashing rounds on imagenet64. Figure 5: Left: LSH attention performance as a function of number of layers on enwik8. Right: Speed of attention evaluation as a function of input length for full- and LSH- attention. see that the resulting model performs comparably to Vaswani et al. (2017) when trained for 100K steps. We also evaluate training for a greater number of steps and with a larger model. Reformer models are very memory-ef\ufb01cient, so for the latter two experiments we do not need to save mem- ory by sharing embedding and output projection weight matrices throughout the model. Results are shown in Table 4. We do not apply LSH attention in this setting because examples are single sentences, and sentences tend to be relatively short. Our typical LSH attention con\ufb01guration uses chunks of 128 tokens after hashing and sorting, whereas the examples in the WMT14 test set are all shorter than"}, {"filename": "3.pdf", "page_number": 9, "content": "128 tokens. LSH attention in Transformer. LSH attention is an approximation for full attention that, as evi- denced in Figure 4, becomes more accurate as the number of hashes increases. At nrounds = 8, it already almost matches full attention. The computational cost of a model grows with the number of hashes, so this hyperparameter can be adjusted depending on the available compute budget. Ad- ditionally, as in Table 2, the number of hashes can be increased at evaluation time to produce more accurate results. On the right half of Figure 5, we plot the speed of different attention types vs. the sequence length, while holding the total number of tokens \ufb01xed. We see that while regular attention becomes slower at longer sequence length, LSH attention speed remains \ufb02at. Large Reformer models. To verify that the Reformer can indeed \ufb01t large models on a single core and train fast on long sequences, we train up to 20-layer big Reformers on enwik8 and imagenet64. As can be seen in Figure 5,"}, {"filename": "3.pdf", "page_number": 9, "content": "these models \ufb01t into memory and train. We were not able to train Trans- former baselines in this case as they are too slow and memory-hungry, but we see clear improvement with the number of layers. A 12-layer model on enwik8 trained for 20K steps with a dropout rate of 0.1 achieves 1.19 bits/dim on the test set. We also trained a 12-layer Reformer model for longer with further tuning and improvements and we reached 1.05 bits/dim on the enwiki8 test set. 9"}, {"filename": "3.pdf", "page_number": 10, "content": "Published as a conference paper at ICLR 2020 6 C ONCLUSION Reformer combines the modeling capacity of a Transformer with an architecture that can be executed ef\ufb01ciently on long sequences and with small memory use even for models with a large number of layers. We believe that this will help large, richly-parameterized Transformer models become more widespread and accessible. Also, the ability to handle long sequences opens the way for the use of the Reformer on many generative tasks. In addition to generating very long coherent text, the Reformer can bring the power of Transformer models to other domains like time-series forecasting, music, image and video generation. REFERENCES Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. Character-level language modeling with deeper self-attention. CoRR , abs/1808.04444, 2018. URL http: //arxiv.org/abs/1808.04444 . Alexandr Andoni, Piotr Indyk, Thijs Laarhoven, Ilya P. Razenshteyn, and Ludwig Schmidt. Practical and optimal LSH"}, {"filename": "3.pdf", "page_number": 10, "content": "for angular distance. CoRR , abs/1509.02897, 2015. URL http://arxiv. org/abs/1509.02897 . Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450 , 2016. URL http://arxiv.org/abs/1607.06450 . Antoine Bordes, Nicolas Usunier, Sumit Chopra, and Jason Weston. Large-scale simple question answering with memory networks. CoRR , abs/1506.02075, 2015. URL http://arxiv.org/ abs/1506.02075 . Sarath Chandar, Sungjin Ahn, Hugo Larochelle, Pascal Vincent, Gerald Tesauro, and Yoshua Ben- gio. Hierarchical memory networks. arXiv preprint arXiv:1605.07427 , 2016. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. URL https://openai.com/blog/sparse-transformers , 2019. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR , abs/1810.04805, 2018. URL http://arxiv.org/abs/1810.04805 . Aidan N"}, {"filename": "3.pdf", "page_number": 10, "content": "Gomez, Mengye Ren, Raquel Urtasun, and Roger B Grosse. The reversible residual net- work: Backpropagation without storing activations. In Advances in neural information processing systems , pp. 2214\u20132224, 2017. Felix Hill, Antoine Bordes, Sumit Chopra, and Jason Weston. The goldilocks principle: Reading children\u2019s books with explicit memory representations. CoRR , abs/1511.02301, 2015. URL http://arxiv.org/abs/1511.02301 . Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer, Curtis Hawthorne, An- drew M Dai, Matthew D Hoffman, and Douglas Eck. Music transformer: Generating music with long-term structure. arXiv preprint arXiv:1809.04281 , 2018. Guillaume Lample, Alexandre Sablayrolles, Marc\u2019Aurelio Ranzato, Ludovic Denoyer, and Herv \u00b4e J\u00b4egou. Large memory layers with product keys. CoRR , abs/1907.05242, 2019. URL http: //arxiv.org/abs/1907.05242 . Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating wikipedia"}, {"filename": "3.pdf", "page_number": 10, "content": "by summarizing long sequences. CoRR , abs/1801.10198, 2018. URL http://arxiv.org/abs/1801.10198 . Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine trans- lation. In Proceedings of the Third Conference on Machine Translation: Research Papers , pp. 1\u20139, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-6301. URL https://www.aclweb.org/anthology/W18-6301 . 10"}, {"filename": "3.pdf", "page_number": 11, "content": "Published as a conference paper at ICLR 2020 Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, and Alexander Ku. Image transformer. CoRR , abs/1802.05751, 2018. URL http://arxiv.org/abs/1802. 05751 . Matt Post. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers , pp. 186\u2013191, Belgium, Brussels, October 2018. As- sociation for Computational Linguistics. URL https://www.aclweb.org/anthology/ W18-6319 . Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. Jack W Rae, Jonathan J Hunt, Tim Harley, Ivo Danihelka, Andrew Senior, Greg Wayne, Alex Graves, and Timothy P Lillicrap. Scaling memory-augmented neural networks with sparse reads and writes. In Advances in Neural Information Processing Systems, (NIPS) , 2016. Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jonathon"}, {"filename": "3.pdf", "page_number": 11, "content": "Shlens. Stand-alone self-attention in vision models. CoRR , abs/1906.05909, 2019. URL http: //arxiv.org/abs/1906.05909 . Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy P. Lillicrap. One- shot learning with memory-augmented neural networks. CoRR , abs/1605.06065, 2016. URL http://arxiv.org/abs/1605.06065 . Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. CoRR , abs/1804.04235, 2018. URL http://arxiv.org/abs/1804.04235 . Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, Ryan Sepassi, and Blake Hecht- man. Mesh-tensor\ufb02ow: Deep learning for supercomputers. CoRR , abs/1811.02084, 2018. URL http://arxiv.org/abs/1811.02084 . Nimit Sharad Sohoni, Christopher Richard Aberger, Megan Leszczynski, Jian Zhang, and Christo- pher R \u00b4e. Low-memory neural network training: A technical report. CoRR , abs/1904.10631, 2019."}, {"filename": "3.pdf", "page_number": 11, "content": "URLhttp://arxiv.org/abs/1904.10631 . Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive atten- tion span in transformers. CoRR , abs/1905.07799, 2019a. URL http://arxiv.org/abs/ 1905.07799 . Sainbayar Sukhbaatar, Edouard Grave, Guillaume Lample, Herv \u00b4e J\u00b4egou, and Armand Joulin. Aug- menting self-attention with persistent memory. CoRR , abs/1907.01470, 2019b. URL http: //arxiv.org/abs/1907.01470 . Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. CoRR , 2017. URL http: //arxiv.org/abs/1706.03762 . Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. CoRR , abs/1410.3916, 2014. URL http://arxiv.org/abs/1410.3916 . 11"}, {"filename": "3.pdf", "page_number": 12, "content": "Published as a conference paper at ICLR 2020 A M ULTI -ROUND LSH A TTENTION In this section we describe in more detail the multi-hash version of our LSH attention mechanism. We \ufb01rst repeat Equation (3) from the main text, which describes a general formulation of attention with sparsity: oi=X j2ePiexp (qi\u0001kj\u0000m(j;Pi)\u0000z(i;Pi))vjwhere m(j;Pi) =\u001a1 ifj =2Pi 0 otherwise(3) In the multi-round case, a query position ican attend to key positions Pias de\ufb01ned in (6), which we also repeat here: Pi=nrounds[ r=1P(r) i whereP(r) i=n j:h(r)(qi) =h(r)(qj)o (6) For batching purposes, attention is performed on chunks of sorted queries/keys: eP(r) i=( j:$ s(r) i m% \u00001\u0014$ s(r) j m% \u0014$ s(r) i m%) (11) Combining (3) and (6) gives: oi=X j2ePiexp (qi\u0001kj\u0000m(j;Pi)\u0000z(i;Pi))vj (12) =nroundsX r=1exp\u0010 z(i;P(r) i)\u0000z(i;Pi)\u0011X j2eP(r) i1 Ni;jexp\u0010 qi\u0001kj\u0000m(j;P(r) i)\u0000z(i;P(r) i)\u0011 vj (13) =nroundsX r=1exp\u0010 z(i;P(r) i)\u0000z(i;Pi)\u0011 o(r) i (14) o(r) i=X j2eP(r) iexp\u0010 qi\u0001kj\u0000m(r) i;j\u0000z(i;P(r) i)\u0011 vj (15) where Ni;j=   n r0:j2P(r0) io"}, {"filename": "3.pdf", "page_number": 12, "content": "andm(r) i;j=8 >< >:1 ifj =2P(r) i 105ifi=j logNi;j otherwise(16) Each round of LSH attention produces a vector o(r) ithat can be computed independently from other rounds, except for the inclusion of a term Ni;jto avoid double-counting elements when constructing the union ofP(r) isets. In our implementation we fold the Ni;jfactor into the masking term m(r) i;j. We also modify m(r) i;jto introduce a special case for i=j. This case is added because causal masking in a standard Transformer allows position ito attend to itself, which is not desirable in a shared-QK formulation. We set the mask to a large but \ufb01nite value to disallow attention-in-place, except in the situation where a token has no other valid attention targets. For example, the \ufb01rst token in a sequence attends only to itself, because no prior context is available. 12"},{"filename": "4.pdf", "page_number": 1, "content": "Longformer: The Long-Document Transformer Iz Beltagy\u0003Matthew E. Peters\u0003Arman Cohan\u0003 Allen Institute for Arti\ufb01cial Intelligence, Seattle, WA, USA fbeltagy,matthewp,armanc g@allenai.org Abstract Transformer-based models are unable to pro- cess long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer\u2019s attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task moti- vated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language mod- eling and achieve state-of-the-art results on text8 andenwik8 . In contrast to most prior work, we also pretrain Longformer and \ufb01netune it on a variety of downstream tasks."}, {"filename": "4.pdf", "page_number": 1, "content": "Our pretrained Longformer consistently out- performs RoBERTa on long document tasks and sets new state-of-the-art results on Wiki- Hop and TriviaQA. We \ufb01nally introduce the Longformer-Encoder-Decoder (LED), a Long- former variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv sum- marization dataset.1 1 Introduction Transformers (Vaswani et al., 2017) have achieved state-of-the-art results in a wide range of natu- ral language tasks including generative language modeling (Dai et al., 2019; Radford et al., 2019) and discriminative language understanding (De- vlin et al., 2019). This success is partly due to the self-attention component which enables the net- work to capture contextual information from the entire sequence. While powerful, the memory and computational requirements of self-attention grow \u0003Equal contribution. 1https://github.com/allenai/longformer Figure 1: Runtime and memory of full self- attention and"}, {"filename": "4.pdf", "page_number": 1, "content": "different implementations of Long- former\u2019s self-attention; Longformer-loop is non- vectorized, Longformer-chunk is vectorized, and Longformer-cuda is a custom cuda kernel im- plementations. Longformer\u2019s memory usage scales linearly with the sequence length, unlike the full self-attention mechanism that runs out of memory for long sequences on current GPUs. Different implementations vary in speed, with the vectorized Longformer-chunk being the fastest. More details are in section 3.2. quadratically with sequence length, making it infea- sible (or very expensive) to process long sequences. To address this limitation, we present Long- former, a modi\ufb01ed Transformer architecture with a self-attention operation that scales linearly with the sequence length, making it versatile for pro- cessing long documents (Fig 1). This is an advan- tage for natural language tasks such as long docu- ment classi\ufb01cation, question answering (QA), and coreference resolution, where existing approaches"}, {"filename": "4.pdf", "page_number": 1, "content": "partition or shorten the long context into smaller sequences that fall within the typical 512 token limit of BERT-style pretrained models. Such parti- tioning could potentially result in loss of important cross-partition information, and to mitigate this problem, existing methods often rely on complex architectures to address such interactions. On the other hand, our proposed Longformer is able to build contextual representations of the entire con- text using multiple layers of attention, reducing thearXiv:2004.05150v2  [cs.CL]  2 Dec 2020"}, {"filename": "4.pdf", "page_number": 2, "content": "need for task-speci\ufb01c architectures. Recent work has addressed the computational in- ef\ufb01ciency of Transformers on long sequences (see Tab. 1). However, they primarily focus on autore- gressive language modeling (LM), while the appli- cation of long document transformers to document- level NLP tasks in the transfer learning setting (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019) has remained largely unexplored. We address this gap and show that Longformer\u2019s attention mechanism can act as a drop-in replacement for the self-attention mecha- nism in pretrained Transformers, and leads to gains across a suite of document NLP tasks. Longformer\u2019s attention mechanism is a combina- tion of a windowed local-context self-attention and an end task motivated global attention that encodes inductive bias about the task. Through ablations and controlled trials we show both attention types are essential \u2013 the local attention is primarily used to build contextual"}, {"filename": "4.pdf", "page_number": 2, "content": "representations, while the global attention allows Longformer to build full sequence representations for prediction. We \ufb01rst evaluate Longformer on autoregressive character-level language modeling using a com- bination of windowed and a new dilated attention pattern, allowing the model to process sequences of up to 32K characters on modern GPUs. We achieve state-of-the-art results on text8 andenwik8 benchmark datasets, demonstrating the effective- ness of Longformer in long document modeling. Then, to evaluate Longformer\u2019s ability to re- place the full self-attention operation of existing pretrained models, we pretrain it with the masked language modeling (MLM) objective, continuing from the RoBERTa (Liu et al., 2019) released checkpoint. After pretraining, we apply it to downstream language tasks through \ufb01netuning and demonstrate that Longformer consistently outper- forms RoBERTa on a wide range of document-level natural language tasks including text classi\ufb01cation, QA, and coreference"}, {"filename": "4.pdf", "page_number": 2, "content": "resolution, achieving state-of- the-art results on two of these datasets. We \ufb01nally introduce a variant of Longformer which instead of an encoder-only Transformer architecture, it follows an encoder-decoder ar- chitecture similar to the original Transformer model (Vaswani et al., 2017), and it is in- tended for sequence-to-sequence (seq2seq) learn- ing (Sutskever et al., 2014). We call this model Longformer-Encoder-Decoder (LED) that usesModel attention char-LM other pretrain matrix tasks Transformer-XL (2019) ltr yes no no Adaptive Span (2019) ltr yes no no Compressive (2020) ltr yes no no Reformer (2020) sparse yes no no Sparse (2019) sparse yes no no Routing (2020) sparse yes no no BP-Transformer (2019) sparse yes MT no Blockwise (2019) sparse no QA yes Our Longformer sparse yes multiple yes Table 1: Summary of prior work on adapting Trans- formers for long documents. ltr: left-to-right. Longformer\u2019s ef\ufb01cient attention pattern on the en- coder network, allowing it to address long"}, {"filename": "4.pdf", "page_number": 2, "content": "docu- ment seq2seq tasks such as summarization. We demonstrate the effectiveness of LED on the arXiv summarization dataset (Cohan et al., 2018). 2 Related Work Long-Document Transformers Tab. 1 summa- rizes recent prior work on long documents. Two types of self-attention approaches have been ex- plored. The \ufb01rst is a left-to-right (ltr) approach that processes the document in chunks moving from left-to-right. While such models have been success- ful in autoregressive language modeling, they are unsuitable for transfer learning approaches with tasks that bene\ufb01t from bidirectional context. Our work falls within the other general approach that de\ufb01nes some form of sparse attention pattern and avoids computing the full quadratic attention matrix multiplication. The model with the most similar attention pattern to ours is Sparse Trans- former (Child et al., 2019), which uses a form of dilated sliding window of blocks of size 8x8 pro- vided by BlockSparse (Gray et al., 2017). Our"}, {"filename": "4.pdf", "page_number": 2, "content": "implementation ( \u00a73) also includes a custom CUDA kernel, but it is more \ufb02exible and maintainable than BlockSparse which is implemented in C++, and designed for a speci\ufb01c version of TensorFlow. We also introduce additional task motivated global at- tention patterns suitable for common NLP tasks (\u00a73) and show they are essential for good perfor- mance in the transfer learning setting. A few models tried tasks other than autoregres- sive language modeling, which is a step forward because arguably focusing on language modeling as the primary evaluation has led to the develop- ment of models with limited applicability. BP- Transformer (Ye et al., 2019) evaluated on machine 2"}, {"filename": "4.pdf", "page_number": 3, "content": "(a) Full n2attention  (b) Sliding window attention  (c) Dilated sliding window  (d) Global+sliding window Figure 2: Comparing the full self-attention pattern and the con\ufb01guration of attention patterns in our Longformer. translation (MT), but didn\u2019t explore the pretrain- \ufb01netune setting. Blockwise attention (Qiu et al., 2019) pretrained their models and evaluated on question answering (QA). However, the evaluation is limited as it doesn\u2019t include language modeling, and the QA datasets are of relatively short docu- ments,2therefore the effectiveness of this model on long document tasks remains unexplored. Task-speci\ufb01c Models for Long Documents Many task-speci\ufb01c approaches have been devel- oped to workaround the 512 limit of pretrained transformer models like BERT. The simplest ap- proach just truncates the document, commonly used for classi\ufb01cation (Xie et al., 2019). An- other approach chunks the document into chunks of length 512 (could be overlapping), processes each chunk separately,"}, {"filename": "4.pdf", "page_number": 3, "content": "then combines the activa- tions with a task speci\ufb01c model (Joshi et al., 2019). A third approach popular for multihop and open domain QA tasks uses a two-stage model where the \ufb01rst stage retrieves relevant documents that are passed onto the second stage for answer extrac- tion (Clark and Gardner, 2017; Chen et al., 2017). All of these approaches suffer from information loss due to truncation or cascading errors from the two stage approach. In contrast, Longformer can process long sequences without truncating or chunking, allowing us to adopt a much simpler ap- proach that concatenates the available context and processes it in a single pass. A few contemporaneous works3have explored similar ideas to Longformer using local + global attention in Transformers, and pre-training it for long document natural language tasks. In particu- lar, ETC (Ainslie et al., 2020) uses a similar local + global attention instead of full self-attention to scale Transformers to long documents. Different from"}, {"filename": "4.pdf", "page_number": 3, "content": "Longformer, ETC uses relative position em- 2SQuAD contexts typically \ufb01t within the 512 limit, and MRQA is constructed by dropping long-document examples. 3All were published on arXiv after Longformer.beddings (which we only used for the Autoregres- sive LM setting), introduces an additional training objective (CPC loss) for pre-training, and con\ufb01g- ures global attention in a slightly different way. It shows strong results on several tasks including reading comprehension and classi\ufb01cation. GMAT (Gupta and Berant, 2020) uses a similar idea of few global locations in the input serving as global memory. BigBird (Zaheer et al., 2020) is an exten- sion over ETC with evaluation on additional tasks, including summarization. Importantly, through the- oretical analysis, BigBird shows that sparse Trans- formers are universal approximators of sequence functions and preserve these properties of the full self-attention. 3 Longformer The original Transformer model has a self-attention component with"}, {"filename": "4.pdf", "page_number": 3, "content": "O(n2)time and memory complex- ity where nis the input sequence length. To address this challenge, we sparsify the full self-attention matrix according to an \u201cattention pattern\u201d specify- ing pairs of input locations attending to one another. Unlike the full self-attention, our proposed atten- tion pattern scales linearly with the input sequence, making it ef\ufb01cient for longer sequences. This sec- tion discusses the design and implementation of this attention pattern. 3.1 Attention Pattern Sliding Window Given the importance of local context (Kovaleva et al., 2019), our attention pat- tern employs a \ufb01xed-size window attention sur- rounding each token. Using multiple stacked lay- ers of such windowed attention results in a large receptive \ufb01eld, where top layers have access to all input locations and have the capacity to build repre- sentations that incorporate information across the entire input, similar to CNNs (Wu et al., 2019). Given a \ufb01xed window size w, each token attends to1 2wtokens"}, {"filename": "4.pdf", "page_number": 3, "content": "on each side (Fig. 2b). The com- putation complexity of this pattern is O(n\u0002w), 3"}, {"filename": "4.pdf", "page_number": 4, "content": "which scales linearly with input sequence length n. In a transformer with `layers, the receptive \ufb01eld size at the top layer is `\u0002w(assuming wis \ufb01xed for all layers). Depending on the application, it might be helpful to use different values of wfor each layer to balance between ef\ufb01ciency and model representation capacity (\u00a74.1). Dilated Sliding Window To further increase the receptive \ufb01eld without increasing computation, the sliding window can be \u201cdilated\u201d. This is analogous to dilated CNNs (van den Oord et al., 2016) where the window has gaps of size dilation d(Fig. 2c). Assuming a \ufb01xed dandwfor all layers, the recep- tive \ufb01eld is `\u0002d\u0002w, which can reach tens of thousands of tokens even for small values of d. In multi-headed attention, each attention head computes a different attention score. We found set- tings with different dilation con\ufb01gurations per head improves performance by allowing some heads without dilation to focus on local context, while others with dilation focus on longer"}, {"filename": "4.pdf", "page_number": 4, "content": "context. Global Attention In state-of-the-art BERT-style models for natural language tasks, the optimal in- put representation differs from language modeling and varies by task. For masked language modeling (MLM), the model uses local context to predict the masked word, while for classi\ufb01cation, the model ag- gregates the representation of the whole sequence into a special token ( [CLS] in case of BERT). For QA, the question and document are concatenated, allowing the model to compare the question with the document through self-attention. In our case, the windowed and dilated attention are not \ufb02exible enough to learn task-speci\ufb01c repre- sentations. Accordingly, we add \u201cglobal attention\u201d on few pre-selected input locations. Importantly, we make this attention operation symmetric: that is, a token with a global attention attends to all tokens across the sequence, and all tokens in the sequence attend to it. Fig. 2d shows an example of a sliding window attention with global attention at a"}, {"filename": "4.pdf", "page_number": 4, "content": "few tokens at custom locations. For example for classi\ufb01cation, global attention is used for the [CLS] token while in QA global attention is pro- vided on all question tokens. Since the number of such tokens is small relative to and independent of nthe complexity of the combined local and global attention is still O(n). While specifying global attention is task speci\ufb01c, it is a easy way to add in- ductive bias to the model\u2019s attention, and it is muchsimpler than existing task speci\ufb01c approaches that use complex architecture to combine information across smaller input chunks. Linear Projections for Global Attention Re- call that given the linear projections Q,K,V, the Transformer model (Vaswani et al., 2017) computes attention scores as follows: Attention (Q; K; V ) = softmax\u0012QKT pdk\u0013 V(1) We use two sets of projections, Qs,Ks,Vsto com- pute attention scores of sliding window attention, andQg,Kg,Vgto compute attention scores for the global attention. The additional projections provide"}, {"filename": "4.pdf", "page_number": 4, "content": "\ufb02exibility to model the different types of attention, which we show is critical for best performance on downstream tasks. Qg,Kg,Vgare all initialized with values that match Qs,Ks,Vs. 3.2 Implementation In regular transformers, attention scores are com- puted as in Eqn. 1. The expensive operation is the matrix multiplication QKTbecause both Q andKhaven(sequence length) projections. For Longformer, the dilated sliding window attention computes only a \ufb01xed number of the diagonals of QKT. As shown in Fig. 1, this results in a linear increase in memory usage compared to quadratic increase for full self-attention. However, imple- menting it requires a form of banded matrix mul- tiplication that is not supported in existing deep learning libraries like PyTorch/Tensor\ufb02ow. Fig. 1 compares the performance of three different ways of implementing it: loop is a memory ef\ufb01cient Py- Torch implementation that supports dilation but is unusably slow and only used for testing; chunks only supports the"}, {"filename": "4.pdf", "page_number": 4, "content": "non-dilated case and is used for the pretraining/\ufb01netuning setting; and cuda is our fully functioning highly optimized custom CUDA kernel implemented using TVM (Chen et al., 2018) and used for the language modeling experiments (see Appendix A for more details). 4 Autoregressive Language Modeling Autoregressive or left-to-right language modeling is loosely de\ufb01ned as estimating the probability dis- tribution of an existing token/character given its previous tokens/characters in an input sequence. This task is considered one of the fundamental tasks in natural language and recent prior work on mod- eling long sequences using transformers has relied 4"}, {"filename": "4.pdf", "page_number": 5, "content": "on this task as their primary evaluation (Dai et al., 2019; Rae et al., 2020; Sukhbaatar et al., 2019). Similarly, we develop and evaluate our model on autoregressive language modeling. 4.1 Attention Pattern For autoregressive language modeling we use our dilated sliding window attention. Follow- ing Sukhbaatar et al. (2019) we use differing win- dow sizes across the layers. In particular, we use small window sizes for the lower layers and in- crease window sizes as we move to higher layers. This allows the top layers to learn higher-level rep- resentation of the entire sequence while having the lower layers capture local information. In addition, it provides balance between ef\ufb01ciency (smaller win- dow sizes are less computationally expensive due to fewer nonzero values) and performance (larger window sizes have richer representation power and often result in performance improvements). We do not use dilated sliding windows for lower layers to maximize their capacity to learn and uti-"}, {"filename": "4.pdf", "page_number": 5, "content": "lize the immediate local context. For the higher layers, we use a small amount of increasing dila- tion only on 2 heads. This gives the model the ability to directly attend to distant tokens without sacri\ufb01cing local context. 4.2 Experiment Setup To compare to prior work we focus on character- level LM ( text8 andenwik8 ; Mahoney, 2009). Training Ideally, we would like to train our model on the largest window size and sequence length we can \ufb01t in a modern GPU memory. How- ever, we found that the model needs a large number of gradient updates to learn the local context \ufb01rst, before learning to utilize longer context. To accom- modate this, we adopt a staged training procedure where we increase the attention window size and sequence length across multiple training phases. In particular, in the \ufb01rst phase we start with a short sequence length and window size, then on each sub- sequent phase, we double the window size and the sequence length, and halve the learning rate. This makes training"}, {"filename": "4.pdf", "page_number": 5, "content": "fast, while keeping the slow part (longest sequences and window sizes) to the end. We train the model over 5 total phases with start- ing sequence length of 2,048 and ending sequence length of 23,040 on the last phase (see Appendix B for detailed con\ufb01gurations of each phase, and for all other hyperparameters).Model #Param Dev Test Dataset text8 T12 (Al-Rfou et al., 2018) 44M - 1.18 Adaptive (Sukhbaatar et al., 2019) 38M 1.05 1.11 BP-Transformer (Ye et al., 2019) 39M - 1.11 Our Longformer 41M 1.04 1.10 Dataset enwik8 T12 (Al-Rfou et al., 2018) 44M - 1.11 Transformer-XL (Dai et al., 2019) 41M - 1.06 Reformer (Kitaev et al., 2020) - - 1.05 Adaptive (Sukhbaatar et al., 2019) 39M 1.04 1.02 BP-Transformer (Ye et al., 2019) 38M - 1.02 Our Longformer 41M 1.02 1.00 Table 2: Small model BPC on text8 &enwik8 Model #Param Test BPC Transformer-XL (18 layers) 88M 1.03 Sparse (Child et al., 2019) \u0019100M 0.99 Transformer-XL (24 layers) 277M 0.99 Adaptive (Sukhbaatar et al., 2019) 209M 0.98 Compressive"}, {"filename": "4.pdf", "page_number": 5, "content": "(Rae et al., 2020) 277M 0.97 Routing (Roy et al., 2020) \u0019223M 0.99 Our Longformer 102M 0.99 Table 3: Performance of large models on enwik8 Evaluation We evaluate with sequences of length 32,256. Following Dai et al. (2019), we split the dataset into overlapping sequences of size 32,256 with a step of size 512, and report the per- formance on the last 512 tokens on the sequence. 4.2.1 Results Tab. 2 and 3 summarize evaluation results on text8 andenwik8 datasets. We achieve a new state-of-the-art on both text8 andenwik8 using the small models with BPC of 1.10 and1.00 on text8 andenwik8 respectively, demonstrating the effectiveness of our model. For large models, given how expensive these experiments are, and following recent work (Ki- taev et al., 2020; Rae et al., 2020), we are only evaluating on enwik8 . Tab. 3 shows that Long- former outperforms the comparable Transformer- XL model, matches the performance of the compa- rable Sparse Transformer (Child et al., 2019), and matches or"}, {"filename": "4.pdf", "page_number": 5, "content": "slightly underperforms recent models that have more than twice the number of parameters. It is worth noting that Adaptive Span (Sukhbaatar et al., 2019) and Compressive Transformer (Rae et al., 2020) are not good \ufb01t for the pretraining- \ufb01netuning paradigm as discussed in \u00a72. 5"}, {"filename": "4.pdf", "page_number": 6, "content": "Model Dev BPC Decreasing w(from 512 to 32) 1.24 Fixed w(= 230) 1.23 Increasing w(from 32 to 512) 1.21 No Dilation 1.21 Dilation on 2 heads 1.20 Table 4: Top: changing window size across layers. Bot- tom: with/without dilation (@ 150K steps on phase1) 4.2.2 Ablation Study To show the importance of the design choices of our attention patterns, we tried different variants and report their controlled experiment results. To make the ablation study more manageable, we train each con\ufb01guration for 150K steps4with phase 1 con\ufb01guration on a small model on text8 , then report the BPC performance on the dev set. The top of Tab. 4 demonstrates the impact of different ways of con\ufb01guring the window sizes per layer. We observe that increasing the window size from the bottom to the top layer leads to the best performance, arranging them in the reverse way leads to worse performance, and using a \ufb01xed window size (the average of window sizes of the other con\ufb01guration) leads to a performance that it is in"}, {"filename": "4.pdf", "page_number": 6, "content": "between. The bottom of Tab. 4 shows the impact of adding dilation. Adding some dilation to two heads leads to some improvement compared with no dilation at all. 5 Pretraining and Finetuning Current state-of-the-art systems for many NLP tasks \ufb01netune a pretrained model with task super- vision (e.g. BERT). One of our main motivations is to develop such a model suitable for long docu- ment tasks. To do so, we pretrained Longformer on a document corpus and \ufb01netune it for six tasks, including classi\ufb01cation, QA and coreference resolu- tion. The resulting model can process sequences up to 4,096 tokens long (8 times longer than BERT)5. We pretrain Longformer with masked language modeling (MLM), where the goal is to recover randomly masked tokens in a sequence. Since MLM pretraining is expensive, we continue pre- training from the RoBERTa (Liu et al., 2019) re- leased checkpoint, while only making the minimal 4One caveat is that the ordering of end performance will not agree with that at step"}, {"filename": "4.pdf", "page_number": 6, "content": "150K. However, this approximation saves the huge cost of running every experiment to completion. 5Sequences up to 16K are possible on current GPUs.Model base large RoBERTa (seqlen: 512) 1.846 1.496 Longformer (seqlen: 4,096) 10.299 8.738 + copy position embeddings 1.957 1.597 + 2K gradient updates 1.753 1.414 + 65K gradient updates 1.705 1.358 Longformer (train extra pos. embed. only) 1.850 1.504 Table 5: MLM BPC for RoBERTa and various pre- trained Longformer con\ufb01gurations. changes necessary to support Longformer\u2019s atten- tion mechanism. Note that our attention pattern can be plugged into any pretrained transformer model without the need to change the model architecture. Attention Pattern We use sliding window atten- tion with window size of 512, therefore using the same amount of computation as RoBERTa.6 Position Embeddings RoBERTa uses learned absolute position embeddings with the maximum position being 512. To support longer documents, we add extra position embeddings to support up"}, {"filename": "4.pdf", "page_number": 6, "content": "to position 4,096. To leverage RoBERTa\u2019s pretrained weights, instead of randomly initializing the new position embeddings, we initialize them by copying the 512 position embeddings from RoBERTa mul- tiple times as analysis of BERT\u2019s attention heads shows a strong learned bias to attending to local context, including the previous or next token (Clark et al., 2019). Using the copy initialization preserves this local structure everywhere except at the parti- tion boundaries. Despite its simplicity, we found this to be a very effective (see Tab. 5), allowing Longformer pretraining to rapidly converge with a small number of gradient updates. Continued MLM Pretraining We pretrain Longformer using fairseq (Ott et al., 2019) on a corpus of long documents that we compiled (see Appendix C for corpus details). We train two model sizes, a base model and a large model. Both models are trained for 65K gradient updates with sequences length 4,096, batch size 64 ( 218tokens), maximum learning rate of"}, {"filename": "4.pdf", "page_number": 6, "content": "3e-5, linear warmup of 500 steps, followed by a power 3 polynomial decay. The rest of the hyperparameters are the same as RoBERTa. Tab. 5 shows the BPC on the development set of our training corpus. The \ufb01rst row shows a 1.846 6Adding dilation on a few heads as in \u00a74.1 hurt perfor- mance, likely because it is not compatible with the pretrained RoBERTa weights. Retraining such model from scratch might be needed to improve performance. 6"}, {"filename": "4.pdf", "page_number": 7, "content": "Wordpieces WH TQA HQA ON IMDB HY avg. 1,535 6,589 1,316 506 300 705 95th pctl. 3,627 17,126 1,889 1,147 705 1,975 Table 6: Average and 95th percentile of context length of datasets in wordpieces. WH: WikiHop, TQA: Triv- iaQA, HQA: HotpotQA, ON: OntoNotes, HY: Hyper- partisan news BPC using RoBERTa-base, which is comparable to the 1.880 BPC reported on the RoBERTa paper on their corpus. This indicates our training corpus is from a distribution close to that used to train RoBERTa. The following two rows show the per- formance of Longformer before pretraining with randomly initialized position embeddings and with copied position embeddings. The signi\ufb01cant differ- ence indicates the importance of the copy initial- ization, and the relative small difference between the RoBERTa BPC and the initialized BPC indi- cates that our sliding window attention is working well with the RoBERTa weights. The following two rows show the impact of continuing pretrain- ing. Traininig for 2K steps improves"}, {"filename": "4.pdf", "page_number": 7, "content": "BPC from 1.957 to 1.753, which further decreases to 1.705 af- ter 65K steps, demonstrating the model is learning to better utilize the sliding window attention and longer context. Similar patterns are observed with RoBERTa-large and Longformer-large. Frozen RoBERTa Weights We also pretrained Longformer while freezing all RoBERTa weights, and only training the new position embeddings. The motivation for this con\ufb01guration is to perfectly preserve the RoBERTa performance on short doc- uments. This con\ufb01guration has a BPC of 1.850 (down from 1.957 at initialization), but higher than 1.705 where all the weights are trainable. 6 Tasks We apply Longformer to multiple long document tasks, including QA, coreference resolution and classi\ufb01cation. Tab. 6 shows the evaluation datasets have contexts signi\ufb01cantly longer than 512 word- pieces. Our primary goal is to evaluate whether our attention mechanism can act as a replace- ment for the standard self-attention mechanism in BERT style models, and to"}, {"filename": "4.pdf", "page_number": 7, "content": "perform controlled tri- als against a strong baseline. We are also interested in evaluating whether we can replace complicated task speci\ufb01c models necessitated by BERT\u2019s lim- ited context with simpler models that just concate-nate all available context into a single sequence. Our baseline is a RoBERTa based model that breaks the context into the longest possible seg- ment, passes each individually through RoBERTa, and concatenates the activations for further process- ing. For QA tasks, we also concatenate the question to each segment so that RoBERTa can condition it\u2019s contextual representations of the context on the question. The Longformer variant replaces the RoBERTa self-attention mechanism with our win- dowed attention used during pretraining, plus a task motivated global attention. The global attention uses additional linear projections (\u00a73.1). 6.1 Question answering We used three datasets: WikiHop (Welbl et al., 2018), TriviaQA (Joshi et al., 2017, Wikipedia set- ting), and"}, {"filename": "4.pdf", "page_number": 7, "content": "HotpotQA, (Yang et al., 2018, distractor setting).7 For WikiHop and TriviaQA we follow the sim- ple QA model of BERT (Devlin et al., 2019), and concatenate question and documents into one long sequence, run it through Longformer, then have a dataset-speci\ufb01c prediction layer. WikiHop uses a classi\ufb01cation layer for the candidate while Trivi- aQA uses the loss function of Clark and Gardner (2017) to predict answer span. We include global attention to question tokens and answer candidates for WikiHop and to question tokens for TriviaQA. HotpotQA is a multihop QA dataset that involves extracting answer spans and evidence sentences from 10 Wikipedia paragraphs, 2 of which are rele- vant and the rest are distractors. We use a two-stage model that \ufb01rst selects the most relevant paragraphs then passes them to a second stage for answer ex- traction. Both stages concatenate question and con- text into one sequence, run it through Longformer, then use task-speci\ufb01c prediction layers. We train the"}, {"filename": "4.pdf", "page_number": 7, "content": "models in a multi-task way to predict relevant paragraphs, evidence sentences, answer spans and question types (yes/no/span) jointly. Note that this model is simpler than recent SOTA models that in- clude complex task-speci\ufb01c architectures (e.g., (Tu et al., 2019; Chen et al., 2019; Tu et al., 2020; Groeneveld et al., 2020)). See Appendix D for fur- ther details about the models and hyperparameters. 6.2 Coreference Resolution We use OntoNotes (Pradhan et al., 2012), and the model from Joshi et al. (2019), a modi\ufb01cation of 7We use the full version of TriviaQA and HotpotQA, not the simpli\ufb01ed versions in MRQA (Fisch et al., 2019). 7"}, {"filename": "4.pdf", "page_number": 8, "content": "QA Coref. Classi\ufb01cation Model WikiHop TriviaQA HotpotQA OntoNotes IMDB Hyperpartisan RoBERTa-base 72.4 74.3 63.5 78.4 95.3 87.4 Longformer-base 75.0 75.2 64.4 78.6 95.7 94.8 Table 7: Summary of \ufb01netuning results on QA, coreference resolution, and document classi\ufb01cation. Results are on the development sets comparing our Longformer-base with RoBERTa-base. TriviaQA, Hyperpartisan metrics are F1, WikiHop and IMDB use accuracy, HotpotQA is joint F1, OntoNotes is average F1. the system from Lee et al. (2018) to replace ELMo with BERT. The Longformer system is a straightfor- ward adaption of the baseline model by replacing RoBERTa with Longformer and extending the se- quence length. We didn\u2019t use global attention for this task. 6.3 Document Classi\ufb01cation We evaluate on IMDB (Maas et al., 2011) and Hy- perpartisan news detection (Kiesel et al., 2019) datasets.8IMDB is a standard sentiment classi\ufb01ca- tion datasets consisting of movie reviews. While most documents in this dataset are short,"}, {"filename": "4.pdf", "page_number": 8, "content": "about 13.6% of them are larger than 512 wordpieces (Tab. 6). Documents in Hyperpartisan are relatively long, and it is small with only 645 documents mak- ing it a good test for Longformer\u2019s ability to adapt to limited data. We use global attention on the [CLS] token. 6.4 Results Main Result Tab. 7 summarizes the results of all our \ufb01netuning experiments. We observe that Long- former consistently outperforms the RoBERTa baseline. Its performance gain is especially ob- vious for tasks that require long context such as WikiHop and Hyperpartisan. For TriviaQA, the improvement is more modest as the local context is often suf\ufb01cient to answer the question. In the case of HotpotQA, the supporting fact auxiliary supervision allows models to easily \ufb01nd relevant contexts and then focus on local context, leading to smaller gains. This is contrasted with WikiHop that only includes distant supervision of intermediate reasoning chains, where our approach excels by reasoning over the entire context. On"}, {"filename": "4.pdf", "page_number": 8, "content": "the IMDB and OntoNotes datasets the performance gains are smaller. For IMDB, the majority of the dataset consists of short documents and thus it is expected to see smaller improvements. For OntoNotes, we 8For Hyperpartisan we split the training data into 80/10/10 train/dev/test sets, and report mean F1 across \ufb01ve seeds.Model WikiHop TriviaQA HotpotQA Current\u0003SOTA 78.3 73.3 74.2 Longformer-large 81.9 77.3 73.2 Table 8: Leaderboard results of Longformer-large at time of submission (May 2020). All numbers are F1 scores. found that the distance between any two mentions is typically quite small so that a baseline that pro- cesses smaller chunks separately is able to stitch together mentions into coreference chains without considering cross chunk interactions. Longformer-large for QA We also evaluate the performance of Longformer-large on long context QA tasks. Tab. 8 shows that our Longformer-large achieves new state-of-the-art results9on WikiHop and TriviaQA by large margins (3.6 and 4"}, {"filename": "4.pdf", "page_number": 8, "content": "points respectively), and for HotpotQA, it underperforms the current state-of-the-art (Fang et al., 2020) by a point. Tab. 9 shows the detailed results of Hot- potQA compared with published and unpublished concurrent models. Longformer places second on the published leaderboard, outperforming all other published results except for HGN (Fang et al., 2020). All published top performing models in this task (Tu et al., 2019; Fang et al., 2020; Shao et al., 2020) use GNNs (Kipf and Welling, 2017) or graph network of entities, which seem to encode an important inductive bias for the task and can po- tentially improve our results further. Nevertheless, Longformer performs strongly outperforming all other methods including the recent non-GNN meth- ods (Gla\u00df et al., 2019; Shao et al., 2020; Groen- eveld et al., 2020). 8"}, {"filename": "4.pdf", "page_number": 9, "content": "Model ans. supp. joint TAP 2 (ensemble) (Gla\u00df et al., 2019) 79.8 86.7 70.7 SAE (Tu et al., 2019) 79.6 86.7 71.4 Quark (dev) (Groeneveld et al., 2020) 81.2 87.0 72.3 C2F Reader (Shao et al., 2020) 81.2 87.6 72.8 Longformer-large 81.3 88.3 73.2 ETC-largey(Ainslie et al., 2020) 81.2 89.1 73.6 GSAN-largey81.6 88.7 73.9 HGN-large (Fang et al., 2020) 82.2 88.5 74.2 Table 9: HotpotQA results in distractor setting test set. Quark\u2019s test results are not available. All numbers are F1 scores.yshows contemporaneous leaderboard sub- missions. Model Accuracy / \u0001 Longformer (seqlen: 4,096) 73.8 RoBERTa-base (seqlen: 512) 72.4 / -1.4 Longformer (seqlen: 4,096, 15 epochs) 75.0 / +1.2 Longformer (seqlen: 512, attention: n2) 71.7 / -2.1 Longformer (seqlen: 2,048) 73.1 / -0.7 Longformer (no MLM pretraining) 73.2 / -0.6 Longformer (no linear proj.) 72.2 / -1.6 Longformer (no linear proj. no global atten.) 65.5 / -8.3 Longformer (pretrain extra position embed. only) 73.5 / -0.3 Table 10: WikiHop development"}, {"filename": "4.pdf", "page_number": 9, "content": "set ablations 6.5 Ablations on WikiHop Tab. 10 presents an ablation study for WikiHop on the development set. All results use Longformer- base, \ufb01ne-tuned for \ufb01ve epochs with identical hy- perparameters except where noted. Longformer bene\ufb01ts from longer sequences, global attention, separate projection matrices for global attention, MLM pretraining, and longer training. In addition, when con\ufb01gured as in RoBERTa-base (seqlen: 512, andn2attention) Longformer performs slightly worse then RoBERTa-base, con\ufb01rming that per- formance gains are not due to additional pretrain- ing. Performance drops slightly when using the RoBERTa model pretrained when only unfreezing the additional position embeddings, showing that Longformer can learn to use long range context in task speci\ufb01c \ufb01ne-tuning with large training datasets such as WikiHop. 9At submission time, May 2020. Later, BigBird (Zaheer et al., 2020) improved leaderboard results on these datasets. There are confounding factors such as using 16X"}, {"filename": "4.pdf", "page_number": 9, "content": "more com- pute in BigBird\u2019s pretraining compared with Longformer, po- tentially affecting the performance.7 Longformer-Encoder-Decoder (LED) The original Transformer (Vaswani et al., 2017) consisted of an encoder-decoder architecture, in- tended for sequence-to-sequence tasks (Sutskever et al., 2014), such as summarization and transla- tion. While encoder-only Transformers are effec- tive on a variety of NLP tasks, pre-trained encoder- decoder Transformer models (e.g. BART (Lewis et al., 2020) and T5 (Raffel et al., 2020)) have achieved strong results on tasks like summariza- tion. Yet, such models can\u2019t ef\ufb01ciently scale to seq2seq tasks with longer inputs. To facilitate modeling long sequences for seq2seq learning, we propose a Longformer variant that has both the encoder and decoder Transformer stacks but instead of the full self-attention in the encoder, it uses the ef\ufb01cient local+global attention pattern of the Longformer. The decoder uses the full self-attention to the entire"}, {"filename": "4.pdf", "page_number": 9, "content": "encoded tokens and to previously decoded locations. We call this model Longformer-Encoder-Decoder (LED) which scales linearly with the input. Since pre-training LED is expensive, we initialize LED parameters from the BART, and follow BART\u2019s exact architecture in terms of number of layers and hidden sizes. The only difference is that to process longer inputs, we extend position embedding to 16K tokens (up from BART\u2019s 1K tokens) and we initialize the new position embedding matrix by repeatedly copying BART\u2019s 1K position embeddings 16 times as in Section 5 for RoBERTa. Following BART, we re- lease two model sizes, LED-base and LED-large, which respectively have 6 and 12 layers in both encoder and decoder stacks. We evaluate LED on the summarization task us- ing the arXiv summarization dataset (Cohan et al., 2018) which focuses on long document summariza- tion in the scienti\ufb01c domain. The 90th percentile of document lengths is 14.5K tokens, making it an appropriate testbed for evaluating"}, {"filename": "4.pdf", "page_number": 9, "content": "LED. LED\u2019s encoder reads the document and its decoder gener- ates the output summary. The encoder uses local attention with window size 1,024 tokens and global attention on the \ufb01rst <s> token. The decoder uses full attention to the entire encoder and previously decoded locations. As standard in seq2seq models, LED is trained using teacher forcing on gold train- ing summaries and uses beam search at inference. Tab. 11 demonstrates the results of LED-large 16K on the arXiv summarization task. This model is merely initialized from BART, with no additional 9"}, {"filename": "4.pdf", "page_number": 10, "content": "R-1 R-2 R-L Discourse-aware (2018) 35.80 11.05 31.80 Extr-Abst-TLM (2020) 41.62 14.69 38.03 Dancer (2020) 42.70 16.54 38.44 Pegasus (2020) 44.21 16.95 38.83 LED-large (seqlen: 4,096) (ours) 44.40 17.94 39.76 BigBird (seqlen: 4,096) (2020) 46.63 19.02 41.77 LED-large (seqlen: 16,384) (ours) 46.63 19.62 41.83 Table 11: Summarization results of Longformer- Encoder-Decoder (LED) on the arXiv dataset. Met- rics from left to right are ROUGE-1, ROUGE-2 and ROUGE-L. 1K 4k 16k1015202530354045 35.2144.4846.23 11.5417.9919.62R1 R2 Figure 3: ROUGE-1 and ROUGE-2 of LED when vary- ing the input size (arXiv validation set). pre-training. We observe that LED achieves state- of-the-art results on arXiv, slightly outperform- ing BigBird (Zaheer et al., 2020). Note that the BigBird summarization model supports sequence length of 4K tokens but starts from and continues pre-training Pegasus (Zhang et al., 2020), a model speci\ufb01cally designed and pre-trained for summa- rization. With no pre-training or task-"}, {"filename": "4.pdf", "page_number": 10, "content": "speci\ufb01c ini- tialization, but with ability to process longer inputs, LED can slightly outperform BigBird. Further im- provements should be possible through pre-training of LED. Fig. 3 further illustrates the importance of sequence length showing the ablility to process longer input signi\ufb01cantly improves the results. 8 Conclusion and Future Work We present Longformer, a transformer-based model that is scalable for processing long documents and that makes it easy to perform a wide range of document-level NLP tasks without chunk- ing/shortening the long input and without com- plex architecture to combine information across these chunks. Longformer employs an attention pattern that combines local and global information while also scaling linearly with the sequence length. Longformer achieves state-of-the-art results on the character-level language modeling tasks of text8andenwik8 . When pretrained, Longformer con- sistently outperforms RoBERTa on long document tasks and sets new state-of-"}, {"filename": "4.pdf", "page_number": 10, "content": "the-art results on Wik- iHop and TriviaQA. We further present LED, an encoder-decoder variant of Longformer for model- ing sequence-to-sequence tasks, and achieve state- of-the-art results on the arXiv long document sum- marization task. For future work, we would like to study other pretraining objectives, especially for LED, increase the sequence length, and explore other tasks that might bene\ufb01t from our model. Acknowledgment We would like to thank Noah Smith, Dan Weld, Dirk Groeneveld, Kyle Lo, Daniel King and Doug Downey for helpful discussions and feedback, and the AI2 infrastructure team for technical support. References Joshua Ainslie, Santiago Ontanon, Chris Alberti, Va- clav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. 2020. ETC: Encoding long and structured inputs in transformers. In Proceedings of the 2020 Con- ference on Empirical Methods in Natural Language Processing (EMNLP) , pages 268\u2013284, Online. Asso- ciation for"}, {"filename": "4.pdf", "page_number": 10, "content": "Computational Linguistics. Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. 2018. Character-level lan- guage modeling with deeper self-attention. In AAAI . Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading wikipedia to answer open- domain questions. In ACL. Jifan Chen, Shih-Ting Lin, and Greg Durrett. 2019. Multi-hop question answering via reasoning chains. arXiv preprint , abs/1910.02610. Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, et al. 2018. TVM: An automated end-to-end optimizing com- piler for deep learning. In OSDI . Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016. Training deep nets with sublinear memory cost. arXiv preprint , abs/1604.06174. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long se- quences with sparse transformers. arXiv preprint , abs/1904.10509. Christopher Clark and Matt Gardner. 2017."}, {"filename": "4.pdf", "page_number": 10, "content": "Simple and effective multi-paragraph reading comprehen- sion. In ACL. 10"}, {"filename": "4.pdf", "page_number": 11, "content": "Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. 2019. What does bert look at? an analysis of bert\u2019s attention. arXiv preprint , abs/1906.04341. Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian. 2018. A discourse-aware attention model for abstractive summarization of long documents. In NAACL-HLT 2018 . Andrew M Dai and Quoc V Le. 2015. Semi-supervised sequence learning. In NeurIPS . Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Car- bonell, Quoc V . Le, and Ruslan Salakhutdinov. 2019. Transformer-XL: Attentive language models beyond a \ufb01xed-length context. In ACL. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In NAACL-HLT . Yuwei Fang, Siqi Sun, Zhe Gan, Rohit Pillai, Shuo- hang Wang, and Jingjing Liu. 2020. Hierarchical graph network for multi-hop question answering. In Proceedings of the 2020"}, {"filename": "4.pdf", "page_number": 11, "content": "Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 8823\u20138838, Online. Association for Computa- tional Linguistics. Adam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eu- nsol Choi, and Danqi Chen. 2019. MRQA 2019 shared task: Evaluating generalization in reading comprehension. In MRQA workshop at EMNLP . Alexios Gidiotis and Grigorios Tsoumakas. 2020. A divide-and-conquer approach to the summarization of academic articles. ArXiv , abs/2004.06190. Michael Gla\u00df, Al\ufb01o Massimiliano Gliozzo, Rishav Chakravarti, Anthony Ferritto, Lin Pan, Gaudani Bhargav, Dinesh Garg, and Avirup Sil. 2019. Span selection pre-training for question answering. arXiv preprint , abs/1909.04120. Scott Gray, Alec Radford, and Diederik P. Kingma. 2017. Gpu kernels for block-sparse weights. Dirk Groeneveld, Tushar Khot, Mausam, and Ashish Sabhwaral. 2020. A simple yet strong pipeline for HotpotQA. arXiv preprint , abs/2004.06753. Ankit Gupta and Jonathan Berant. 2020. Gmat: Global memory"}, {"filename": "4.pdf", "page_number": 11, "content": "augmentation for transformers. ArXiv , abs/2006.03274. Jeremy Howard and Sebastian Ruder. 2018. Universal language model \ufb01ne-tuning for text classi\ufb01cation. In ACL. Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. 2017. TriviaQA: A large scale dis- tantly supervised challenge dataset for reading com- prehension. In ACL.Mandar Joshi, Omer Levy, Luke Zettlemoyer, and Daniel Weld. 2019. BERT for coreference resolu- tion: Baselines and analysis. In EMNLP-IJCNLP . Johannes Kiesel, Maria Mestre, Rishabh Shukla, Em- manuel Vincent, Payam Adineh, David Corney, Benno Stein, and Martin Potthast. 2019. SemEval- 2019 task 4: Hyperpartisan news detection. In Proceedings of the 13th International Workshop on Semantic Evaluation , pages 829\u2013839, Minneapo- lis, Minnesota, USA. Association for Computational Linguistics. Thomas N Kipf and Max Welling. 2017. Semi- supervised classi\ufb01cation with graph convolutional networks. ICLR . Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. 2020."}, {"filename": "4.pdf", "page_number": 11, "content": "Reformer: The ef\ufb01cient transformer. In ICLR . Olga V . Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. 2019. Revealing the dark secrets of bert. In EMNLP/IJCNLP . Kenton Lee, Luheng He, and Luke Zettlemoyer. 2018. Higher-order coreference resolution with coarse-to- \ufb01ne inference. In NAACL . Mike Lewis, Yinhan Liu, Naman Goyal, Mar- jan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre- training for natural language generation, translation, and comprehension. In Proceedings of the 58th An- nual Meeting of the Association for Computational Linguistics , pages 7871\u20137880, Online. Association for Computational Linguistics. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A robustly optimized bert pretraining ap- proach. arXiv preprint , abs/1907.11692. Andrew L. Maas, Raymond E. Daly, Peter T."}, {"filename": "4.pdf", "page_number": 11, "content": "Pham, Dan Huang, Andrew Y . Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analy- sis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Hu- man Language Technologies , pages 142\u2013150, Port- land, Oregon, USA. Association for Computational Linguistics. Matt Mahoney. 2009. Large text compression bench- mark. A\u00a8aron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew W. Senior, and Koray Kavukcuoglu. 2016. Wavenet: A generative model for raw audio. In SSW. Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. fairseq: A fast, extensible 11"}, {"filename": "4.pdf", "page_number": 12, "content": "toolkit for sequence modeling. In Proceedings of NAACL-HLT 2019: Demonstrations . Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word repre- sentations. In NAACL . Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Olga Uryupina, and Yuchen Zhang. 2012. CoNLL- 2012 shared task: Modeling multilingual unre- stricted coreference in OntoNotes. In Joint Confer- ence on EMNLP and CoNLL - Shared Task , pages 1\u201340, Jeju Island, Korea. Association for Computa- tional Linguistics. Jiezhong Qiu, Hao Ma, Omer Levy, Scott Yih, Sinong Wang, and Jie Tang. 2019. Blockwise self-attention for long document understanding. arXiv preprint , abs/1911.02972. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. Jack W. Rae, Anna Potapenko, Siddhant M. Jayaku- mar, and Timothy P. Lillicrap. 2020. Compressive transformers for long-"}, {"filename": "4.pdf", "page_number": 12, "content": "range sequence modelling. In ICLR . Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, W. Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a uni\ufb01ed text-to-text trans- former. J. Mach. Learn. Res. , 21:140:1\u2013140:67. Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. 2020. Ef\ufb01cient content-based sparse attention with routing transformers. arXiv preprint , abs/2003.05997. Nan Shao, Yiming Cui, Ting Liu, Shijin Wang, and Guoping Hu. 2020. Is graph structure neces- sary for multi-hop reasoning? arXiv preprint , abs/2004.03096. Sandeep Subramanian, Raymond Li, Jonathan Pilault, and C. Pal. 2020. On extractive and abstractive neu- ral document summarization with transformer lan- guage models. In EMNLP . Sainbayar Sukhbaatar, Edouard Grave, Piotr Bo- janowski, and Armand Joulin. 2019. Adaptive at- tention span in transformers. In ACL. Ilya Sutskever, Oriol Vinyals, and Quoc V . Le. 2014. Sequence to"}, {"filename": "4.pdf", "page_number": 12, "content": "sequence learning with neural networks. InNIPS . Trieu H. Trinh and Quoc V . Le. 2018. A simple method for commonsense reasoning. arXiv preprint , abs/1806.02847. Ming Tu, Jinke Huang, Xiaodong He, and Bowen Zhou. 2020. Graph sequential network for reasoning over sequences. In NeurIPS Graph Representation Learning workshop .Ming Tu, Kevin Huang, Guangtao Wang, Jing Huang, Xiaodong He, and Bufang Zhou. 2019. Select, an- swer and explain: Interpretable multi-hop reading comprehension over multiple documents. arXiv preprint , abs/1911.00484. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In NIPS . Johannes Welbl, Pontus Stenetorp, and Sebastian Riedel. 2018. Constructing datasets for multi-hop reading comprehension across documents. TACL , 6:287\u2013302. Felix Wu, Angela Fan, Alexei Baevski, Yann Dauphin, and Michael Auli. 2019. Pay less attention with lightweight and dynamic"}, {"filename": "4.pdf", "page_number": 12, "content": "convolutions. arXiv preprint , abs/1901.10430. Qizhe Xie, Zihang Dai, Eduard H. Hovy, Minh-Thang Luong, and Quoc V . Le. 2019. Unsupervised data augmentation for consistency training. arXiv preprint , abs/1904.12848. Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shu xin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Li-Wei Wang, and Tie-Yan Liu. 2020. On layer normalization in the transformer architecture. arXiv preprint , abs/2002.04745. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben- gio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In EMNLP . Zihao Ye, Qipeng Guo, Quan Gan, Xipeng Qiu, and Zheng Zhang. 2019. BP-Transformer: Modelling long-range context via binary partitioning. arXiv preprint , abs/1911.04070. Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, C. Alberti, S. Onta \u02dcn\u00b4on, Philip Pham, Anirudh Ravula, Qifan Wang, L. Yang, and A. Ahmed. 2020. Big"}, {"filename": "4.pdf", "page_number": 12, "content": "bird: Transformers for longer sequences. ArXiv , abs/2007.14062. Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. 2019. Defending against neural fake news. In NeurIPS . Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J Liu. 2020. Pegasus: Pre-training with ex- tracted gap-sentences for abstractive summarization. ICML . Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. ICCV , pages 19\u201327. 12"}, {"filename": "4.pdf", "page_number": 13, "content": "A Implementation Details Implementing Longformer\u2019s dilated sliding win- dow attention requires a form of banded matrix multiplication (matrix multiplication where the out- put is all zero except certain diagonals) that is not directly supported in existing deep learning libraries like PyTorch/Tensor\ufb02ow. Fig. 1 compares the runtime and memory of three different ways of implementing it. Longformer-loop is a naive implementation that computes each diagonal separately in a loop. It is memory ef\ufb01cient because it only computes the non-zero values, but it is unusably slow. We only use it for testing because it is easy to implement but don\u2019t use it to run experiments. Longformer-chunks only supports the non- dilated case. It chunks QandKinto overlapping blocks of size wand overlap of size1 2w, multiplies the blocks, then mask out the diagonals. This is very compute ef\ufb01cient because it uses a single ma- trix multiplication operation from PyTorch, but it consumes 2x the amount of memory a"}, {"filename": "4.pdf", "page_number": 13, "content": "perfectly op- timized implementation should consume because it computes some of the zero values. Because of the compute ef\ufb01ciency, this implementation is most suitable for the pretrain/\ufb01netune case. We didn\u2019t \ufb01nd the increase in memory to be a problem for this setting. Longformer-cuda is a custom CUDA kernel that we implement using TVM (Chen et al., 2018). It is a fully functioning implementation of our at- tention (not limited as Longformer-chunks ), it is the most memory ef\ufb01cient, and it is as fast as the highly optimized full self-attention.10We mainly use this implementation for the autoregres- sive language modeling experiments because of the memory ef\ufb01ciency (allows the longest sequences) and the support of dilation (needed for character- LM experiments). Tensor Virtual Machine (TVM) We build our custom CUDA kernel using TVM (Chen et al., 2018), a deep learning compiler stack that compiles high level description of a function into optimized device-speci\ufb01c code. Using TVM, we"}, {"filename": "4.pdf", "page_number": 13, "content": "describe our banded matrix multiplication in high-level python 10It is worth noting that theoretically, a perfectly optimized Longformer-cuda should be faster than the n2computa- tion. However, achieving this level of performance requires special knowledge of low-level GPU programming, similar to implementing a highly optimized matrix multiplication. Our current implementation is suf\ufb01ciently fast and practical to use.constructs, then TVM generates the corresponding CUDA code and compiles it for GPUs. B Character LM Hyperparameters We evaluate on text8 andenwik8 , both contain 100M characters from Wikipedia split into 90M, 5M, 5M for train, dev, test. Our model only speci- \ufb01es how the self-attention component works, and it is agnostic to the other design choices for the trans- former model. Our implementation is based on the Transformer-XL (Dai et al., 2019) code11with the memory mechanism disabled. We use relative posi- tion embeddings with sinusoidal weights as in Dai et al. (2019)."}, {"filename": "4.pdf", "page_number": 13, "content": "We use two different model sizes, a small (12 layers, 512 hidden size) model as in Dai et al. (2019), and a large (30 layers, 512 hidden size) model as in Child et al. (2019). We employed mixed precision training (\ufb02oating points 16 and 32) using apex12to reduce memory consumption and speed-up training. However, we kept the attention computation in fp32 to avoid numerical instability issues.13We used gradient checkpointing (Chen et al., 2016) to reduce memory usage, and ran our experiments on 48GB RTX8000 GPUs. All hyper- parameters and stage con\ufb01gurations are listed in Tab. 12. Our CUDA kernel supports the autoregres- sive mode where each token attends to a window of previous tokens only. Our implementation also in- cludes a version of the relative position embedding that is compatible with our dilated sliding window attention. We ran the small model experiments on 4 RTX8000 GPUs for 16 days. For the large model, we ran experiments on 8 RTX8000 GPUs for 13 days. Most of our"}, {"filename": "4.pdf", "page_number": 13, "content": "hyperparameter search is similar to the ablation in Tab. 4 where we run the con\ufb01gu- ration for 150K steps on text8 . We experimented with absolute position embeddings and learned po- sition embeddings, dropout values of [0.1, 0.2] (small model) and [0.1, 0.4] (large model), pre- layernorm and post-layernorm (Xiong et al., 2020), learning rate (LR) of phase1 of values [2.5e-5, 5e- 4, 1e-4] constant and cosine LR schedules, and different con\ufb01gurations for dilation (on all heads, on 2 heads, no dilation). Number of gradient up- dates/phase reported in Tab. 12 is determined by running each phase until the validation BPC stops 11https://github.com/kimiyoung/ transformer-xl 12https://github.com/NVIDIA/apex 13We found that using fp16 in attention operation results in \ufb02oating point over\ufb02ow and NaNs in later stages of training. 13"}, {"filename": "4.pdf", "page_number": 14, "content": "getting better. C Pretraining Data In order to allow the model to learn long depen- dencies in pretraining, we compiled a corpus of long documents. Some of these data sources were also included in the original RoBERTa pretraining including the Books corpus (Zhu et al., 2015) plus English Wikipedia. We additionally included one third of a subset of the Realnews dataset (Zellers et al., 2019) with documents longer than 1,200 to- kens as well as one third of the Stories (Trinh and Le, 2018) corpus. Our goal was to include a mix of long and short documents to both allow the model to learn longer dependencies while not to forget in- formation from the original RoBERTa pretraining. The statistics of the pretraining data is shown in Tab. 13. D Task speci\ufb01c model details All the QA and classi\ufb01cation models are imple- mented using PyTorch-Lightning14. We use the of\ufb01cial train/dev/test splits of all datasets except for the Hyperpartisan news which we randomely split into 80/10/10 for"}, {"filename": "4.pdf", "page_number": 14, "content": "train/dev/test. WikiHop Instances in WikiHop consist of: a question, answer candidates (ranging from two candidates to 79 candidates), supporting contexts (ranging from three paragraphs to 63 paragraphs), and the correct answer. The dataset does not pro- vide any intermediate annotation for the multihop reasoning chains, requiring models to instead infer them from the indirect answer supervision. To prepare the data for input to Longformer and RoBERTa, we \ufb01rst tokenize the question, answer candidates, and support contexts using RoBERTa\u2019s wordpiece tokenizer. Then we concatenate the question and answer candi- dates with special tokens as [q] question [/q] [ent] candidate1 [/ent] ... [ent] candidateN [/ent] . The contexts are also concatenated using RoBERTa\u2019s doc- ument delimiter tokens as separators: </s> context1 </s> ... </s> contextM </s> . The special tokens [q], [/q], [ent], [/ent] were added to the RoBERTa vocabulary and randomly initialized before task \ufb01netuning."}, {"filename": "4.pdf", "page_number": 14, "content": "14https://github.com/PyTorchLightning/ pytorch-lightningAfter preparing the input data, we compute acti- vations from the top layer of each model as follows. We take the question and answer candidates and concatenate them to as much context as possible up to the model sequence length (512 for RoBERTa, 4,096 for Longformer), run the sequence through the model, collect the output activations, and repeat until all of the context is exhausted (for all models except Longformer-large, where we just include the \ufb01rst 4,096 length sequence due to memory re- quirements). Then all activations for all chunks are concatenated into one long sequence. In the case of Longformer, we use global attention to the entire question and answer candidate sequence. For prediction, we attach a linear layer to each [ent] that outputs a single logit, average over all logits for each candidate across the chunks, apply a softmax and use the cross entropy loss with the correct answer candidate. Training used the Adam"}, {"filename": "4.pdf", "page_number": 14, "content": "optimizer with linear warmup over 200 gradient updates to a maximum LR, and linear decay over the remainder of training. We used gradient accumulation to effective batch size of 32 instances, checking the development ac- curacy every 250 gradient updates and reported the maximum development accuracy. Other hyperpa- rameters (dropout, weight decay) were identical to RoBERTa pretraining. In general, we ran minimal hyperparameter trials, but for fair comparison between Longformer and RoBERTa ran an identical hyperparameter search with Longformer-base and RoBERTa-base. This consisted of a grid search of LR in [2e-5, 3e-5, 5e-5] and number epochs in [5, 10, 15]. The best Longformer-base con\ufb01guration used lr=3e-5, 15 epochs. We ran two hyperparameter trials for Longformer-large, lr=3e-5 and number epochs in [5, 15] (the 5 epoch model had higher dev accuracy of 77.6, and was the single model submitted to the public leaderboard for test set evaluation). All mod- els were trained on a single"}, {"filename": "4.pdf", "page_number": 14, "content": "RTX8000 GPU, with Longformer-base taking about a day for 5 epochs. TriviaQA TriviaQA has more than 100K ques- tion, answer, document triplets for training. Doc- uments are Wikipedia articles, and answers are named entities mentioned in the article. The span that answers the question is not annotated, but it is found using simple text matching. Similar to WikiHop, we tokenize the question and the document using RoBERTa\u2019s tokenizer, then form the input as [s] question [/s] 14"}, {"filename": "4.pdf", "page_number": 15, "content": "Param Value Position Embeddings Relative and Sinusoidal as in Dai et al. (2019) Small model con\ufb01g 12 layers, 8 heads, 512 hidden size as in Dai et al. (2019) Large model con\ufb01g 30 layers, 8 heads, 512 hidden size as in Child et al. (2019) Optimizer AdamW Dropout 0.2 (small model), 0.4 (large model) Gradient clipping 0.25 Weight Decay 0.01 Layernorm Location pre-layernorm (Xiong et al., 2020) Activation GeLU Number of phases 5 Phase 1 window sizes 32 (bottom layer) - 8,192 (top layer) Phase 5 window sizes 512 (bottom layer) - (top layer) Phase 1 sequence length 2,048 Phase 5 sequence length 23,040 (gpu memory limit) Phase 1 LR 0.00025 Phase 5 LR 000015625 Batch size per phase 32, 32, 16, 16, 16 #Steps per phase (small) 430K, 50k, 50k, 35k, 5k #Steps per phase (large) 350K, 25k, 10k, 5k, 5k Warmup 10% of the phase steps with maximum 10K steps LR scheduler constant throughout each phase Dilation (small model) 0 (layers 0-5), 1 (layers 6-7), 2 (layers 8-9), 3 (layers 10-11) Dilation (large"}, {"filename": "4.pdf", "page_number": 15, "content": "model) 0 (layers 0-14), 1 (layers 15-19), 2 (layers 20-24), 3 (layers 25-29) Dilation heads 2 heads only Table 12: Hyperparameters for the best performing model for character-level language modeling Source Tokens Avg doc len Books (Zhu et al., 2015) 0.5B 95.9K English Wikipedia 2.1B 506 Realnews (Zellers et al., 2019) 1.8B 1.7K Stories (Trinh and Le, 2018) 2.1B 7.8K Table 13: Pretraining data document [/s] . We truncate the document at 4,096 wordpiece to avoid it being very slow. After- wards, we get the activations from RoBERTa and Longformer similar to WikiHop (discussed above). We use global attention on all question tokens. For prediction, we add one layer that predicts the beginning and end of the answer span. Because of the distant supervision nature of the training data (no gold answer spans), we use the loss function of Clark and Gardner (2017) which works like an OR that the model only needs to get one answer span right, not all of them. Hyperparameters of the best"}, {"filename": "4.pdf", "page_number": 15, "content": "con\ufb01guration are listed in Tab. 14. All other hyperparameters are similar to RoBERTa\u2019s. For hyperparameter search, we only tuned LR for the RoBERTa baseline and tried rates [3e-5, 5e-5, 1e-4], then used the best, which is 3e-5, for all subsequent experiments with no further tuning. We trained the Longformer-large with the best con\ufb01guration once and submitted its output to the leaderboard. We ran our experimentson 32GB V100 GPUs. Small model takes 1 day to train on 4 GPUs, while large model takes 1 day on 8 GPUs. HotpotQA HotpotQA dataset involves answer- ing questions from a set of 10 paragraphs from 10 different Wikipedia articles where 2 paragraphs are relevant to the question and the rest are dis- tractors. It includes 2 tasks of answer span ex- traction and evidence sentence identi\ufb01cation. Our model for HotpotQA combines both answer span extraction and evidence extraction in one joint model. We found a higher performance using a two-stage Longformer model with similar setup that"}, {"filename": "4.pdf", "page_number": 15, "content": "\ufb01rst identi\ufb01es relevant paragraphs and then does \ufb01nd the \ufb01nal answer span and evidence.15 This is largely because removing the distracting paragraphs \ufb01rst reduces the noise for the \ufb01nal ev- idence and span detection as also found to be im- portant by recent state-of-the-art methods in this dataset (Fang et al., 2020). Similar to Wikihop and TriviaQA, to prepare the data for input to Long- former, we concatenate question and then all the 10 paragraphs in one long context. We particu- larly use the following input format with special tokens: \u201c [CLS] [q] question [/q] hti title 1h/tisent 1,1[s] sent 1,2[s] ... 15The \ufb01nal dev performance of the two stage model im- proves over a single stage model by about 4.2 points on joint- F1 metric 15"}, {"filename": "4.pdf", "page_number": 16, "content": "htititle 2h/tisent 2,1[s] sent 2,2 [s] ... \u201d where [q],[/q] ,hti,h/ti,[s], [p] are special tokens representing, question start and end, paragraph title start and end, and sentence, respectively. The special tokens were added to the Longformer vocabulary and randomly initialized before task \ufb01netuning. For Longformer, we use global attention to question tokens, paragraph ti- tle start tokens as well as sentence tokens. The model includes additional feedforward layers on top of paragraph title start tokens for prediction of relevant paragraphs, as well as sentence tokens for predicting evidence sentences. After training the \ufb01rst stage model, we predict relevant paragraph scores for both training and development set. We then keep up to 5 paragraphs whose raw score is higher than a pre-speci\ufb01ed threshold (-3.0), and remove the other paragraphs from the context. We then train the second stage model on the resulting shortened context. For answer span extraction we use BERT\u2019s QA model (Devlin"}, {"filename": "4.pdf", "page_number": 16, "content": "et al., 2019) with addition of a question type (yes/no/span) classi\ufb01- cation head over the \ufb01rst special token ( [CLS] ). For evidence extraction we apply 2 layer feedfor- ward networks on top of the representations corre- sponding to sentence and paragraph tokens to get the corresponding evidence prediction scores and use binary cross entropy loss to train the model. At inference time for evidence extraction, we use a constrained decoding strategy similar to Groen- eveld et al. (2020) that ensures that the evidence sentences come from exactly two paragraphs which is the setup of this dataset. We combine span, ques- tion classi\ufb01cation, sentence, and paragraphs losses and train the model in a multitask way using lin- ear combination of losses. Our experiments are done on RTX8000 GPUs and training each epoch takes approximately half a day on 4 GPUs. We trained the model using Adam optimizer with lin- ear warmup (1000 steps) and linear decay. We used minimal hyperparameter tuning using LRs"}, {"filename": "4.pdf", "page_number": 16, "content": "of 3e-5 and 5e-5 and epochs of 3 to 7 and found the model with LR of 3e-5 and 5 epochs to work best. We conduct the same hyperparameter search for the RoBERTa baseline as well. The rest of hyperpa- rameters are reported in Tab 14. Coreference model details The coreference model is a straightforward adaptation of the coarse- to-\ufb01ne BERT based model from Joshi et al. (2019). After preprocessing each document with the RoBERTa wordpiece tokenizer, it splits eachParam WikiHop TriviaQA HotpotQA Epochs 15 5 5 LR 3e-5 3e-5 5e-5 Warmup steps 200 1000 1000 Batch size 32 32 32 Optimizer Adam Adam Adam Table 14: Hyperparameters of the QA models. All mod- els use a similar scheduler with linear warmup and de- cay. document into non-overlapping segments up to the maximum sequence length, then concatenates the activations for the coarse-to-\ufb01ne clustering stage that forms coreference clusters. The maximum se- quence length was 384 for RoBERTa-base, chosen after three trials from [256, 384, 512] using"}, {"filename": "4.pdf", "page_number": 16, "content": "the default hyperparameters in the original implemen- tation.16For Longformer-base the sequence length was 4,096. Similar to the original implementation, different learning rates were used for the pretrained RoBERTa parameters and the randomly initialized task parameters. Using a larger learning rate in the task parameters allows the optimizer to adjust them farther from their randomly initialized values with- out destroying the information in the pretrained RoBERTa parameters. Hyperparameter searches were minimal and con- sisted of grid searches of RoBERTa LR in [1e-5, 2e-5, 3e-5] and task LR in [1e-4, 2e-4, 3e-4] for both RoBERTa and Longformer for a fair compari- son. The best con\ufb01guration for Longformer-base was RoBERTa lr=1e-5, task lr=1e-4. All other hy- perparameters were the same as in the original im- plementation. Training takes about 10 hours on a single GPU. Our implementation is a superhack that involves PyTorch and Tensor\ufb02ow sharing a single process and GPU. To avoid re-"}, {"filename": "4.pdf", "page_number": 16, "content": "implementing the com- plicated coarse-to-\ufb01ne logic from Tensor\ufb02ow in PyTorch (that involves a highly optimized cus- tom GPU kernel originally released by Lee et al. (2018)), we devised a system where the lower trans- former portion of the model passes activations and gradients back and forth between PyTorch and Ten- sor\ufb02ow. The input tensors are \ufb01rst run through the transformer in PyTorch, the activations are col- lected from the top layer, transferred from GPU to CPU then from CPU to Tensor\ufb02ow and back to GPU to run the coarse-to-\ufb01ne clustering and com- pute the loss. Then gradients are back propogated 16https://github.com/mandarjoshi90/coref 16"}, {"filename": "4.pdf", "page_number": 17, "content": "in Tensor\ufb02ow to the top of the transformer and the process reversed to transfer them to PyTorch for back propogation through the remainder of the model. Separate optimizers are maintained with identical LR schedules for parameter updates. The overhead in this approach is minimal compared to the overall cost of running the model. Text classi\ufb01cation For classi\ufb01cation, following BERT, we used a simple binary cross entropy loss on top of a \ufb01rst [CLS] token with addition of global attention to [CLS] . We used Adam opti- mizer with batch sizes of 32 and linear warmup and decay with warmup steps equal to 0.1 of the total training steps. For both IMDB and Hyperpar- tisan news we did grid search of LRs [3e-5, 5e-5] and epochs [10, 15, 20] and found the model with [3e-5] and epochs 15 to work best. Experiments were done on a single RTX8000 GPU. 17"},{"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 1, "content": "BERT : A Review of Applications in Natural Language Processing and Understanding Koroteev M.V., Financial University under the government of the Russian Federation, Moscow, Russia mvkoroteev@fa.ru Abstract : In this review, we describe the application of one of the most popular deep learning-based language models - BERT. The paper describes the mechanism of operation of this model, the main areas of its application to the tasks of text analytics, comparisons with similar models in each task, as well as a description of some proprietary models. In preparing this review, the data of several dozen original scientific articles published over the past few years, which attracted the most attention in the scientific community, were systematized. This survey will be useful to all students and researchers who want to get acquainted with the latest advances in the field of natural language text analysis. Keywords : natural language processing, review, BERT, language models, machine learning,"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 1, "content": "deep learning, transfer learning, NPL applications, Introduction The search for a universal representation of text is at the heart of the automated processing of natural languages. The big breakthrough in this area has been with the development of pretrained text attachments such as word2vec [52] or GloVe [64] . Over the past years, supervised models have shown consistently better results than unsupervised models [49] . However, in recent years, models based on learning without a teacher have become much more widespread since they do not require the preparation of a specially labeled dataset, but can use already existing or automatically generated huge corpora of texts and, as a result, learn on much a larger sample, thus taking full advantage of deep learning. The centerpiece of 2019 in the field of natural language processing was the introduction of a new pretrained BERT text attachment model, which enables unprecedented precision results in many automated word processing tasks. This"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 1, "content": "model is likely to replace the widely known word2vec model in prevalence, becoming, in fact, the industry standard. Throughout 2019, almost all scientific articles devoted to the problem of word processing in natural languages, in one way or another, were a reaction to the release of this new model, the authors of which have become one of the most cited researchers in the field of machine learning. Natural language processing tasks include a wide range of applications from conversational bots and machine translation to voice assistants and online speech translation. Over the past few years, this industry has experienced rapid growth, both quantitatively, in the volume of market applications and products, and qualitatively, in the effectiveness of the latest models and the proximity to the human level of language understanding. One of the central themes in natural language processing is the task of text representation. Text representation is a kind of rule for converting natural"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 1, "content": "language input information into machine-readable data. A representation can also be considered simply a computer encoding of text, but in the context of applied machine learning problems, such representations that reflect the internal content and conceptual structure of the text are more useful. The most simple textual representations are categorical encoding when each word is represented as a vector filled with zeros everywhere, except for one position corresponding to the number of this word in the dictionary. This concept was used in the early stages of the industry. It is quite simple, does not require computational resources to implement, and conceptually very simple. However, such a representation does not take into account the semantic features of words, it is rather voluminous and redundant since a vector with the dimension of the number of words in the dictionary is used to represent each word. A similar view is the well-known bag of words model. This model represents the"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 1, "content": "entire text as a vector with the dimension of a vocabulary, in which each component represents the number of occurrences of a given word in the text. This is also a fairly simple model that does not take into account the semantics of words, however, it is quite successfully applied to tasks, for example, the categorization of texts. The most applicable in the modern industry is the representation of text in the form of so-called attachments - a mechanism for representing each word in the form of a vector, each coordinate of which has a certain semantic meaning. Most often, attachments with hundreds of coordinates are used. Attachments can capture the semantic meaning of specific words in a text and display it as a coordinate in multidimensional space. One well-known illustration of this is the ability to perform vector operations in this semantic latent space. Nesting is most often done by unsupervised model training on large text corpora. An example of such tasks can be filling in"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 1, "content": "gaps in the text, determining the relevance of sentences. Learning textual representations is a very"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 2, "content": "computationally intensive task. For most problems of analysis of general-purpose texts, ready-made representations, trained in advance, are used [70] . Basic concepts about the new BERT linguistic model Today, the most advanced text models use transformers to teach how to represent text. Transformers are a type of neural network that are increasingly finding their use in various branches of machine learning, most often in sequence transduction problems, that is, such problems when both the input and output information is a sequence [22] . Such models use a combination of recurrent and convolutional neural networks. Regular recurrent networks perform rather poorly with long-range context. However, in natural text, the representation of the token can be influenced by the context through several words and even sentences from the token itself. To take into account the long-range influence, LSTMs are used in conjunction with the attention mechanism to improve the learning efficiency, taking"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 2, "content": "into account the influence of distant tokens [19] . At the end of 2018, a group of scientists from the Google AI Language laboratory under the leadership of J. Devlin presented a new linguistic model called BERT [16] . This model is intended for deep preliminary learning of bidirectional text representation for subsequent use in machine learning models. The advantage of this model is its ease of use, which involves adding just one output layer to the existing neural architecture to obtain text models that surpass the inaccuracy of all existing ones in several natural text processing problems. There are two categories of natural text processing tasks: holistic, operating with text at the sentence level, and tokenized ones, such as answering a question and attribution of entities, which produce more detailed output at the level of individual text elements. Both categories of problems have recently been using pretrained models, which can significantly reduce the time for designing and"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 2, "content": "training private models while maintaining a high level of efficiency [14,24] . Regarding the approaches to pre-training models of deep text representation, two approaches are also distinguished: feature extraction and fine-tuning of models. Both approaches use the same pre-learning objective functions and unidirectional text analysis. The first, implemented, for example, in the ELMo model [65], is to use models with a task-specific architecture to train a certain representation, which will then be used as additional features in applied models. The second involves the construction of models that do not use parameters specific to a particular task but training the model, which is then supposed to be retrained by adjusting all the internal parameters of the model. This approach is used, for example, in the OpenAI GPT model [66] . The authors of the BERT model point out that a significant limitation of existing approaches is their focus, which narrows the choice of possible neural network"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 2, "content": "architectures. For example, the GPT model uses left-to-right text scans so that the representation of each element of text (token) can only take into account the previous, but not subsequent tokens. This may not be optimal for holistic text analysis, but for tokenized analysis, this approach significantly reduces the semantic power of the model, since the meaning of a word can depend on its context, both left and right. BERT tries to get around this limitation by using learning according to the so-called \u201cmasked language models\u201d, that is, the target function of learning a given representation formalizes the task of predicting a randomly selected and masked word in a text, taking into account only the surrounding context. Thus, a deep bi-directional transformer is trained. The BERT model training process includes two stages: pre-training on unlabeled data, and additional training on labeled data for a specific application problem. Depending on the task, the retraining process and the"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 2, "content": "architectures used may differ, although they are all based on the same model with the same set of parameters. The BERT architecture is based on the multilayer bidirectional transformer described in 2017 by A. Washwani in [80] . The authors trained two versions of the neural network - a standard one with 12 layers and 768 coordinates in the view (110 million trained parameters in total) and a large one with 24 layers and 1024 coordinates (340 million parameters). BERT uses text embeddings described in 2016 to represent an input sequence [85] . A sequence is an arbitrary set of contiguous text tokens. For example, the model uses the same representations for one sentence and for a pair, which allows BERT to be used for a wide range of tasks. Using the described process of additional training for a specific task, BERT was tested on several standard datasets to compare its performance with other published models. So, on the GLUE test (General Language Understanding Evaluation [83] , a set"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 2, "content": "of tasks and datasets that test natural language comprehension), the BERT-based model showed an average superiority of 4.5% and 7% (for standard and large neural networks, respectively) compared to the best-known models. The father-in-law of SQuAD (The Stanford Question Answering Dataset [67] , the model is asked a question and a passage of text, the model must choose from the text the passage that answers the question) also shows the superiority of BERT over all existing models (according to the F1 metric, the best model showed 78.0; BERT - 83.1; human result - 89.5). And the SWAG test (The Situations With"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 3, "content": "Adversarial Generations [91] , a set of questions with four possible answers to choose from) shows an accuracy of 86.3, which is higher than that of a human expert (85.0). In the field of teaching linguistic models without a teacher, two main areas can be distinguished: autoregressive models and autoencoders. Both those and others have their significant disadvantages. For example, autoregressive models are inherently unidirectional and cannot take into account the general environment of a particular token. Autoencoder-based models such as BERT do not have this drawback but rely on artificial text tokens such as [SEP] or [MASK] in their learning process, which creates a contradiction between pre-training and additional training for a specific task, where such tokens are absent ... A group of scientists led by J. Young from Carnegie Melon University [87] tried to combine these two approaches, proposing the XLNet model. Moreover, the obtained model gives better results in comparison with"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 3, "content": "BERT on a wide range of linguistic problems, such as GLUE [83] , SQuAD [67] , and RACE. BERT Retraining Methodology for TextProblems Text Classificationclassification is a classic natural language processing problem. It consists of assigning one of a predefined set of categories to a given text sequence. Currently, machine learning systems for text classification use convolutional neural networks [29] , recurrent models [42] , and attention mechanisms [88] . Also in this area models like word2vecvery effective [52] , pretrained, GloVe [64] , and ELMo [65] are . Such models avoid the computationally complex initial training of the model on a large body of texts. For text classification tasks, the BERT model is also used, which allows additional training of the basic model for a specific task using just one additional layer of neurons. The use of BERT makes it possible to obtain models with the currently best performance in text classification problems after additional training according"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 3, "content": "to a certain methodology [74] . There is a growing interest in approaches that make it possible to use knowledge from related problems in word processing problems. Two groups of such methodologies can be distinguished: the use of pre-trained models (transfer learning) and multitasking training. BERT, like earlier models, belongs to the first category. Multitasking learning [8] is also a promising, although not the newest, line of research. Certain results were obtained [41,69] in the joint training of textual representations and the target problem within the same model. The basic BERT model contains an encoder with 12 transformer blocks, 12 attention areas, and a textual representation dimension of 768. The model receives a text sequence of no more than 512 tokens as input and outputs its vector representation. The sequence can contain one or two segments, separated by a special token [SEP], each of which necessarily begins with the token [CLS], with a special classification"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 3, "content": "representation. When adapting BERT to specific word processing tasks, a special retraining technique is required. Such techniques can be of three types: F u r t h e r p r e - training: BERT is initially trained on general-purpose texts, which may have a different data distribution than the texts of the target application area. Naturally, the desire to retrain BERT on a specific text corpus can be based on an intra-task dataset (the same dataset that will be used to train the target model), an intra-subject dataset (a set of text data obtained from the same subject area) or a cross-subject dataset, in depending on the nature of the texts and the availability of data sets. Studies of the effectiveness of BERT retraining [74] show that, first, retrained models show significantly better results compared to models without retraining, and, secondly, intra-subject learning is generally more effective than intra-task learning. Cross-subject learning does not significantly improve performance"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 3, "content": "relative to the original BERT model, which is logical given that BERT is trained on a general set of texts. R e t r a i n i n g S t r a t e g i e s : There are many ways to use BERT for a target. For example, you can use the representation provided by BERT as additional or basic characteristics in the classification model, you can use the inner layers of BERT to get information about the text. Different layers of the neural network can display different levels of syntactic and semantic information of the text [24] . Intuitively, the earlier layers contain more general information. Accordingly, the problem arises of choosing the required layer for inclusion in a specific model. During additional training of models, the problem of so-called \u201ccatastrophic forgetting\u201d often arises [51] , which is that in the process of additional training on a specific set of data, knowledge expressed in the form of model weights trained at the stage of preliminary training is quickly erased. This leads to"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 3, "content": "the leveling of the use of pre-trained models. It has been shown that using low learning rates can overcome this problem. The study described above used a learning rate of about 1e-5. Higher speed (4e-4) leads to the fact that training on the target dataset does not converge. M u l t i t a s k i n g L e a r n i n g : In the absence of pre-trained natural language models, multitasking learning is effective in leveraging shared knowledge of multiple target tasks. When researchers are faced with several word processing tasks in the same subject area, it makes sense to retrain the BERT model in these tasks simultaneously."}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 4, "content": "The problem of improving the subject-specific classification of texts using BERT Traditional text embedding models represent each word of the input text (token) as a numerical multidimensional semantic vector (embedding) following the assumption that words with similar semantic meaning usually appear in similar contexts. Accordingly, the embedding of each word is calculated based on its environment within a certain window size. The result is a kind of context-independent representation of each text token. This approach has certain characteristics that represent the challenges facing word processing experts and machine learning architects [90] . A challenge to ambiguity: In traditional text attachments, each token is represented as a fixed vector representation, regardless of the context in which the word appears in the text. However, in natural languages, a large number of words are polysemantic, that is, they carry completely different semantic loads depending on the context. In the"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 4, "content": "problems of text classification, the differentiation of such semantic differences may be necessary, especially with indicative words. Subject-specific challenge: when using most traditional textual models, the classification efficiency strongly depends on the architectures of neural networks, which are usually built specifically for each specific subject area, which gives rise to the need for the manual search for the model architecture and the intensive process of model training. To overcome the first challenge, so-called contextual text models have been developed, for example, CoVe [50] or ELMo [65] , which produce different vector representations, depending on the environment of a particular token. The CoVe model uses a deep encoder with LSTM cells in conjunction with an attention mechanism trained on machine translation tasks. The practice of using this model shows that adding context does significantly improve the efficiency of text models in many word processing tasks. The ELMo"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 4, "content": "model is based on the representation of text from a bi-directional neural network with memory trained on the tasks of searching for a language model on a huge corpus of texts. Both approaches successfully generalize traditional textual representations to context-sensitive models and are used in specific tasks as sources of additional features that are input to specific models. Over the past few years, the efforts of researchers have been aimed at creating a general (universal) text model, pre-trained on a large corpus of general-purpose texts, suggesting limited additional training for solving specific word problems. This approach is designed to solve the problem of finding specific model architectures for each problem and, as a result, to significantly reduce the number of trained model parameters. Such models include ULM-FiT [24] , OpenAI GPT [66], and BERT. It is the efficiency of using BERT today that is significantly higher than other text attachments. However, the researchers"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 4, "content": "note that the full potential of BERT as a universal text model has not yet been revealed and point to the main directions of promising research: \u25cf development and detailed analysis of techniques for retraining BERT for specific word problems. For example, the authors in [74] consider techniques for retraining a model on long text sequences consisting of more than 512 tokens. \u25cf a methodology for retraining BERT on small sets of texts, which are not enough to highlight full knowledge of the textual features of the subject area. Since manual data markup is still a very time-consuming process, in many specific areas there is not enough marked-up text selection to fully retrain the text model. \u25cf developing a methodology for unsupervised learning on a large unlabeled corpus of subject-specific texts can be another way to increase the flexibility and universality of BERT for different subject areas. Using BERT for Text Annotation Tasks Another interesting problem in natural language"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 4, "content": "processing is text annotation - the automatic selection of key phrases and sentences that best characterize the content of a given text. Text annotation methods are widely used in education. An example is a project to manually annotate publicly available transcriptions of MIT lectures [20,51] . As the researchers note, for a limited set of data, such an approach has the right to exist, however, given the ever-increasing volume of information, it is not possible to continue to rely on hand tools. There are two approaches to annotating text - abstractive and extractive. The first imitates the human approach to natural language processing - the use of a wide vocabulary, highlighting key ideas in a compressed volume. This approach seems to be more desirable and is the subject of a large amount of research [18] , but it requires enormous computational power and time to process deep machine learning models or to compile complex algorithms and rule bases."}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 5, "content": "In contrast, the extractive approach uses syntactic structures, sentences, and phrases contained in the text, manipulating only the input content. The use of the manual annotation method throughout the 2000s is associated with the poor quality of the extractive techniques that existed at that time. For example, the model created in 2005 by G. Murray [55] for annotating the minutes of corporate meetings using probabilistic models gave incomparably worse results than human text processing. Several attempts have been made to build a better algorithm using rhetorical information [92] , TextRank ranking algorithms [79] , a naive Bayesian classifier [4], or TF-IDF metrics [97] . These methods have been used with varying success, but what they all have in common is that they don't all use deep learning. There is a methodological reason for this: over the years, the most effective method of word processing using machine learning has been recurrent neural networks, which entails the need to use"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 5, "content": "huge data sets for training over many machine hours to achieve an acceptable result. And even so, such models gave low efficiency on long text sequences and were prone to overfitting. The situation also changed with the introduction in 20017 of special neural network architecture - a transformer [80] , which shifted the focus from recurrent and convolutional networks to feedforward networks with an attention mechanism. That is why the widespread introduction of transformers into the practice of text processing in natural languages, the presentation of the pretrained BERT model gave such an impetus to the development of the field of word processing in all tasks. For text annotation tasks, the use of text attachments gives another non-obvious advantage - the ability to create variable-length annotations. Since the nesting produces a vector representation of the text, this representation can be clustered with an arbitrary size K, which allows building a dynamic presentation of the text"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 5, "content": "[53] . Evaluating the performance of text annotation models is complicated by the fact that there is no generally accepted standard for annotation, which means that there are no automated metrics for assessing the quality of annotation. The only way is to manually assess the quality of the resulting text. The use of BERT improves the perceived quality of text annotation, although such models are not free from classic drawbacks, such as the difficulty in separating indicative sentences that are a small proportion of the original text. Thus, annotating long texts is still a promising area of   research, along with the development of model datasets and quantitative metrics for evaluating the effectiveness of annotation models. BERTScore: BERT Based Text Generation Quality Assessment The problem of text generation quality assessment arises in the process of solving many problems, for example, machine translation. It can be reduced to comparing a set of candidate proposals with a sample"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 5, "content": "proposal in a given textual context. However, the most commonly used sentence similarity metrics, such as the one described in the 2002 BLEU (bilingual evaluation understudy) [63,72] , focus only on superficial similarity. The aforementioned BLEU metric, the most common in the development of machine translation systems, relies on the comparison of the intersection of n-grams of text. For all its simplicity, such metrics miss the lexical and semantic diversity of natural languages. A vivid example of this problem, described in 2005 by B. Sataneev in [5] : several popular text metrics for this reference sentence \u201cPeople love foreign cars\u201d prefers the sentence \u201cPeople like to travel abroad\u201d over those that are more semantically close to the original \u201c Customers prefer foreign cars \u201d. As a consequence, machine translation systems that use such metrics to assess translation quality will prefer syntactically and lexically similar constructs, which is suboptimal in the context of wide"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 5, "content": "linguistic diversity. The representation of the BERT system allows it to be used as a basis for measuring the similarity of sentences in natural languages, using the metric of the distance between text attachments of compared sentences. In general, the text metric, or the metric of the quality of text generation, is a function , where is the vectorized representation of the sample proposal, and is the vectorized representation of the candidate proposal. A good metric should reflect the person's judgment as closely as possible, that is, show a high correlation with the person's assessment results. All existing metrics can be roughly classified into one of four categories: n-gram match, edit distance, attachment comparison, and trained functions. A good metric should reflect the person's judgment as closely as possible, that is, show a high correlation with the person's assessment results. All existing metrics can be roughly classified into one of four categories: n-gram match, edit"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 5, "content": "distance, attachment comparison, and trained functions. The most common textual metrics are based on counting the number of n-grams found in both sentences. The larger the dimension n in an n-gram, the more the metric can capture the similarity of whole words and their order, but at the same time, the more this metric is limited and tied to a specific formulation and word forms. The already mentioned BLUE and MeTEOR belong to this category of metrics."}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 6, "content": "Some methods calculate the proximity of proposals by the number of edits that translate a candidate into a reference. Methods such as TER [72] and ITER [61] take into account the semantic proximity of words and the normalization of grammatical forms. This can also include methods like PER [77] and CDER [37] , taking into account the permutation of text blocks. Some more modern methods (CharacTER [84] , EED [73] ) have unexpectedly better results. In recent years, metrics have begun to appear based on the use of dictionary embeddings, that is, trained dense word vectorizations (MEANT [44] , YISI-1 [46] ). The advantage of using BERT as a basis for constructing such metrics is that it takes into account the context of a word within its environment, which makes it possible to use attachments not at the level of individual words, but the level of the entire sentence. Since the criterion for the quality of a text metric is its correlation with human judgments, it is not surprising that"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 6, "content": "machine learning is used to build trained metrics, where the target function is just such a coincidence. For example, BLEND [48] uses a regression model to weigh 29 known metrics. The disadvantage of this approach is the dependence on the presence of a corpus of pre-labeled data for the training set, as well as the risk of overfitting in a certain subject area and, as a consequence, the loss of generalizing ability and universality. In February 2020, researchers from Cornwell University proposed a special mechanism for evaluating the effectiveness of text models based on BERT - BERTScore [93] . BERTScore is used to automatically assess the quality of natural language text generation. The authors of the proposal argue that BERTScore correlates better with human judgments about the quality of the text and, as a result, can form the basis for a more efficient and effective model selection process. Today it is the most progressive metric for evaluating the quality of text generation. At"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 6, "content": "the heart of BERTScore is a simple algorithm for calculating the cosine distance between vectorized representations of each word in a reference sentence and a candidate sentence. Distances are calculated in pairs, and then, for each word in the reference, the closest words in the candidate are selected, these distances are averaged and make up an estimate - a review score. Accuracy score is considered similar, but inverted relative to candidate and reference - . In addition, an F1 score is calculated - . - review score. Accuracy score is considered similar, but inverted relative to candidate and reference - . Also, an F1 score is calculated - . Source: [93] In addition to the basic algorithm, the authors of BERTScore use the IDF metric to determine the rarer words, as previous studies of text metrics [81] indicate that sparse words may be more indicative of the similarity of two sentences. The IDF metric estimated on the reference and smoothed (+1) to account for new words is used as"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 6, "content": "the weight of the corresponding cosine measure when averaged. In a comparative analysis of text similarity metrics, metrics based on BERT show consistently higher results than classic text metrics. This means that they are statistically significantly closer to human estimates. Besides, the original article introducing BERTScore focuses on performance issues. In this regard, BERTScore is, of course, often slower than classic models. The authors give an assessment of the comparison with"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 7, "content": "the popular implementation of SacreBLEU, in which BERTScore is about three times slower. Since the pretrained BERT model is used to assess the similarity, the increase in the accuracy of the estimate is given at the expense of a decrease in speed that would not be expected from such a more complex model. Given the typical size of natural text processing datasets, the increase in computation time during model validation should not have a significant effect on the performance of the machine learning system as a whole. Publications already beginning to appear [95] that improve the original BERTScore algorithm by parallelizing computational operations. Variations of classical metrics [45] using BERT and showing improved correlation with human estimates are presented. Undoubtedly, the direction of the future development of textual metrics will be the wider use of BERT as a basis for assessment, a kind of semantic engine. Also promising is the development of specific models that take into"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 7, "content": "account the peculiarities of specific subject areas and increase the basic level of accuracy through specialization. Concerning BERTScore, another advantage is its differentiability, which will allow it to be integrated into the methodology for training text models in the future, which promises to further increase the performance and quality of machine learning models of natural text processing. BERT Based Attacks on Text Classification Models Machine learning models can very often be vulnerable to input data that are indistinguishable from a real example for humans, but still, give an incorrect output [32] . Such specially selected (adversarial) examples are correctly classified by a human expert, but give an incorrect result in the model, which casts doubt on the safety and reliability of the applied machine learning models [62] . At the same time, it was shown that the robustness and generalizability of the models can be significantly increased by generating high-quality competitive"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 7, "content": "examples and including them in the training sample [21] . Currently, some success has been achieved in creating a methodology for constructing adversarial examples in the field of image recognition [75] and audio processing [7] . However, the field of natural language word processing remains under-explored due to the specific nature of the input data. In addition to adversarial itself, such textual examples must have additional conditions: \u25cf consistency of human interpretation - an adversarial example must be perceived by a person in the same way as the original one; \u25cf semantic similarity - an adversarial example must carry the same semantic meaning for a person as the original one; \u25cf linguistic correctness - an adversarial example must be perceived by a person as syntactically correct. Existing methods of creating textual adversarial examples include misspelling words [39] , removing individual words from text [40] , deleting or inserting entire phrases [38] , all of which generate"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 7, "content": "artificial examples. The statement of a method for generating adversarial examples for text classification problems can be formalized as follows [25] : having a set of sentences and a corresponding set of labels , a pre-trained model is given that assigns a label Y to a sentence X. In this case, for an arbitrary sentence, the adversarial example must satisfy the following conditions: where is the function of syntactic and semantic similarity, and is the minimum level of similarity between the original and adversarial examples. If the black box condition is met, the adversarial example generation technique should not have information about the architecture, parameters, and structure of the training sample of the target machine learning model. She can only use the model on the given input example to obtain the classification result and confidence level. J. Howard, J. Jijding, J. Tianijou, and P. Zolovits from MIT and Hong Kong University [25] proposed an algorithm for constructing"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 7, "content": "adversarial textual examples TEXTFOOLER for studying black-box models, including BERT, consisting of the following steps: \u25cf ranking the importance of words. The practice of using modern textual models shows that only some words serve as indicative factors in the operation of models, which is consistent with the studies of BERT T. Niven and H. Kao [56] , which show the importance of the attention mechanism. Note that this step is trivial under the condition of the white box model and is reduced to calculating the gradient of the model output when changing individual words. \u25cf transformation of words. In this step, words with the identified high importance for the model are replaced. The replacement must satisfy three conditions: have similar semantic meaning, be syntactically appropriate to the original context, and cause the model to mispredict. At this stage, several sub-stages can be distinguished:"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 8, "content": "\u25cb highlighting synonyms. A set of synonyms is selected based on the measure of the cosine distance between the original and all other words in the dictionary. Text embeddings at the word level can be used here [54] . \u25cb filtering by part of speech. To ensure syntactic consistency, it is necessary to leave only those candidate synonyms that coincide in grammatical role with the original word. \u25cb checking semantic similarity. For each candidate, the similarity of the textual representation of the original sentence and the sentence in which the original word is replaced by it (the candidate for adversarial examples) is checked. For this, the USE model [9] is used , which represents a universal textual representation at the sentence level and a cosine similarity metric. It is necessary to achieve a similarity that exceeds a predetermined threshold . \u25cb validation of model prediction. For each candidate for adversarial examples, the prediction of the model is evaluated and, if examples are"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 8, "content": "found that change the prediction of the model relative to the original proposal, they are added to the result set. If not, then the word with the next in descending order of importance, assessed in the first step, is selected and the process is repeated. This algorithm was tested on five publicly available datasets for tact classification - AG's news [94] , Fake news detection [98] , MR [60] , IMDB [99] , and Yelp. Three modern text representation models were trained: WordCNN [30] , WordLSTM [23] and BERT. The simulation results are shown in the table: Source: [25] When analyzing the results, it turned out that a dataset consisting of generated adversarial examples can reduce the efficiency of text classification from 80-97% to 0-20%. This indicates the success of the attack on the machine learning model. Adding the generated adversarial dataset and additional training of the model on it significantly increases the efficiency of text classification models, which show 2-7 percentage"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 8, "content": "points higher efficiency on the test adversarial dataset than without such additional training. Source: [25] Undoubtedly, studies of the stability of machine learning models are an indispensable condition for the widespread introduction of such intelligent systems in the decision-making process, which is the relevance of this area of   research. As mentioned earlier, the development and study of various types of automated attacks on machine learning models can give us a direction for further improving the development and testing of intelligent systems in general, not limited to classification models. Interlingual training of linguistic models Generative pre-sentence encoders show their effectiveness in many natural text processing tasks [16,24,66] . Although the efforts of researchers are aimed at building a model for the general understanding of the text, most of the research is focused on monolingual representations trained on the corpus of English-language texts [11, 83] . Recent"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 8, "content": "advances in teaching and evaluating interlanguage representations of sentences are aimed at overcoming the anglocentric bias and suggest that language models can learn truly universal representations in the common latent space of features, being trained on a corpus of texts in different languages. Such models belong to the group of interlanguage representations (XLU, [34] ). Along with monolingual studies, some works [68] show that the use of pre-trained language models can give an advantage in solving machine translation problems even in such classical language pairs as EN-DE, for which there is a large corpus of parallel texts, which facilitates the use of learning models with the teacher. Equalization of distributions of sentence representations has been used for a long time in text nesting research. For example, T. Mikolov's work [52] uses short dictionaries to align representations for different languages. Further studies show that interlanguage representations can improve the"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 8, "content": "quality of monolingual text"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 9, "content": "embeddings [17] , orthogonal transformations are sufficient to equalize the distribution of words between several languages [86], and this approach can be extended to an arbitrary number of languages [2] . The need to train such models with a teacher on pairs of parallel texts in different languages   is reduced [71] and even eliminated [12] . A certain number of works in the field of searching for interlanguage representations are devoted to the problem of the so-called \u201czero-shot\u201d problem - after training on a corpus of parallel texts, the model should adequately perform the tasks of text classification or translation into a pair of languages   that it did not see clearly during its training. One of the most successful works in this area [26] presented a machine translation model based on a simple pair of encoder and decoder in LSTM. This model shows the best result among all machine translation models in rare language pairs, including new pairs that have not been studied in the"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 9, "content": "learning process. The work of M. Artetze and others [3] shows that this model can serve as a basis for the creation of interlanguage textual representations. This approach uses training on over 200 million parallel sentences. Recent work in the field of unsupervised learning shows that sentence representations can be aligned completely unsupervised [35] . Work [82] presents an LSTM model trained on sentences in different languages. The authors use common memory cell weights, but use different lookup tables to encode words in different languages. Their model shows good results on word translation problems. The problem of building cross-language models (XLM) is discussed in detail in [34] , where the authors investigate the process of training models on three learning tasks (conditional linguistic modeling, modeling of masked words, and modeling of text translation, CLM, MLM and TLM, respectively), two of which require training unsupervised on monolingual data, and the third is"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 9, "content": "supervised learning on a set of parallel sentences. The task of conditional modeling is to train the model to predict the probability of a word appearing in a sentence for a given set of previous words. Traditionally, this task is solved using recurrent neural networks, although the use of transformers [15] also shows its effectiveness. Masked language modeling, also known as the Klose problem [76] , is widely used to train text attachments and consists of teaching a model to predict a word (token) in a text sequence, replaced by a special token [MASK]. This technique is one of two BERT training tasks and is detailed in [16] . The text translation modeling task aims to use the parallel text corpus, when available, to improve the alignment of textual representations between languages. It is similar to the MLM problem with the difference that two parallel sentences in different languages   are submitted as input. Words are masked in both sentences at random. To predict the masked word,"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 9, "content": "the model can use its context in the original language, or, if there is not enough information, a second sentence, which is a translation of this sentence into another language. Thus, the model is stimulated to develop aligned textual representations between different languages. A graphical comparison of the last two tasks is shown in the figure: Source: [34] To assess the quality of learning interlanguage models, the XLNI dataset [13] is used from the corpus of texts in 15 different languages, significantly differing in prevalence, from English to Nepali. Research [34] shows that the use of unsupervised multilingual learning can force the model to generate reliable cross-language textual representations. MLM training is especially effective. Without the use of parallel corpora, the model shows an improvement in the quality of machine translation by 1.3% on average relative to the best-known machine learning"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 10, "content": "models. The TLM learning procedure can further improve machine translation accuracy by an average of 4.9% on average. Construction of domain-specific text models based on BERT The exponential growth in the number of scientific publications leads to the need to use automated word processing tools for large-scale knowledge extraction. The use of neural networks and deep learning is currently the most widespread, but such approaches require a large amount of pre-mapped text. For general language corpora, the availability of such data sets does not pose such a problem as for highly specialized scientific texts, where manual marking is associated with expensive expertise. Modern ELMo, GPT, and BERT models show that using unsupervised learning can significantly improve model performance on a wide range of machine learning problems. Such models compute contextualized representations of words or text tokens that can be used as input to minimalistic domain-specific models. Such models are"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 10, "content": "trained on large corpora of general-purpose texts, which makes them rather general and universal. However, when analyzing texts from a specific subject area, general models may show sub-optimal efficiency due to a lack of specific training. In this regard, the emergence of some specialized models based on the architecture and teaching methodology of BERT, which has shown its effectiveness, but previously trained on specialized text corpora, is quite logical. The authors of the BioBERT model (Seoul University), as the name implies, propose a specialized BERT-based text nesting model for the analysis of biomedical publications. The motivation for developing this model and the approach to teaching are completely similar. The training took place according to the BERT method [16] on a corpus consisting of annotations of articles from the PubMed database and full-text versions of articles from the PMC database. The training took place for 23 days on 8 NVIDIA V100 GPUs based on weights"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 10, "content": "exported from the original pretrained BERT model, that is, formally, this is additional training. Note that the total volume of the biomedical corpus was 18 billion words, while the volume for training the original model was only 3.3 billion. The learning process is schematically shown in the figure: Source: [36] Unlike SciBERT, the authors of this model did not use a specialized dictionary to tokenize text. The BioBERT training process, like most similar models, consists of two parts: initial training and adjustment (additional training) for a specific word processing task. The authors of the model used three common tasks to assess the performance of the model: named entity recognition (NER [89] ), relation extraction (REL), and answering a question (QA [67] ). The simulation showed the full advantage of the new model over conventional BERT in all specific tasks. The best results to date have been achieved in several tasks. BioBERT is the first, but not the only problem-specific model"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 10, "content": "based on BERT. I. Betalgi, K. Lo, and A. Cohan from the University of Seattle [6] present a SciBERT model trained on a corpus of scientific articles. This model was trained on a random sample of more than 1 million scientific articles from the SemanticScholar database [1] . SciBERT also reconfigures the tokenization tool underlying BERT to account for the different vocabulary of the selected corpus. In the process of building the tokenizer, it turned out that the intersection of general-purpose dictionaries and the scientific corpus was 42%, which indicates a significant difference in the distribution of the token between these corpora. Along with an intuitive understanding, this fact gives reason to believe that the use of specific models can give increase productivity. The model was trained following the original BERT training method, but on a different, more compact body of texts and using a specialized vocabulary for tokenization. The training took 7 days on an 8-core TPU (by"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 11, "content": "comparison, training the full version of the original BERT model took 4 days on a 16-core TPU and is estimated to be 40-70 days on an 8-core). As a result of numerical experiments on some tasks (extraction of entities, classification of texts, determination of dependencies, etc.) for the processing of scientific texts. Datasets from subject areas were used: biomedicine (ChemProt [31] , EBM-NLP [57] and others), computer science (SciERC [47] , ACL-ARC [28] ), and interdisciplinary (SciCite, described by A. Cohan in [10 ] ). On all datasets tested, SciBERT performed better than overall BERT and performed best to date for most. Source: [6] It is noteworthy that this model shows comparable and sometimes superior results compared to BioBERT [36] , despite being trained on a significantly smaller body of biomedical texts. The authors of the SciBERT model separately note that in all cases, additional training of text attachments on a specific task gives a greater effect than building special"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 11, "content": "architectures based on fixed attachments. Also, the use of a vocabulary built for the subject area for text tokenization gives a positive effect. Investigation of the robustness of BERT learning Despite the rapid and successful spread of BERT in almost all areas of word processing in natural languages, there are no guarantees that the presented pre-trained model is optimal, both in terms of architecture and in terms of training methodology and text corpora used. I. Liu and colleagues from the Facebook AI group [43] analyzed the robustness of the BERT model and came to some interesting conclusions. Following, to a large extent, the original procedure for training the model, they concentrated on obtaining the largest possible open corpus of texts for the formation of the training sample. Initially, BERT is trained on a collection of BookCorpus [96] and Wikipedia, which is about 16 GB of uncompressed text. In this work, we used four additional public corpora of English-language texts -"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 11, "content": "CC-News [100] , OpenWebText [58] , and Stories [78] , totaling about 160 GB, that is, an order of magnitude more than the original. After training, the resulting model was tested against three common benchmarks measuring natural text understanding - GLUE [83] , SQuAD [67], and RACE [33] . On all these tasks, the authors received an improvement in the initial indicators, which allows us to conclude that the BERT model is undertrained. In the course of training, some modifications of the original method were analyzed and factor analysis of their influence on the overall performance of the model on the three above-mentioned tasks was made. For example, in the process of preparing a training sample for the masked language model BERT problem, the input text tokens are masked once at the preprocessing stage, that is, static text masking is implemented. Thus, the model sees the same sentence with the same masks at all learning epochs. In contrast, one can duplicate the original sentence"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 11, "content": "multiple times and apply randomized masking for each learning epoch. This is called dynamic text masking. It turns out that such dynamic masking can slightly increase the robustness and efficiency of the resulting model."}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 12, "content": "During the BERT training process, the importance of the second training task, that is, the next sentence prediction (NSP), was revealed. The authors of [16] note that removing this loss function from training significantly reduces performance. However, more recent studies [27,34,87] question the need for NSP inclusion. Several training options for this task were tested to address this issue. First, the original method of two segments, each of which could contain several sentences of the original text, but both together do not exceed 512 tokens in length. Second, the use of a pair of complete sentences selected either sequentially from one text, or different texts in the corpus. Thirdly, training without NSP, when whole related sentences from one or several documents are submitted for input. And fourthly, the same thing, but with the prohibition of sentences from several documents. The learning outcomes are shown in Figure Source: [43] When testing the training, it was concluded that"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 12, "content": "removing the NSP function improves performance on subsequent tasks, which directly contradicts the original publication. Also, using sentences instead of segments degrades performance, presumably by removing the ability to make long-term generalizations. Initially, BERT was trained in 1 million steps in bursts of 256 sequences. This is computationally equivalent to training in 30 thousand steps but large packets of 8 thousand sequences. There are studies by M. Ott et al. [59] showing that language models can benefit from an increase in the size of the training packet, provided the learning rate is increased accordingly. Research [43] shows that this effect also takes place for BERT, but to a limited extent; the highest performance is achieved with a packet size of about 2 thousand sequences. A scrupulous study of all the intricacies of learning a linguistic model, given in [43], led to the creation of a variation of the model called RoBERTa (robustly optimized approach to BERT), which"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 12, "content": "combines the best practices from those analyzed. As a result, it was found that the performance of the original model can be improved by an average of 3-4 percentage points, reaching the best values   to date in all tested problems. Conclusion Immediately after its appearance, the BERT model received an intense reaction from the scientific community and is now used in almost all word processing problems. Almost immediately, the proposals for improving the model considered in this work appeared, which led to an improvement in the results of its application in all subsequent problems. With all that said, we can confidently assert that BERT represented a quantum leap in the field of intelligent natural language processing and consolidated the superiority of using pre-trained text representation models on huge data sets as a universal basis for building intelligent algorithms for solving specific problems. BERT has also shown the advantage of bidirectional contextual models of text"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 12, "content": "comprehension based on the architecture of transformers with an attention mechanism. Undoubtedly, we will see many more new scientific results based on the application and adaptation of the BERT model to various problems of word processing in natural languages. Further improvement of the neural network architecture, coupled with fine-tuning the training procedure and parameters, will inevitably lead to significant improvements in many computer NLP algorithms, from text classification and annotation to machine translation and question-answer systems. References [1] Waleed Ammar, Dirk Groeneveld, Chandra Bhagavatula, Iz Beltagy, Miles Crawford, Doug Downey, Jason"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 13, "content": "Dunkelberger, Ahmed Elgohary, Sergey Feldman, Vu Ha, Rodney Kinney, Sebastian Kohlmeier, Kyle Lo, Tyler- Murray Han Ooi, Matthew Peters, Joanna Power, Sam Skjonsberg, Lucy Wang, Chris Willhelm, Zheng Yuan, Madeleine Zuylen, and oren. 2018. Construction of the Literature Graph in Semantic Scholar. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 3 (Industry Papers) . DOI: https: //doi.org/ 10.18653 / v1 / n18-3011 [2] Waleed Ammar, George Mulcaire, Yulia Tsvetkov, Guillaume Lample, Chris Dyer, and Noah A. Smith. 2016. Massively Multilingual Word Embeddings. (2016). Retrieved March 24, 2020 from https://arxiv.org/pdf/1602.01925.pdf [3] Mikel Artetxe, Gorka Labaka, Eneko Agirre, and Kyunghyun Cho. 2017. Unsupervised Neural Machine Translation. (2017). Retrieved March 24, 2020 from https://arxiv.org/pdf/1710.11041.pdf [4] Vidhya Balasubramanian, Doraisamy Gobu Sooryanarayan, and Navaneeth"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 13, "content": "Kumar Kanakarajan. 2015. A multimodal approach for extracting content descriptive metadata from lecture videos. (2015). Retrieved March 20, 2020 from https://doi.org/10.1007/s10844-015-0356-5 [5] Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and / or Summarization , 65\u201372. Retrieved March 20, 2020 from https://www.aclweb.org/anthology/W05-0909.pdf [6] Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: A Pretrained Language Model for Scientific Text. Retrieved March 21, 2020 from http://arxiv.org/abs/1903.10676 [7] Nicholas Carlini and David Wagner. 2018. Audio Adversarial Examples: Targeted Attacks on Speech-to-Text. Retrieved March 20, 2020 from http://arxiv.org/abs/1801.01944 [8] Richard A. Caruana. 1993. Multitask Learning: A Knowledge-Based Source of Inductive Bias. Machine Learning"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 13, "content": "Proceedings 1993 , 41\u201348. DOI: https: //doi.org/ 10.1016 / b978-1-55860-307-3.50012-5 [9] Daniel Cer, Yinfei Yang, Sheng-Yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil. 2018. Universal Sentence Encoder. Retrieved March 20, 2020 from http://arxiv.org/abs/1803.11175 [10] Arman Cohan, Waleed Ammar, Madeleine van Zuylen, and Field Cady. 2019. Structural Scaffolds for Citation Intent Classification in Scientific Publications. Proceedings of the 2019 Conference of the North . DOI: https: //doi.org/ 10.18653 / v1 / n19-1361 [11] Alexis Conneau and Douwe Kiela. 2018. SentEval: An Evaluation Toolkit for Universal Sentence Representations. (2018). Retrieved March 24, 2020 from https://arxiv.org/pdf/1803.05449.pdf [12] Alexis Conneau, Guillaume Lample, Marc 'aurelio Ranzato, Ludovic Denoyer, and Herv\u00e9 J\u00e9gou. 2017. Word Translation Without Parallel Data. (2017). Retrieved March 24,"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 13, "content": "2020 from https://arxiv.org/pdf/1710.04087.pdf [13] Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. XNLI: Evaluating Cross-lingual Sentence Representations. Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing . DOI: https: //doi.org/ 10.18653 / v1 / d18-1269 [14] Andrew M. Dai and Quoc V. Le. 2015. Semi-supervised Sequence Learning. In Advances in Neural Information Processing Systems , 3079-3087. Retrieved March 20, 2020 from http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning.pdf [15] Zihang Dai, Zhilin Yang, Yiming Yang, William W. Cohen, and Ruslan Salakhutdinov ... 2019. Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context. (January 2019). Retrieved March 24, 2020 from http://dx.doi.org/ [16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of Deep Bidirectional Transformers for Language"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 13, "content": "Understanding. Retrieved March 20, 2020 from http://arxiv.org/abs/1810.04805 [17] Manaal Faruqui and Chris Dyer. 2014. Improving Vector Space Word Representations Using Multilingual Correlation. (2014). Retrieved March 24, 2020 from https://pdfs.semanticscholar.org/8e14/bb86d0b6b28e40b6193a2d0fe80e258751ca.pdf [18] Pierre-Etienne Genest and Guy Lapalme. 2011. Framework for abstractive summarization using text-to-text generation. In Proceedings of the Workshop on Monolingual Text-To-Text Generation , unknown, 64\u201373. Retrieved March 20, 2020 from http://dx.doi.org/ [19] Giuliano Giacaglia. 2019. Transformers. Medium . Retrieved March 19, 2020 from https://towardsdatascience.com/transformers-141e32e69591 [20] James R. Glass, Timothy J. Hazen, D. Scott Cyphers, Igor Malioutov, and Regina Barzilay. 2007. Recent progress in the MIT spoken lecture processing project. In INTERSPEECH 2007, 8th Annual Conference of"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 14, "content": "the International Speech Communication Association, Antwerp, Belgium, August 27-31, 2007 , unknown, 2553-2556. Retrieved March 20, 2020 from http://dx.doi.org/ [21] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014. Explaining and Harnessing Adversarial Examples. (December 2014). Retrieved March 20, 2020 from http://dx.doi.org/ [22] Alex Graves. 2012. Sequence Transduction with Recurrent Neural Networks. Retrieved March 19, 2020 from http://arxiv.org/abs/1211.3711 [23] Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long Short-Term Memory. (1997). Retrieved March 20, 2020 from https://doi.org/10.1162/neco.1997.9.8.1735 [24] Jeremy Howard and Sebastian Ruder. 2018. Universal Language Model Fine-tuning for Text Classification. Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . DOI: https: //doi.org/ 10.18653 / v1 / p18-1031 [25] Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. 2019. Is BERT Really"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 14, "content": "Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment. Retrieved March 20, 2020 from http://arxiv.org/abs/1907.11932 [26] Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Vi\u00e9gas, Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2017. Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation. Transactions of the Association for Computational Linguistics 5 , 339-351. DOI: https: //doi.org/ 10.1162 / tacl_a_00065 [27] Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. 2020. SpanBERT: Improving Pre-training by Representing and Predicting Spans. Transactions of the Association for Computational Linguistics 8 , 64-77. DOI: https: //doi.org/ 10.1162 / tacl_a_00300 [28] David Jurgens, Srijan Kumar, Raine Hoover, Dan McFarland, and Dan Jurafsky. 2018. Measuring the Evolution of a Scientific Field through Citation"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 14, "content": "Frames. Transactions of the Association for Computational Linguistics 6 , 391-406. DOI: https: //doi.org/ 10.1162 / tacl_a_00028 [29] Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. 2014. A Convolutional Neural Network for Modeling Sentences. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , 655\u2013665. DOI: https: //doi.org/ 10.3115 / v1 / P14-1062 [30] Yoon Kim. 2014. Convolutional Neural Networks for Sentence Classification. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) , 1746-1751. DOI: https: //doi.org/ 10.3115 / v1 / D14-1181 [31] Jens Kringelum, Sonny Kim Kjaerulff, S\u00f8ren Brunak, Ole Lund, Tudor I. Oprea, and Olivier Taboureau. 2016. ChemProt-3.0: a global chemical biology diseases mapping. Database 2016, (February 2016). DOI: https: //doi.org/ 10.1093 / database / bav123 [32] Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. 2016. Adversarial examples"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 14, "content": "in the physical world. (2016). Retrieved March 20, 2020 from https://arxiv.org/pdf/1607.02533.pdf [33] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017. RACE: Large-scale ReAding Comprehension Dataset From Examinations. Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing . DOI: https: //doi.org/ 10.18653 / v1 / d17-1082 [34] Guillaume Lample and Alexis Conneau. 2019. Cross-lingual Language Model Pretraining. (2019). Retrieved March 21, 2020 from https://arxiv.org/pdf/1901.07291.pdf [35] Guillaume Lample, Ludovic Denoyer, and Marc 'aurelio Ranzato. 2017. Unsupervised Machine Translation Using Monolingual Corpora Only. (2017). Retrieved March 24, 2020 from https://arxiv.org/pdf/1711.00043.pdf [36] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. 2019. BioBERT: a pre-trained biomedical language representation model for biomedical text mining. DOI: https: //doi.org/ 10.1093 /"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 14, "content": "bioinformatics / btz682 [37] Gregor Leusch, Nicola Ueffing, and Hermann Ney. 2006. CDER: Efficient MT Evaluation Using Block Movements. EACL (2006). Retrieved March 20, 2020 from https://pdfs.semanticscholar.org/aa77/9e4e9f381ad98d66f7f415463542ba74c92d.pdf [38] Bin Liang, Hongcheng Li, Miaoqiang Su, Pan Bian, Xirong Li, and Wenchang Shi. 2017. Deep Text Classification Can be Fooled. DOI: https: //doi.org/ 10.24963 / ijcai.2018 / 585 [39] Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting Wang. 2018. TextBugger: Generating Adversarial Text Against Real-world Applications. (2018). Retrieved March 20, 2020 from https://arxiv.org/pdf/1812.05271.pdf [40] Jiwei Li, Will Monroe, and Dan Jurafsky. 2016. Understanding Neural Networks through Representation Erasure. Retrieved March 20, 2020 from http://arxiv.org/abs/1612.08220 [41] Liyuan Liu, Jingbo Shang, Frank Xu, Xiang Ren, and Jiawei Han. 2017. Empower Sequence Labeling with Task-Aware Neural Language Model. (September 2017). Retrieved"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 14, "content": "March 20, 2020 from http://dx.doi.org/"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 15, "content": "[42] Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2016. Recurrent Neural Network for Text Classification with Multi-Task Learning. (2016). Retrieved March 20, 2020 from https://arxiv.org/pdf/1605.05101.pdf [43] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. (2019). Retrieved March 21, 2020 from https://arxiv.org/pdf/1907.11692.pdf [44] Chi-Kiu Lo. 2017. MEANT 2.0: Accurate semantic MT evaluation for any output language. Proceedings of the Second Conference on Machine Translation . DOI: https: //doi.org/ 10.18653 / v1 / w17-4767 [45] Chi-Kiu Lo. 2019. YiSi - a Unified Semantic MT Quality Evaluation and Estimation Metric for Languages   with Different Levels of Available Resources. Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1) . DOI: https: //doi.org/ 10.18653 / v1 / w19-5358 [46] Chi-"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 15, "content": "Kiu Lo, Michel Simard, Darlene Stewart, Samuel Larkin, Cyril Goutte, and Patrick Littell. 2018. Accurate semantic textual similarity for cleaning noisy parallel corpora using semantic machine translation evaluation metric: The NRC supervised submissions to the Parallel Corpus Filtering task. Proceedings of the Third Conference on Machine Translation: Shared Task Papers . DOI: https: //doi.org/ 10.18653 / v1 / w18-6481 [47] Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh Hajishirzi. 2018. Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction. Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing . DOI: https: //doi.org/ 10.18653 / v1 / d18-1360 [48] Qingsong Ma, Yvette Graham, Shugen Wang, and Qun Liu. 2017. Blend: a Novel Combined MT Metric Based on Direct Assessment - CASICT-DCU submission to WMT17 Metrics Task. In Proceedings of the Second Conference on Machine Translation , unknown, 598-603."}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 15, "content": "DOI: https: //doi.org/ 10.18653 / v1 / W17-4768 [49] Xiaofei Ma, Zhiguo Wang, Patrick Ng, Ramesh Nallapati, and Bing Xiang. 2019. Universal Text Representation from BERT: An Empirical Study. Retrieved March 20, 2020 from http://arxiv.org/abs/1910.07973 [50] Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017. Learned in Translation: Contextualized Word Vectors. In Advances in Neural Information Processing Systems , 6294-6305. Retrieved March 20, 2020 from http://papers.nips.cc/paper/7209-learned-in-translation-contextualized-word-vectors.pdf [51] Michael McCloskey and Neal J. Cohen. 1989. Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem. Psychology of Learning and Motivation , 109-165. DOI: https: //doi.org/ 10.1016 / s0079-7421 (08) 60536-8 [52] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed Representations of Words and Phrases and their Compositionality. Adv. Neural Inf. Process. Syst."}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 15, "content": "26, (October 2013). Retrieved March 20, 2020 from http://dx.doi.org/ [53] Derek Miller. 2019. Leveraging BERT for Extractive Text Summarization on Lectures. (June 2019). Retrieved March 20, 2020 from http://dx.doi.org/ [54] Nikola Mrksic, Diarmuid \u00d3. S\u00e9aghdha, Blaise Thomson, Milica Ga\u0161i\u0107, Lina Maria Rojas-Barahona, Pei-Hao Su, David Vandyke, Tsung-Hsien Wen, and Steve J. Young. 2016. Counter-fitting Word Vectors to Linguistic Constraints. (2016). Retrieved March 20, 2020 from https://arxiv.org/pdf/1603.00892.pdf [55] Gabriel Murray, Steve Renals, and Jean Carletta. 2005. Extractive summarization of meeting recordings. In INTERSPEECH 2005 - Eurospeech, 9th European Conference on Speech Communication and Technology, Lisbon, Portugal, September 4-8, 2005 , unknown, 593-596. Retrieved March 20, 2020 from http://dx.doi.org/ [56] Timothy Niven and Hung-Yu Kao. 2019. Probing Neural Network Comprehension of Natural Language Arguments. Retrieved March 20, 2020 from"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 15, "content": "http://arxiv.org/abs/1907.07355 [57] Benjamin Nye, Junyi Jessy Li, Roma Patel, Yinfei Yang, Iain J. Marshall, Ani Nenkova, and Byron C. Wallace. 2018. A Corpus with Multi-Level Annotations of Patients, Interventions and Outcomes to Support Language Processing for Medical Literature. Proc Conf Assoc Comput Linguist Meet 2018, (July 2018), 197\u2013207. Retrieved from https://www.ncbi.nlm.nih.gov/pubmed/30305770 [58] OpenWebTextCorpus. Download. OpenWebTextCorpus . Retrieved April 7, 2020 from https://skylion007.github.io/OpenWebTextCorpus/ [59] Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. 2018. Scaling Neural Machine Translation. Proceedings of the Third Conference on Machine Translation: Research Papers . DOI: https: //doi.org/ 10.18653 / v1 / w18-6301 [60] Bo Pang and Lillian Lee. 2005. Seeing Stars: Exploiting Class Relationships for Sentiment Categorization with Respect to Rating Scales. In Proceedings of the 43rd Annual Meeting of the Association for Computational"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 15, "content": "Linguistics (ACL'05) , 115-124. DOI: https: //doi.org/ 10.3115 / 1219840.1219855"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 16, "content": "[61] Joybrata Panja and Sudip Kumar Naskar. 2018. ITER: Improving Translation Edit Rate through Optimizable Edit Costs. Proceedings of the Third Conference on Machine Translation: Shared Task Papers . DOI: https: //doi.org/ 10.18653 / v1 / w18-6455 [62] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z. Berkay Celik, and Ananthram Swami. 2017. Practical black-box attacks against machine learning. In ASIA CCS 2017 - Proceedings of the 2017 ACM Asia Conference on Computer and Communications Security , Association for Computing Machinery, Inc, 506-519. DOI: https: //doi.org/ 10.1145 / 3052973.3053009 [63] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. (October 2002). DOI: https: //doi.org/ 10.3115 / 1073083.1073135 [64] Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global Vectors for Word Representation. Proceedings of the 2014 Conference on Empirical Methods in"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 16, "content": "Natural Language Processing (EMNLP) . DOI: https: //doi.org/ 10.3115 / v1 / d14-1162 [65] Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep Contextualized Word Representations. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) . DOI: https: //doi.org/ 10.18653 / v1 / n18-1202 [66] Alec Radford. 2018. Improving Language Understanding with Unsupervised Learning. OpenAI . Retrieved March 20, 2020 from https://openai.com/blog/language-unsupervised/ [67] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ Questions for Machine Comprehension of Text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , 2383-2392. DOI: https: //doi.org/ 10.18653 / v1 / D16-1264 [68] Prajit Ramachandran, Peter J. Liu, and Quoc V. Le. 2016."}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 16, "content": "Unsupervised Pretraining for Sequence to Sequence Learning. Retrieved March 24, 2020 from http://arxiv.org/abs/1611.02683 [69] Marek Rei. 2017. Semi-supervised Multitask Learning for Sequence Labeling. Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . DOI: https: //doi.org/ 10.18653 / v1 / p17-1194 [70] Joao Schapke. 2019. Evolution of word representations in NLP. Medium . Retrieved March 19, 2020 from https://towardsdatascience.com/evolution-of-word-representations-in-nlp-d4483fe23e93 [71] Samuel L. Smith, David HP Turban, Steven Hamblin, and Nils Y. Hammerla. 2017. Offline bilingual word vectors, orthogonal transformations and the inverted softmax. (2017). Retrieved March 24, 2020 from https://arxiv.org/pdf/1702.03859.pdf [72] Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In In Proceedings of Association for"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 16, "content": "Machine Translation in the Americas . Retrieved March 20, 2020 from http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.129.4369 [73] Peter Stanchev, Weiyue Wang, and Hermann Ney. 2019. EED: Extended Edit Distance Measure for Machine Translation. Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1) . DOI: https: //doi.org/ 10.18653 / v1 / w19-5359 [74] Chi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang. 2019. How to Fine-Tune BERT for Text Classification? Lecture Notes in Computer Science , 194-206. DOI: https: //doi.org/ 10.1007 / 978-3-030-32381-3_16 [75] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. 2014. Intriguing properties of neural networks. (2014). Retrieved March 20, 2020 from https://research.google/pubs/pub42503/ [76] Wilson L. Taylor. 1953. \u201cCloze Procedure\u201d: A New Tool for Measuring Readability. Journalism Quarterly 30 , 415-433. DOI: https: //doi.org/ 10.1177"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 16, "content": "/ 107769905303000401 [77] Christoph Tillmann, Stephan Vogel, Hermann Ney, A. Zubiaga, and Hassan Sawaf. 1997. Accelerated DP based search for statistical translation. In the Fifth European Conference on Speech Communication and Technology, EUROSPEECH 1997, Rhodes, Greece, September 22-25, 1997 . Retrieved March 20, 2020 from http://dx.doi.org/ [78] Trieu H. Trinh and Quoc V. Le. 2018. A Simple Method for Commonsense Reasoning. (2018). Retrieved April 7, 2020 from https://arxiv.org/pdf/1806.02847.pdf [79] Nicolas Van Labeke, Denise Whitelock, Debora Field, Stephen Pulman, and John TE Richardson. 2013. What is my essay really saying? Using extractive summarization to motivate reflection and redrafting. In Artificial Intelligence in Education , unknown. Retrieved March 20, 2020 from http://dx.doi.org/ [80] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Advances in Neural"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 16, "content": "Information Processing Systems , 5998-6008. Retrieved March 20, 2020 from http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 17, "content": "[81] Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. 2015. CIDEr: Consensus-based image description evaluation. 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) . DOI: https: //doi.org/ 10.1109 / cvpr.2015.7299087 [82] Takashi Wada, Tomoharu Iwata, and Yuji Matsumoto. 2019. Unsupervised Multilingual Word Embedding with Limited Resources using Neural Language Models. Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics . DOI: https: //doi.org/ 10.18653 / v1 / p19-1300 [83] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP , 353\u2013355. DOI: https: //doi.org/ 10.18653 / v1 / W18-5446 [84] Weiyue Wang, Jan-Thorsten Peter, Hendrik Rosendahl, and Hermann Ney. 2016. CharacTer: Translation Edit"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 17, "content": "Rate on Character Level. Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers . DOI: https: //doi.org/ 10.18653 / v1 / w16-2342 [85] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason, Smith, Jason Riesa Alex Rudnick, Oriol Vinyals, Gregory S. Corrado, Macduff Hughes, and Jeffrey Dean. 2016. Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. (2016). Retrieved March 20, 2020 from https://arxiv.org/pdf/1609.08144.pdf [86] Chao Xing, Dong Wang, Chao Liu, and Yiye Lin. 2015. Normalized Word Embedding and Orthogonal Transform for Bilingual Word Translation. (2015). Retrieved March 24, 2020 from"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 17, "content": "https://pdfs.semanticscholar.org/77e5/76c02792d7df5b102bb81d49df4b5382e1cc.pdf [87] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salocakhutdinov., And Qu. 2019. XLNet: Generalized Autoregressive Pretraining for Language Understanding. (2019). Retrieved March 21, 2020 from https://arxiv.org/pdf/1906.08237.pdf [88] Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard Hovy. 2016. Hierarchical Attention Networks for Document Classification. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , 1480-1489. DOI: https: //doi.org/ 10.18653 / v1 / N16-1174 [89] Wonjin Yoon, Chan Ho So, Jinhyuk Lee, and Jaewoo Kang. 2019. CollaboNet: collaboration of deep neural networks for biomedical named entity recognition. BMC Bioinformatics 20, Suppl 10 (May 2019), 249. DOI: https: //doi.org/ 10.1186 / s12859-019-2813-6 [90] Shanshan Yu, Jindian Su, and Da Luo. 2019."}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 17, "content": "Improving BERT-Based Text Classification With Auxiliary Sentence and Domain Knowledge. IEEE Access 7 , 176600-176612. DOI: https: //doi.org/ 10.1109 / access.2019.2953990 [91] Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference.  \u0418\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u043e 20 \u043c\u0430\u0440\u0442 2020 \u0433. \u043e\u0442 http://arxiv.org/abs/1808.05326 [92] Justin Jian Zhang, Ricky Ho Yin Chan,  \u0438  Pascale Fung. 2007. Improving lecture speech summarization using rhetorical information. (2007  \u0433.). \u0418\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u043e 20 \u043c\u0430\u0440\u0442 2020 \u0433. \u043e\u0442 http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4430108 [93] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger,  \u0438  Yoav Artzi. 2019. BERTScore: Evaluating Text Generation with BERT.  \u0418\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u043e 20 \u043c\u0430\u0440\u0442 2020 \u0433. \u043e\u0442 http://arxiv.org/abs/1904.09675 [94] Xiang Zhang, Junbo Zhao,  \u0438  Yann LeCun. 2015. Character-level convolutional networks for text classification.  \u0412 Advances in Neural Information Processing Systems , Neural"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 17, "content": "information processing systems foundation, 649\u2013657.  \u0418\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u043e 20 \u043c\u0430\u0440\u0442 2020 \u0433. \u043e\u0442 https://nyuscholars.nyu.edu/en/publications/character-level-convolutional-networks-for-text-classification [95] Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M. Meyer,  \u0438  Steffen Eger. 2019. MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) . DOI:https://doi.org/ 10.18653/v1/d19-1053 [96] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba,  \u0438  Sanja Fidler. 2015. Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books. 2015 IEEE International Conference on Computer Vision (ICCV) . DOI:https://doi.org/ 10.1109/iccv.2015.11 [97] Automatic Text Summarization of Video Lectures Using Subtitles."}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 17, "content": "springerprofessional.de . \u0418\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u043e 20 \u043c\u0430\u0440\u0442 2020 \u0433. \u043e\u0442"}, {"filename": "BERT: A Review of Applications in Natural Language Processing and Understanding.pdf", "page_number": 18, "content": "https://www.springerprofessional.de/automatic-text-summarization-of-video-lectures-using-subtitles/1421174 8 [98] https://www.kaggle.com/c/fake-news/data [99] https://datasets.imdbws.com/ [100] https://commoncrawl.org/2016/10/news-dataset-available/"},{"filename": "5.pdf", "page_number": 1, "content": "Published as a conference paper at ICLR 2020 ELECTRA: P RE-TRAINING TEXT ENCODERS ASDISCRIMINATORS RATHER THAN GENERATORS Kevin Clark Stanford University kevclark@cs.stanford.eduMinh-Thang Luong Google Brain thangluong@google.comQuoc V . Le Google Brain qvl@google.com Christopher D. Manning Stanford University & CIFAR Fellow manning@cs.stanford.edu ABSTRACT Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to re- construct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-ef\ufb01cient pre-training task called replaced token detection. Instead of masking the input, our approach cor- rupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original"}, {"filename": "5.pdf", "page_number": 1, "content": "identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more ef- \ufb01cient than MLM because the task is de\ufb01ned over allinput tokens rather than just the small subset that was masked out. As a result, the contextual representa- tions learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural lan- guage understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute. 1 I NTRODUCTION Current state-of-the-art representation learning"}, {"filename": "5.pdf", "page_number": 1, "content": "methods for language can be viewed as learning denoising autoencoders (Vincent et al., 2008). They select a small subset of the unlabeled input sequence (typically 15%), mask the identities of those tokens (e.g., BERT; Devlin et al. (2019)) or attention to those tokens (e.g., XLNet; Yang et al. (2019)), and then train the network to recover the original input. While more effective than conventional language-model pre-training due to learning bidirectional representations, these masked language modeling (MLM) approaches incur a substan- tial compute cost because the network only learns from 15% of the tokens per example. As an alternative, we propose replaced token detection, a pre-training task in which the model learns to distinguish real input tokens from plausible but synthetically generated replacements. Instead of masking, our method corrupts the input by replacing some tokens with samples from a proposal distribution, which is typically the output of a small masked language"}, {"filename": "5.pdf", "page_number": 1, "content": "model. This corruption proce- dure solves a mismatch in BERT (although not in XLNet) where the network sees arti\ufb01cial [MASK] tokens during pre-training but not when being \ufb01ne-tuned on downstream tasks. We then pre-train the network as a discriminator that predicts for every token whether it is an original or a replacement. In contrast, MLM trains the network as a generator that predicts the original identities of the corrupted tokens. A key advantage of our discriminative task is that the model learns from allinput tokens instead of just the small masked-out subset, making it more computationally ef\ufb01cient. Although our 1arXiv:2003.10555v1  [cs.CL]  23 Mar 2020"}, {"filename": "5.pdf", "page_number": 2, "content": "Published as a conference paper at ICLR 2020 0 1 2 3 4 5 6 7 8 Pre-train FLOPs 1e207075808590GLUE ScoreELECTRA-Small GloVeELECTRA-Large 100k steps300k steps GPT400k steps BERT-Base BERT-Small200k steps ELMoELECTRA-Base BERT-LargeRoBERTa 100k stepsXLNet Replaced Token Detection Pre-training Masked Language Model Pre-training 0 1 2 3 4 Pre-train FLOPs 1e217075808590 RoBERTa 300k stepsRoBERTa 500k stepsXLNet Figure 1: Replaced token detection pre-training consistently outperforms masked language model pre-training given the same compute budget. The left \ufb01gure is a zoomed-in view of the dashed box. approach is reminiscent of training the discriminator of a GAN, our method is not adversarial in that the generator producing corrupted tokens is trained with maximum likelihood due to the dif\ufb01culty of applying GANs to text (Caccia et al., 2018). We call our approach ELECTRA1for \u201cEf\ufb01ciently Learning an Encoder that Classi\ufb01es Token Re- placements Accurately.\u201d As in prior work, we apply it to pre-"}, {"filename": "5.pdf", "page_number": 2, "content": "train Transformer text encoders (Vaswani et al., 2017) that can be \ufb01ne-tuned on downstream tasks. Through a series of ablations, we show that learning from all input positions causes ELECTRA to train much faster than BERT. We also show ELECTRA achieves higher accuracy on downstream tasks when fully trained. Most current pre-training methods require large amounts of compute to be effective, raising con- cerns about their cost and accessibility. Since pre-training with more compute almost always re- sults in better downstream accuracies, we argue an important consideration for pre-training methods should be compute ef\ufb01ciency as well as absolute downstream performance. From this viewpoint, we train ELECTRA models of various sizes and evaluate their downstream performance vs. their compute requirement. In particular, we run experiments on the GLUE natural language understand- ing benchmark (Wang et al., 2019) and SQuAD question answering benchmark (Rajpurkar et al., 2016). ELECTRA"}, {"filename": "5.pdf", "page_number": 2, "content": "substantially outperforms MLM-based methods such as BERT and XLNet given the same model size, data, and compute (see Figure 1). For example, we build an ELECTRA-Small model that can be trained on 1 GPU in 4 days.2ELECTRA-Small outperforms a comparably small BERT model by 5 points on GLUE, and even outperforms the much larger GPT model (Radford et al., 2018). Our approach also works well at large scale, where we train an ELECTRA-Large model that performs comparably to RoBERTa (Liu et al., 2019) and XLNet (Yang et al., 2019), de- spite having fewer parameters and using 1/4 of the compute for training. Training ELECTRA-Large further results in an even stronger model that outperforms ALBERT (Lan et al., 2019) on GLUE and sets a new state-of-the-art for SQuAD 2.0. Taken together, our results indicate that the discrim- inative task of distinguishing real data from challenging negative samples is more compute-ef\ufb01cient and parameter-ef\ufb01cient than existing generative approaches for language"}, {"filename": "5.pdf", "page_number": 2, "content": "representation learning. 2 M ETHOD We \ufb01rst describe the replaced token detection pre-training task; see Figure 2 for an overview. We suggest and evaluate several modeling improvements for this method in Section 3.2. 1Code and pre-trained weights will be released at https://github.com/google-research/ electra 2It has 1/20th the parameters and requires 1/135th the pre-training compute of BERT-Large. 2"}, {"filename": "5.pdf", "page_number": 3, "content": "Published as a conference paper at ICLR 2020                 [MASK]  meal chef ate the the meal [MASK]  chef artist artist artist artist  the [MASK]v  artist Generator (typically a small MLM) original  replaced  original  original original Discriminator (ELECTRA) sample sample chef cooked the the meal  Figure 2: An overview of replaced token detection. The generator can be any model that produces an output distribution over tokens, but we usually use a small masked language model that is trained jointly with the discriminator. Although the models are structured like in a GAN, we train the generator with maximum likelihood rather than adversarially due to the dif\ufb01culty of applying GANs to text. After pre-training, we throw out the generator and only \ufb01ne-tune the discriminator (the ELECTRA model) on downstream tasks. Our approach trains two neural networks, a generator Gand a discriminator D. Each one primarily consists of an encoder (e.g., a Transformer network) that maps a sequence on"}, {"filename": "5.pdf", "page_number": 3, "content": "input tokens x= [x1;:::;xn]into a sequence of contextualized vector representations h(x) = [h1;:::;hn]. For a given positiont, (in our case only positions where xt=[MASK] ), the generator outputs a probability for generating a particular token xtwith a softmax layer: pG(xtjx) =exp\u0000 e(xt)ThG(x)t\u0001 =X x0exp\u0000 e(x0)ThG(x)t\u0001 whereedenotes token embeddings. For a given position t, the discriminator predicts whether the tokenxtis \u201creal,\u201d i.e., that it comes from the data rather than the generator distribution, with a sigmoid output layer: D(x;t) =sigmoid (wThD(x)t) The generator is trained to perform masked language modeling (MLM). Given an input x= [x1;x2;:::;xn], MLM \ufb01rst select a random set of positions (integers between 1 and n) to mask outm= [m1;:::;mk].3The tokens in the selected positions are replaced with a [MASK] token: we denote this as xmasked=REPLACE (x;m;[MASK] ). The generator then learns to predict the original identities of the masked-out tokens. The discriminator is trained to"}, {"filename": "5.pdf", "page_number": 3, "content": "distinguish tokens in the data from tokens that have been replaced by generator samples. More speci\ufb01cally, we create a corrupted example xcorruptby replacing the masked-out tokens with generator samples and train the discriminator to predict which tokens in xcorruptmatch the original input x. Formally, model inputs are constructed according to mi\u0018uniff1;ngfori= 1tok xmasked=REPLACE (x;m;[MASK] ) ^xi\u0018pG(xijxmasked)fori2m xcorrupt=REPLACE (x;m;^x) and the loss functions are LMLM(x;\u0012G) =E X i2m\u0000logpG(xijxmasked)! LDisc(x;\u0012D) =E nX t=1\u0000 1(xcorrupt t =xt) logD(xcorrupt;t)\u0000 1(xcorrupt t6=xt) log(1\u0000D(xcorrupt;t))! Although similar to the training objective of a GAN, there are several key differences. First, if the generator happens to generate the correct token, that token is considered \u201creal\u201d instead of \u201cfake\u201d; we found this formulation to moderately improve results on downstream tasks. More importantly, the generator is trained with maximum likelihood rather than being trained adversarially"}, {"filename": "5.pdf", "page_number": 3, "content": "to fool the discriminator. Adversarially training the generator is challenging because it is impossible to back- propagate through sampling from the generator. Although we experimented circumventing this issue 3Typically k=d0:15ne, i.e., 15% of the tokens are masked out. 3"}, {"filename": "5.pdf", "page_number": 4, "content": "Published as a conference paper at ICLR 2020 by using reinforcement learning to train the generator (see Appendix F), this performed worse than maximum-likelihood training. Lastly, we do not supply the generator with a noise vector as input, as is typical with a GAN. We minimize the combined loss min \u0012G;\u0012DX x2XLMLM(x;\u0012G) +\u0015LDisc(x;\u0012D) over a large corpus Xof raw text. We approximate the expectations in the losses with a single sample. We don\u2019t back-propagate the discriminator loss through the generator (indeed, we can\u2019t because of the sampling step). After pre-training, we throw out the generator and \ufb01ne-tune the discriminator on downstream tasks. 3 E XPERIMENTS 3.1 E XPERIMENTAL SETUP We evaluate on the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019) and Stanford Question Answering (SQuAD) dataset (Rajpurkar et al., 2016). GLUE contains a variety of tasks covering textual entailment (RTE and MNLI) question-answer entailment (QNLI), paraphrase (MRPC),"}, {"filename": "5.pdf", "page_number": 4, "content": "question paraphrase (QQP), textual similarity (STS), sentiment (SST), and lin- guistic acceptability (CoLA). See Appendix C for more details on the GLUE tasks. Our evaluation metrics are Spearman correlation for STS, Matthews correlation for CoLA, and accuracy for the other GLUE tasks; we generally report the average score over all tasks. For SQuAD, we evaluate on versions 1.1, in which models select the span of text answering a question, and 2.0, in which some questions are unanswerable by the passage. We use the standard evaluation metrics of Exact-Match (EM) and F1 scores. For most experiments we pre-train on the same data as BERT, which consists of 3.3 Billion tokens from Wikipedia and BooksCorpus (Zhu et al., 2015). However, for our Large model we pre-trained on the data used for XLNet (Yang et al., 2019), which extends the BERT dataset to 33B tokens by including data from ClueWeb (Callan et al., 2009), CommonCrawl, and Gigaword (Parker et al., 2011). All of the pre-training and"}, {"filename": "5.pdf", "page_number": 4, "content": "evaluation is on English data, although we think it would be interesting to apply our methods to multilingual data in the future. Our model architecture and most hyperparameters are the same as BERT\u2019s. For \ufb01ne-tuning on GLUE, we add simple linear classi\ufb01ers on top of ELECTRA. For SQuAD, we add the question- answering module from XLNet on top of ELECTRA, which is slightly more sophisticated than BERT\u2019s in that it jointly rather than independently predicts the start and end positions and has a \u201canswerability\u201d classi\ufb01er added for SQuAD 2.0. Some of our evaluation datasets are small, which means accuracies of \ufb01ne-tuned models can vary substantially depending on the random seed. We therefore report the median of 10 \ufb01ne-tuning runs from the same pre-trained checkpoint for each result. Unless stated otherwise, results are on the dev set. See the appendix for further training details and hyperparameter values. 3.2 M ODEL EXTENSIONS We improve our method by proposing and evaluating several"}, {"filename": "5.pdf", "page_number": 4, "content": "extensions to the model. Unless stated otherwise, these experiments use the same model size and training data as BERT-Base. Weight Sharing We propose improving the ef\ufb01ciency of the pre-training by sharing weights be- tween the generator and discriminator. If the generator and discriminator are the same size, all of the transformer weights can be tied. However, we found it to be more ef\ufb01cient to have a small genera- tor, in which case we only share the embeddings (both the token and positional embeddings) of the generator and discriminator. In this case we use embeddings the size of the discriminator\u2019s hidden states.4The \u201cinput\u201d and \u201coutput\u201d token embeddings of the generator are always tied as in BERT. We compare the weight tying strategies when the generator is the same size as the discriminator. We train these models for 500k steps. GLUE scores are 83.6 for no weight tying, 84.3 for tying token embeddings, and 84.4 for tying all weights. We hypothesize that ELECTRA bene\ufb01ts from 4We"}, {"filename": "5.pdf", "page_number": 4, "content": "add linear layers to the generator to project the embeddings into generator-hidden-sized representations. 4"}, {"filename": "5.pdf", "page_number": 5, "content": "Published as a conference paper at ICLR 2020 unigram 32 64 128 256 512 7681024 Generator Size7879808182838485GLUE Score Which generator size works best? Discriminator Size 768 512 256 0 1 2 3 4 5 6 Pre-Train FLOPs 1e19747678808284GLUE ScoreSwitch to  Discriminator  LossComparison of Training Algorithms ELECTRA Adversarial ELECTRA Two-Stage ELECTRA BERT Figure 3: Left : GLUE scores for different generator/discriminator sizes (number of hidden units). Interestingly, having a generator smaller than the discriminator improves results. Right : Comparison of different training algorithms. As our focus is on ef\ufb01ciency, the x-axis shows FLOPs rather than train steps (e.g., ELECTRA is trained for fewer steps than BERT because it includes the generator). tied token embeddings because masked language modeling is particularly effective at learning these representations: while the discriminator only updates tokens that are present in the input or are sampled by the generator, the generator\u2019s"}, {"filename": "5.pdf", "page_number": 5, "content": "softmax over the vocabulary densely updates all token embeddings. On the other hand, tying all encoder weights caused little improvement while incurring the signi\ufb01cant disadvantage of requiring the generator and discriminator to be the same size. Based on these \ufb01ndings, we use tied embeddings for further experiments in this paper. Smaller Generators If the generator and discriminator are the same size, training ELECTRA would take around twice as much compute per step as training only with masked language mod- eling. We suggest using a smaller generator to reduce this factor. Speci\ufb01cally, we make models smaller by decreasing the layer sizes while keeping the other hyperparameters constant. We also explore using an extremely simple \u201cunigram\u201d generator that samples fake tokens according their frequency in the train corpus. GLUE scores for differently-sized generators and discriminators are shown in the left of Figure 3. All models are trained for 500k steps, which puts the smaller gen-"}, {"filename": "5.pdf", "page_number": 5, "content": "erators at a disadvantage in terms of compute because they require less compute per training step. Nevertheless, we \ufb01nd that models work best with generators 1/4-1/2 the size of the discriminator. We speculate that having too strong of a generator may pose a too-challenging task for the discriminator, preventing it from learning as effectively. In particular, the discriminator may have to use many of its parameters modeling the generator rather than the actual data distribution. Further experiments in this paper use the best generator size found for the given discriminator size. Training Algorithms Lastly, we explore other training algorithms for ELECTRA, although these did not end up improving results. The proposed training objective jointly trains the generator and discriminator. We experiment with instead using the following two-stage training procedure: 1. Train only the generator with LMLM fornsteps. 2. Initialize the weights of the discriminator with the weights of the generator."}, {"filename": "5.pdf", "page_number": 5, "content": "Then train the discriminator with LDiscfornsteps, keeping the generator\u2019s weights frozen. Note that the weight initialization in this procedure requires having the same size for the generator and discriminator. We found that without the weight initialization the discriminator would some- times fail to learn at all beyond the majority class, perhaps because the generator started so far ahead of the discriminator. Joint training on the other hand naturally provides a curriculum for the dis- criminator where the generator starts off weak but gets better throughout training. We also explored training the generator adversarially as in a GAN, using reinforcement learning to accommodate the discrete operations of sampling from the generator. See Appendix F for details. Results are shown in the right of Figure 3. During two-stage training, downstream task performance notably improves after the switch from the generative to the discriminative objective, but does not end up outscoring joint"}, {"filename": "5.pdf", "page_number": 5, "content": "training. Although still outperforming BERT, we found adversarial training to underperform maximum-likelihood training. Further analysis suggests the gap is caused by two 5"}, {"filename": "5.pdf", "page_number": 6, "content": "Published as a conference paper at ICLR 2020 Model Train / Infer FLOPs Speedup Params Train Time + Hardware GLUE ELMo 3.3e18 / 2.6e10 19x / 1.2x 96M 14d on 3 GTX 1080 GPUs 71.2 GPT 4.0e19 / 3.0e10 1.6x / 0.97x 117M 25d on 8 P6000 GPUs 78.8 BERT-Small 1.4e18 / 3.7e9 45x / 8x 14M 4d on 1 V100 GPU 75.1 BERT-Base 6.4e19 / 2.9e10 1x / 1x 110M 4d on 16 TPUv3s 82.2 ELECTRA-Small 1.4e18 / 3.7e9 45x / 8x 14M 4d on 1 V100 GPU 79.9 50% trained 7.1e17 / 3.7e9 90x / 8x 14M 2d on 1 V100 GPU 79.0 25% trained 3.6e17 / 3.7e9 181x / 8x 14M 1d on 1 V100 GPU 77.7 12.5% trained 1.8e17 / 3.7e9 361x / 8x 14M 12h on 1 V100 GPU 76.0 6.25% trained 8.9e16 / 3.7e9 722x / 8x 14M 6h on 1 V100 GPU 74.1 ELECTRA-Base 6.4e19 / 2.9e10 1x / 1x 110M 4d on 16 TPUv3s 85.1 Table 1: Comparison of small models on the GLUE dev set. BERT-Small/Base are our implemen- tation and use the same hyperparameters as ELECTRA-Small/Base. Infer FLOPs assumes single length-128 input. Training times should be taken with a grain of salt as"}, {"filename": "5.pdf", "page_number": 6, "content": "they are for different hard- ware and with sometimes un-optimized code. ELECTRA performs well even when trained on a single GPU, scoring 5 GLUE points higher than a comparable BERT model and even outscoring the much larger GPT model. problems with adversarial training. First, the adversarial generator is simply worse at masked lan- guage modeling; it achieves 58% accuracy at masked language modeling compared to 65% accuracy for an MLE-trained one. We believe the worse accuracy is mainly due to the poor sample ef\ufb01ciency of reinforcement learning when working in the large action space of generating text. Secondly, the adversarially trained generator produces a low-entropy output distribution where most of the proba- bility mass is on a single token, which means there is not much diversity in the generator samples. Both of these problems have been observed in GANs for text in prior work (Caccia et al., 2018). 3.3 S MALL MODELS As a goal of this work is to improve the ef\ufb01ciency of pre-"}, {"filename": "5.pdf", "page_number": 6, "content": "training, we develop a small model that can be quickly trained on a single GPU. Starting with the BERT-Base hyperparameters, we shortened the sequence length (from 512 to 128), reduced the batch size (from 256 to 128), reduced the model\u2019s hidden dimension size (from 768 to 256), and used smaller token embeddings (from 768 to 128). To provide a fair comparison, we also train a BERT-Small model using the same hyperparameters. We train BERT-Small for 1.5M steps, so it uses the same training FLOPs as ELECTRA-Small, which was trained for 1M steps.5In addition to BERT, we compare against two less resource-intensive pre-training methods based on language modeling: ELMo (Peters et al., 2018) and GPT (Radford et al., 2018).6We also show results for a base-sized ELECTRA model comparable to BERT-Base. Results are shown in Table 1. See Appendix D for additional results, including stronger small-sized and base-sized models trained with more compute. ELECTRA-Small performs remarkably well given its"}, {"filename": "5.pdf", "page_number": 6, "content": "size, achieving a higher GLUE score than other methods using substantially more compute and parameters. For example, it scores 5 points higher than a comparable BERT-Small model and even outperforms the much larger GPT model. ELECTRA-Small is trained mostly to convergence, with models trained for even less time (as little as 6 hours) still achieving reasonable performance. While small models distilled from larger pre-trained transformers can also achieve good GLUE scores (Sun et al., 2019b; Jiao et al., 2019), these models require \ufb01rst expending substantial compute to pre-train the larger teacher model. The results also demonstrate the strength of ELECTRA at a moderate size; our base-sized ELECTRA model substantially outperforms BERT-Base and even outperforms BERT-Large (which gets 84.0 GLUE score). We hope ELECTRA\u2019s ability to achieve strong results with relatively little compute will broaden the accessibility of developing and applying pre-trained models in NLP. 5ELECTRA requires"}, {"filename": "5.pdf", "page_number": 6, "content": "more FLOPs per step because it consists of the generator as well as the discriminator. 6GPT is similar in size to BERT-Base, but is trained for fewer steps. 6"}, {"filename": "5.pdf", "page_number": 7, "content": "Published as a conference paper at ICLR 2020 Model Train FLOPs Params CoLA SST MRPC STS QQP MNLI QNLI RTE Avg. BERT 1.9e20 (0.27x) 335M 60.6 93.2 88.0 90.0 91.3 86.6 92.3 70.4 84.0 RoBERTa-100K 6.4e20 (0.90x) 356M 66.1 95.6 91.4 92.2 92.0 89.3 94.0 82.7 87.9 RoBERTa-500K 3.2e21 (4.5x) 356M 68.0 96.4 90.9 92.1 92.2 90.2 94.7 86.6 88.9 XLNet 3.9e21 (5.4x) 360M 69.0 97.0 90.8 92.2 92.3 90.8 94.9 85.9 89.1 BERT (ours) 7.1e20 (1x) 335M 67.0 95.9 89.1 91.2 91.5 89.6 93.5 79.5 87.2 ELECTRA-400K 7.1e20 (1x) 335M 69.3 96.0 90.6 92.1 92.4 90.5 94.5 86.8 89.0 ELECTRA-1.75M 3.1e21 (4.4x) 335M 69.1 96.9 90.8 92.6 92.4 90.9 95.0 88.0 89.5 Table 2: Comparison of large models on the GLUE dev set. ELECTRA and RoBERTa are shown for different numbers of pre-training steps, indicated by the numbers after the dashes. ELECTRA performs comparably to XLNet and RoBERTa when using less than 1/4 of their pre-training compute and outperforms them when given a similar amount of pre-training compute. BERT dev"}, {"filename": "5.pdf", "page_number": 7, "content": "results are from Clark et al. (2019). Model Train FLOPs CoLA SST MRPC STS QQP MNLI QNLI RTE WNLI Avg.* Score BERT 1.9e20 (0.06x) 60.5 94.9 85.4 86.5 89.3 86.7 92.7 70.1 65.1 79.8 80.5 RoBERTa 3.2e21 (1.02x) 67.8 96.7 89.8 91.9 90.2 90.8 95.4 88.2 89.0 88.1 88.1 ALBERT 3.1e22 (10x) 69.1 97.1 91.2 92.0 90.5 91.3 \u2013 89.2 91.8 89.0 \u2013 XLNet 3.9e21 (1.26x) 70.2 97.1 90.5 92.6 90.4 90.9 \u2013 88.5 92.5 89.1 \u2013 ELECTRA 3.1e21 (1x) 71.7 97.1 90.7 92.5 90.8 91.3 95.8 89.8 92.5 89.5 89.4 Table 3: GLUE test-set results for large models. Models in this table incorporate additional tricks such as ensembling to improve scores (see Appendix B for details). Some models do not have QNLI scores because they treat QNLI as a ranking task, which has recently been disallowed by the GLUE benchmark. To compare against these models, we report the average score excluding QNLI (Avg.*) in addition to the GLUE leaderboard score (Score). \u201cELECTRA\u201d and \u201cRoBERTa\u201d refer to the fully-trained ELECTRA-1.75M and RoBERTa-500K"}, {"filename": "5.pdf", "page_number": 7, "content": "models. 3.4 L ARGE MODELS We train big ELECTRA models to measure the effectiveness of the replaced token detection pre- training task at the large scale of current state-of-the-art pre-trained Transformers. Our ELECTRA- Large models are the same size as BERT-Large but are trained for much longer. In particular, we train a model for 400k steps (ELECTRA-400K; roughly 1/4 the pre-training compute of RoBERTa) and one for 1.75M steps (ELECTRA-1.75M; similar compute to RoBERTa). We use a batch size 2048 and the XLNet pre-training data. We note that although the XLNet data is similar to the data used to train RoBERTa, the comparison is not entirely direct. As a baseline, we trained our own BERT-Large model using the same hyperparameters and training time as ELECTRA-400K. Results on the GLUE dev set are shown in Table 2. ELECTRA-400K performs comparably to RoBERTa and XLNet. However, it took less than 1/4 of the compute to train ELECTRA-400K as it did to train RoBERTa and XLNet, demonstrating"}, {"filename": "5.pdf", "page_number": 7, "content": "that ELECTRA\u2019s sample-ef\ufb01ciency gains hold at large scale. Training ELECTRA for longer (ELECTRA-1.75M) results in a model that outscores them on most GLUE tasks while still requiring less pre-training compute. Surprisingly, our baseline BERT model scores notably worse than RoBERTa-100K, suggesting our models may bene\ufb01t from more hyperparameter tuning or using the RoBERTa training data. ELECTRA\u2019s gains hold on the GLUE test set (see Table 3), although these comparisons are less apples-to-apples due to the additional tricks employed by the models (see Appendix B). Results on SQuAD are shown in Table 4. Consistent, with the GLUE results, ELECTRA scores better than masked-language-modeling-based methods given the same compute resources. For ex- ample, ELECTRA-400K outperforms RoBERTa-100k and our BERT baseline, which use similar amounts of pre-training compute. ELECTRA-400K also performs comparably to RoBERTa-500K despite using less than 1/4th of the compute. Unsurprisingly, training"}, {"filename": "5.pdf", "page_number": 7, "content": "ELECTRA longer improves results further: ELECTRA-1.75M scores higher than previous models on the SQuAD 2.0 bench- 7"}, {"filename": "5.pdf", "page_number": 8, "content": "Published as a conference paper at ICLR 2020 Model Train FLOPs ParamsSQuAD 1.1 dev SQuAD 2.0 dev SQuAD 2.0 test EM F1 EM F1 EM F1 BERT-Base 6.4e19 (0.09x) 110M 80.8 88.5 \u2013 \u2013 \u2013 \u2013 BERT 1.9e20 (0.27x) 335M 84.1 90.9 79.0 81.8 80.0 83.0 SpanBERT 7.1e20 (1x) 335M 88.8 94.6 85.7 88.7 85.7 88.7 XLNet-Base 6.6e19 (0.09x) 117M 81.3 \u2013 78.5 \u2013 \u2013 \u2013 XLNet 3.9e21 (5.4x) 360M 89.7 95.1 87.9 90.6 87.9 90.7 RoBERTa-100K 6.4e20 (0.90x) 356M \u2013 94.0 \u2013 87.7 \u2013 \u2013 RoBERTa-500K 3.2e21 (4.5x) 356M 88.9 94.6 86.5 89.4 86.8 89.8 ALBERT 3.1e22 (44x) 235M 89.3 94.8 87.4 90.2 88.1 90.9 BERT (ours) 7.1e20 (1x) 335M 88.0 93.7 84.7 87.5 \u2013 \u2013 ELECTRA-Base 6.4e19 (0.09x) 110M 84.5 90.8 80.5 83.3 \u2013 \u2013 ELECTRA-400K 7.1e20 (1x) 335M 88.7 94.2 86.9 89.6 \u2013 \u2013 ELECTRA-1.75M 3.1e21 (4.4x) 335M 89.7 94.9 88.0 90.6 88.7 91.4 Table 4: Results on the SQuAD for non-ensemble models. mark. ELECTRA-Base also yields strong results, scoring substantially better than BERT-Base and XLNet-Base, and even surpassing BERT-Large according to most"}, {"filename": "5.pdf", "page_number": 8, "content": "metrics. ELECTRA generally performs better at SQuAD 2.0 than 1.1. Perhaps replaced token detection, in which the model distinguishes real tokens from plausible fakes, is particularly transferable to the answerability clas- si\ufb01cation of SQuAD 2.0, in which the model must distinguish answerable questions from fake unan- swerable questions. 3.5 E FFICIENCY ANALYSIS We have suggested that posing the training objective over a small subset of tokens makes masked language modeling inef\ufb01cient. However, it isn\u2019t entirely obvious that this is the case. After all, the model still receives a large number of input tokens even though it predicts only a small number of masked tokens. To better understand where the gains from ELECTRA are coming from, we compare a series of other pre-training objectives that are designed to be a set of \u201cstepping stones\u201d between BERT and ELECTRA. \u000fELECTRA 15% : This model is identical to ELECTRA except the discriminator loss only comes from the 15% of the tokens that"}, {"filename": "5.pdf", "page_number": 8, "content": "were masked out of the input. In other words, the sum in the discriminator loss LDiscis overi2minstead of from 1 to n.7 \u000fReplace MLM : This objective is the same as masked language modeling except instead of replacing masked-out tokens with [MASK] , they are replaced with tokens from a generator model. This objective tests to what extent ELECTRA\u2019s gains come from solving the dis- crepancy of exposing the model to [MASK] tokens during pre-training but not \ufb01ne-tuning. \u000fAll-Tokens MLM : Like in Replace MLM, masked tokens are replaced with generator sam- ples. Furthermore, the model predicts the identity of all tokens in the input, not just ones that were masked out. We found it improved results to train this model with an explicit copy mechanism that outputs a copy probability Dfor each token using a sigmoid layer. The model\u2019s output distribution puts Dweight on the input token plus 1\u0000Dtimes the output of the MLM softmax. This model is essentially a combination of BERT and ELEC- TRA. Note"}, {"filename": "5.pdf", "page_number": 8, "content": "that without generator replacements, the model would trivially learn to make predictions from the vocabulary for [MASK] tokens and copy the input for other ones. Results are shown in Table 5. First, we \ufb01nd that ELECTRA is greatly bene\ufb01ting from having a loss de\ufb01ned over all input tokens rather than just a subset: ELECTRA 15% performs much worse than ELECTRA. Secondly, we \ufb01nd that BERT performance is being slightly harmed from the pre-train \ufb01ne-tune mismatch from [MASK] tokens, as Replace MLM slightly outperforms BERT. We note that BERT (including our implementation) already includes a trick to help with the pre-train/\ufb01ne- tune discrepancy: masked tokens are replaced with a random token 10% of the time and are kept the 7We also trained a discriminator that learns from a random 15% of the input tokens distinct from the subset that was originally masked out; this model performed slightly worse. 8"}, {"filename": "5.pdf", "page_number": 9, "content": "Published as a conference paper at ICLR 2020 Model ELECTRA All-Tokens MLM Replace MLM ELECTRA 15% BERT GLUE score 85.0 84.3 82.4 82.4 82.2 Table 5: Compute-ef\ufb01ciency experiments (see text for details). 128 256 384 512 768 Hidden State Size68707274767880828486GLUE Score ELECTRA BERT 128 256 384 512 768 Hidden State Size01234567ELECTRA improvement over BERT 0 1 2 3 4 5 Pre-Train FLOPs 1e1865707580GLUE Score ELECTRA-256 BERT-256 Figure 4: Left and Center : Comparison of BERT and ELECTRA for different model sizes. Right : A small ELECTRA model converges to higher downstream accuracy than BERT, showing the im- provement comes from more than just faster training. same 10% of the time. However, our results suggest these simple heuristics are insuf\ufb01cient to fully solve the issue. Lastly, we \ufb01nd that All-Tokens MLM, the generative model that makes predictions over all tokens instead of a subset, closes most of the gap between BERT and ELECTRA. In total, these results suggest a large amount of"}, {"filename": "5.pdf", "page_number": 9, "content": "ELECTRA\u2019s improvement can be attributed to learning from all tokens and a smaller amount can be attributed to alleviating the pre-train \ufb01ne-tune mismatch. The improvement of ELECTRA over All-Tokens MLM suggests that the ELECTRA\u2019s gains come from more than just faster training. We study this further by comparing BERT to ELECTRA for various model sizes (see Figure 4, left). We \ufb01nd that the gains from ELECTRA grow larger as the models get smaller. The small models are trained fully to convergence (see Figure 4, right), showing that ELECTRA achieves higher downstream accuracy than BERT when fully trained. We speculate that ELECTRA is more parameter-ef\ufb01cient than BERT because it does not have to model the full distribution of possible tokens at each position, but we believe more analysis is needed to completely explain ELECTRA\u2019s parameter ef\ufb01ciency. 4 R ELATED WORK Self-Supervised Pre-training for NLP Self-supervised learning has been used to learn word rep- resentations (Collobert et al.,"}, {"filename": "5.pdf", "page_number": 9, "content": "2011; Pennington et al., 2014) and more recently contextual represen- tations of words though objectives such as language modeling (Dai & Le, 2015; Peters et al., 2018; Howard & Ruder, 2018). BERT (Devlin et al., 2019) pre-trains a large Transformer (Vaswani et al., 2017) at the masked-language modeling task. There have been numerous extensions to BERT. For example, MASS (Song et al., 2019) and UniLM (Dong et al., 2019) extend BERT to generation tasks by adding auto-regressive generative training objectives. ERNIE (Sun et al., 2019a) and SpanBERT (Joshi et al., 2019) mask out contiguous sequences of token for improved span representations. This idea may be complementary to ELECTRA; we think it would be interesting to make ELECTRA\u2019s generator auto-regressive and add a \u201creplaced span detection\u201d task. Instead of masking out input tokens, XLNet (Yang et al., 2019) masks attention weights such that the input sequence is auto- regressively generated in a random order. However, this method"}, {"filename": "5.pdf", "page_number": 9, "content": "suffers from the same inef\ufb01ciencies as BERT because XLNet only generates 15% of the input tokens in this way. Like ELECTRA, XL- Net may alleviate BERT\u2019s pretrain-\ufb01netune discrepancy by not requiring [MASK] tokens, although this isn\u2019t entirely clear because XLNet uses two \u201cstreams\u201d of attention during pre-training but only one for \ufb01ne-tuning. Recently, models such as TinyBERT (Jiao et al., 2019) and MobileBERT (Sun et al., 2019b) show that BERT can effectively be distilled down to a smaller model. In contrast, we focus more on pre-training speed rather than inference speed, so we train ELECTRA-Small from scratch. 9"}, {"filename": "5.pdf", "page_number": 10, "content": "Published as a conference paper at ICLR 2020 Generative Adversarial Networks GANs (Goodfellow et al., 2014) are effective at generating high-quality synthetic data. Radford et al. (2016) propose using the discriminator of a GAN in downstream tasks, which is similar to our method. GANs have been applied to text data (Yu et al., 2017; Zhang et al., 2017), although state-of-the-art approaches still lag behind standard maximum- likelihood training (Caccia et al., 2018; Tevet et al., 2018). Although we do not use adversarial learning, our generator is particularly reminiscent of MaskGAN (Fedus et al., 2018), which trains the generator to \ufb01ll in tokens deleted from the input. Contrastive Learning Broadly, contrastive learning methods distinguish observed data points from \ufb01ctitious negative samples. They have been applied to many modalities including text (Smith & Eisner, 2005), images (Chopra et al., 2005), and video (Wang & Gupta, 2015; Sermanet et al., 2017) data. Common approaches learn"}, {"filename": "5.pdf", "page_number": 10, "content": "embedding spaces where related data points are similar (Saunshi et al., 2019) or models that rank real data points over negative samples (Collobert et al., 2011; Bordes et al., 2013). ELECTRA is particularly related to Noise-Contrastive Estimation (NCE) (Gutmann & Hyv\u00a8arinen, 2010), which also trains a binary classi\ufb01er to distinguish real and fake data points. Word2Vec (Mikolov et al., 2013), one of the earliest pre-training methods for NLP, uses contrastive learning. In fact, ELECTRA can be viewed as a massively scaled-up version of Continuous Bag- of-Words (CBOW) with Negative Sampling. CBOW also predicts an input token given surrounding context and negative sampling rephrases the learning task as a binary classi\ufb01cation task on whether the input token comes from the data or proposal distribution. However, CBOW uses a bag-of- vectors encoder rather than a transformer and a simple proposal distribution derived from unigram token frequencies instead of a learned generator. 5 C ONCLUSION"}, {"filename": "5.pdf", "page_number": 10, "content": "We have proposed replaced token detection, a new self-supervised task for language representation learning. The key idea is training a text encoder to distinguish input tokens from high-quality nega- tive samples produced by an small generator network. Compared to masked language modeling, our pre-training objective is more compute-ef\ufb01cient and results in better performance on downstream tasks. It works well even when using relatively small amounts of compute, which we hope will make developing and applying pre-trained text encoders more accessible to researchers and practi- tioners with less access to computing resources. We also hope more future work on NLP pre-training will consider ef\ufb01ciency as well as absolute performance, and follow our effort in reporting compute usage and parameter counts along with evaluation metrics. ACKNOWLEDGEMENTS We thank Allen Nie, Prajit Ramachandran, audiences at the CIFAR LMB meeting and U. de Montr \u00b4eal, and the anonymous reviewers for their"}, {"filename": "5.pdf", "page_number": 10, "content": "thoughtful comments and suggestions. We thank Matt Peters for answering our questions about ELMo, Alec Radford for answers about GPT, Naman Goyal and Myle Ott for answers about RoBERTa, Zihang Dai for answers about XLNet, Zhenzhong Lan for answers about ALBERT, and Danqi Chen and Mandar Joshi for answers about SpanBERT. Kevin is supported by a Google PhD Fellowship. REFERENCES Antoine Bordes, Nicolas Usunier, Alberto Garc \u00b4\u0131a-Dur \u00b4an, Jason Weston, and Oksana Yakhnenko. Translating embeddings for modeling multi-relational data. In NeurIPS , 2013. Avishek Joey Bose, Huan Ling, and Yanshuai Cao. Adversarial contrastive estimation. In ACL, 2018. Massimo Caccia, Lucas Caccia, William Fedus, Hugo Larochelle, Joelle Pineau, and Laurent Char- lin. Language GANs falling short. arXiv preprint arXiv:1811.02549 , 2018. Jamie Callan, Mark Hoy, Changkuk Yoo, and Le Zhao. Clueweb09 data set, 2009. URL https: //lemurproject.org/clueweb09.php/ . 10"}, {"filename": "5.pdf", "page_number": 11, "content": "Published as a conference paper at ICLR 2020 Daniel M. Cer, Mona T. Diab, Eneko Agirre, I \u02dcnigo Lopez-Gazpio, and Lucia Specia. Semeval- 2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In SemEval@ACL , 2017. Sumit Chopra, Raia Hadsell, and Yann LeCun. Learning a similarity metric discriminatively, with application to face veri\ufb01cation. CVPR , 2005. Kevin Clark, Minh-Thang Luong, Urvashi Khandelwal, Christopher D. Manning, and Quoc V . Le. BAM! Born-again multi-task networks for natural language understanding. In ACL, 2019. Ronan Collobert, Jason Weston, L \u00b4eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel P. Kuksa. Natural language processing (almost) from scratch. JMLR , 2011. Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In NeurIPS , 2015. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT , 2019. William B."}, {"filename": "5.pdf", "page_number": 11, "content": "Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. InIWP@IJCNLP , 2005. Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. Uni\ufb01ed language model pre-training for natural language understanding and generation. In NeurIPS , 2019. William Fedus, Ian J. Goodfellow, and Andrew M. Dai. MaskGAN: Better text generation via \ufb01lling in the . InICLR , 2018. Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and William B. Dolan. The third pascal recog- nizing textual entailment challenge. In ACL-PASCAL@ACL , 2007. Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS , 2014. Michael Gutmann and Aapo Hyv \u00a8arinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In AISTATS , 2010. Jeremy Howard and Sebastian Ruder. Universal language model"}, {"filename": "5.pdf", "page_number": 11, "content": "\ufb01ne-tuning for text classi\ufb01cation. InACL, 2018. Shankar Iyer, Nikhil Dandekar, and Kornl Csernai. First Quora dataset re- lease: Question pairs, 2017. URL https://data.quora.com/ First-Quora-Dataset-Release-Question-Pairs . Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. Tinybert: Distilling bert for natural language understanding. arXiv preprint arXiv:1909.10351 , 2019. Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy. SpanBERT: Improving pre-training by representing and predicting spans. arXiv preprint arXiv:1907.10529 , 2019. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Sori- cut. ALBERT: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942 , 2019. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized"}, {"filename": "5.pdf", "page_number": 11, "content": "BERT pre- training approach. arXiv preprint arXiv:1907.11692 , 2019. Tomas Mikolov, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. Ef\ufb01cient estimation of word representations in vector space. In ICLR Workshop Papers , 2013. Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. English gigaword, \ufb01fth edition. Technical report, Linguistic Data Consortium, Philadelphia, 2011. 11"}, {"filename": "5.pdf", "page_number": 12, "content": "Published as a conference paper at ICLR 2020 Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word representation. In EMNLP , 2014. Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In NAACL-HLT , 2018. Jason Phang, Thibault F \u00b4evry, and Samuel R Bowman. Sentence encoders on STILTs: Supplemen- tary training on intermediate labeled-data tasks. arXiv preprint arXiv:1811.01088 , 2018. Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In ICLR , 2016. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under- standing by generative pre-training. https://blog.openai.com/language-unsupervised , 2018. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy S. Liang. Squad: 100, 000+ questions for machine comprehension of text."}, {"filename": "5.pdf", "page_number": 12, "content": "In EMNLP , 2016. Nikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail Khodak, and Hrishikesh Khandeparkar. A theoretical analysis of contrastive unsupervised representation learning. In ICML , 2019. Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan Schaal, and Sergey Levine. Time-contrastive networks: Self-supervised learning from video. ICRA , 2017. Noah A. Smith and Jason Eisner. Contrastive estimation: Training log-linear models on unlabeled data. In ACL, 2005. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Y . Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In EMNLP , 2013. Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. MASS: Masked sequence to sequence pre-training for language generation. In ICML , 2019. Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, and Hua Wu. Ernie: Enhanced"}, {"filename": "5.pdf", "page_number": 12, "content": "representation through knowledge integration. arXiv preprint arXiv:1904.09223 , 2019a. Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. Mobile- BERT: Task-agnostic compression of bert for resource limited devices, 2019b. URL https: //openreview.net/forum?id=SJxjVaNKwB . Guy Tevet, Gavriel Habib, Vered Shwartz, and Jonathan Berant. Evaluating text gans as language models. In NAACL-HLT , 2018. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS , 2017. Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In ICML , 2008. Alex Wang, Amapreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In ICLR , 2019. Xiaolong Wang and Abhinav Gupta. Unsupervised"}, {"filename": "5.pdf", "page_number": 12, "content": "learning of visual representations using videos. ICCV , 2015. Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. Neural network acceptability judgments. arXiv preprint arXiv:1805.12471 , 2018. Adina Williams, Nikita Nangia, and Samuel R. Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In NAACL-HLT , 2018. 12"}, {"filename": "5.pdf", "page_number": 13, "content": "Published as a conference paper at ICLR 2020 Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning , 8(3-4):229\u2013256, 1992. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le. XLNet: Generalized autoregressive pretraining for language understanding. In NeurIPS , 2019. Lantao Yu, Weinan Zhang, Jun Wang, and Yingrui Yu. SeqGAN: Sequence generative adversarial nets with policy gradient. In AAAI , 2017. Yizhe Zhang, Zhe Gan, Kai Fan, Zhi Chen, Ricardo Henao, Dinghan Shen, and Lawrence Carin. Adversarial feature matching for text generation. In ICML , 2017. Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Tor- ralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. ICCV , 2015. A P RE-TRAINING DETAILS The following details apply to both our ELECTRA models and BERT"}, {"filename": "5.pdf", "page_number": 13, "content": "baselines. We mostly use the same hyperparameters as BERT. We set \u0015, the weight for the discriminator objective in the loss to 50.8We use dynamic token masking with the masked positions decided on-the-\ufb02y instead of during preprocessing. Also, we did not use the next sentence prediction objective proposed in the original BERT paper, as recent work has suggested it does not improve scores (Yang et al., 2019; Liu et al., 2019). For our ELECTRA-Large model, we used a higher mask percent (25 instead of 15) because we noticed the generator was achieving high accuracy with 15% masking, resulting in very few replaced tokens. We searched for the best learning rate for the Base and Small models out of [1e-4, 2e-4, 3e-4, 5e-4] and selected \u0015out of [1, 10, 20, 50, 100] in early experiments. Otherwise we did no hyperparameter tuning beyond the experiments in Section 3.2. The full set of hyperparameters are listed in Table 6. B F INE-TUNING DETAILS For Large-sized models, we used the hyperparameters"}, {"filename": "5.pdf", "page_number": 13, "content": "from Clark et al. (2019) for the most part. However, after noticing that RoBERTa (Liu et al., 2019) uses more training epochs (up to 10 rather than 3) we searched for the best number of train epochs out of [10, 3] for each task. For SQuAD, we decreased the number of train epochs to 2 to be consistent with BERT and RoBERTa. For Base- sized models we searched for a learning rate out of [3e-5, 5e-5, 1e-4, 1.5e-4] and the layer-wise learning-rate decay out of [0.9, 0.8, 0.7], but otherwise used the same hyperparameters as for Large models. We found the small models bene\ufb01t from a larger learning rate and searched for the best one out of [1e-4, 2e-4, 3e-4, 5e-3]. With the exception of number of train epochs, we used the same hyperparameters for all tasks. In contrast, previous research on GLUE such as BERT, XLNet, and RoBERTa separately searched for the best hyperparameters for each task. We expect our results would improve slightly if we performed the same sort of additional hyperparameter"}, {"filename": "5.pdf", "page_number": 13, "content": "search. The full set of hyperparameters is listed in Table 7. Following BERT, we do not show results on the WNLI GLUE task for the dev set results, as it is dif\ufb01cult to beat even the majority classi\ufb01er using a standard \ufb01ne-tuning-as-classi\ufb01er approach. For the GLUE test set results, we apply the standard tricks used by many of the GLUE leaderboard submissions including RoBERTa (Liu et al., 2019), XLNet (Yang et al., 2019), and ALBERT (Lan et al., 2019). Speci\ufb01cally: \u000fFor RTE and STS we use intermediate task training (Phang et al., 2018), starting from an ELECTRA checkpoint that has been \ufb01ne-tuned on MNLI. For RTE, we found it helpful to combine this with a lower learning rate of 2e-5. 8As a binary classi\ufb01cation task instead of the 30,000-way classi\ufb01cation task in MLM, the discriminator\u2019s loss was typically much lower than the generator\u2019s. 13"}, {"filename": "5.pdf", "page_number": 14, "content": "Published as a conference paper at ICLR 2020 Hyperparameter Small Base Large Number of layers 12 12 24 Hidden Size 256 768 1024 FFN inner hidden size 1024 3072 4096 Attention heads 4 12 16 Attention head size 64 64 64 Embedding Size 128 768 1024 Generator Size (multiplier for hidden-size,1/4 1/3 1/4FFN-size, and num-attention-heads) Mask percent 15 15 25 Learning Rate Decay Linear Linear Linear Warmup steps 10000 10000 10000 Learning Rate 5e-4 2e-4 2e-4 Adam\u000f 1e-6 1e-6 1e-6 Adam 1 0.9 0.9 0.9 Adam 2 0.999 0.999 0.999 Attention Dropout 0.1 0.1 0.1 Dropout 0.1 0.1 0.1 Weight Decay 0.01 0.01 0.01 Batch Size 128 256 2048 Train Steps (BERT/ELECTRA) 1.45M/1M 1M/766K 464K/400K Table 6: Pre-train hyperparameters. We also train an ELECTRA-Large model for 1.75M steps (other hyperparameters are identical). Hyperparameter GLUE Value Learning Rate 3e-4 for Small, 1e-4 for Base, 5e-5 for Large Adam\u000f 1e-6 Adam 1 0.9 Adam 2 0.999 Layerwise LR decay 0.8 for Base/Small, 0.9 for Large Learning rate decay"}, {"filename": "5.pdf", "page_number": 14, "content": "Linear Warmup fraction 0.1 Attention Dropout 0.1 Dropout 0.1 Weight Decay 0 Batch Size 32 Train Epochs 10 for RTE and STS, 2 for SQuAD, 3 for other tasks Table 7: Fine-tune hyperparameters \u000fFor WNLI, we follow the trick described in Liu et al. (2019) where we extract candidate antecedents for the pronoun using rules and train a model to score the correct antecedent highly. However, different from Liu et al. (2019), the scoring function is not based on MLM probabilities. Instead, we \ufb01ne-tune ELECTRA\u2019s discriminator so it assigns high scores to the tokens of the correct antecedent when the correct antecedent replaces the pronoun. For example, if the Winograd schema is \u201cthe trophy could not \ufb01t in the suitcase because it was too big,\u201d we train the discriminator so it gives a high score to \u201ctrophy\u201d in \u201cthe trophy could not \ufb01t in the suitcase because the trophy was too big\u201d but a low score to \u201csuitcase\u201d in \u201cthe trophy could not \ufb01t in the suitcase because the suitcase was too big.\u201d \u000fFor each"}, {"filename": "5.pdf", "page_number": 14, "content": "task we ensemble the best 10 of 30 models \ufb01ne-tuned with different random seeds but initialized from the same pre-trained checkpoint. While these tricks do improve scores, they make having clear scienti\ufb01c comparisons more dif\ufb01cult because they require extra work to implement, require lots of compute, and make results less apples- 14"}, {"filename": "5.pdf", "page_number": 15, "content": "Published as a conference paper at ICLR 2020 to-apples because different papers implement the tricks differently. We therefore also report results for ELECTRA-1.75M with the only trick being dev-set model selection (best of 10 models), which is the setting BERT used to report results, in Table 8. For our SQuAD 2.0 test set submission, we \ufb01ne-tuned 20 models from the same pre-trained check- point and submitted the one with the best dev set score. C D ETAILS ABOUT GLUE We provide further details about the GLUE benchmark tasks below \u000fCoLA: Corpus of Linguistic Acceptability (Warstadt et al., 2018). The task is to determine whether a given sentence is grammatical or not. The dataset contains 8.5k train examples from books and journal articles on linguistic theory. \u000fSST: Stanford Sentiment Treebank (Socher et al., 2013). The tasks is to determine if the sentence is positive or negative in sentiment. The dataset contains 67k train examples from movie reviews. \u000fMRPC: Microsoft Research"}, {"filename": "5.pdf", "page_number": 15, "content": "Paraphrase Corpus (Dolan & Brockett, 2005). The task is to predict whether two sentences are semantically equivalent or not. The dataset contains 3.7k train examples from online news sources. \u000fSTS: Semantic Textual Similarity (Cer et al., 2017). The tasks is to predict how seman- tically similar two sentences are on a 1-5 scale. The dataset contains 5.8k train examples drawn from new headlines, video and image captions, and natural language inference data. \u000fQQP: Quora Question Pairs (Iyer et al., 2017). The task is to determine whether a pair of questions are semantically equivalent. The dataset contains 364k train examples from the community question-answering website Quora. \u000fMNLI: Multi-genre Natural Language Inference (Williams et al., 2018). Given a premise sentence and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis, contradicts the hypothesis, or neither. The dataset contains 393k train examples drawn from ten different sources. \u000fQNLI:"}, {"filename": "5.pdf", "page_number": 15, "content": "Question Natural Language Inference; constructed from SQuAD (Rajpurkar et al., 2016). The task is to predict whether a context sentence contains the answer to a question sentence. The dataset contains 108k train examples from Wikipedia. \u000fRTE: Recognizing Textual Entailment (Giampiccolo et al., 2007). Given a premise sen- tence and a hypothesis sentence, the task is to predict whether the premise entails the hy- pothesis or not. The dataset contains 2.5k train examples from a series of annual textual entailment challenges. D F URTHER RESULTS ON GLUE We report results for ELECTRA-Base and ELECTRA-Small on the GLUE test set in Table 8. Furthermore, we push the limits of base-sized and small-sized models by training them on the XLNet data instead of wikibooks and for much longer (4e6 train steps); these models are called ELECTRA-Base++ and ELECTRA-Small++ in the table. For ELECTRA-Small++ we also in- creased the sequence length to 512; otherwise the hyperparameters are the same as the ones"}, {"filename": "5.pdf", "page_number": 15, "content": "listed in Table 6. Lastly, the table contains results for ELECTRA-1.75M without the tricks described in Appendix B. Consistent with dev-set results in the paper, ELECTRA-Base outperforms BERT-Large while ELECTRA-Small outperforms GPT in terms of average score. Unsurprisingly, the ++ models perform even better. The small model scores are even close to TinyBERT (Jiao et al., 2019) and Mo- bileBERT (Sun et al., 2019b). These models learn from BERT-Base using sophisticated distillation procedures. Our ELECTRA models, on the other hand, are trained from scratch. Given the success of distilling BERT, we believe it would be possible to build even stronger small pre-trained models by distilling ELECTRA. ELECTRA appears to be particularly effective at CoLA. In CoLA the goal is to distinguish linguistically acceptable sentences from ungrammatical ones, which fairly closely matches ELECTRA\u2019s pre-training task of identifying fake tokens, perhaps explaining ELECTRA\u2019s strength at the task. 15"}, {"filename": "5.pdf", "page_number": 16, "content": "Published as a conference paper at ICLR 2020 Model Train FLOPs Params CoLA SST MRPC STS QQP MNLI QNLI RTE Avg. TinyBERT 6.4e19+ (45x+) 14.5M 51.1 93.1 82.6 83.7 89.1 84.6 90.4 70.0 80.6 MobileBERT 6.4e19+ (45x+) 25.3M 51.1 92.6 84.5 84.8 88.3 84.3 91.6 70.4 81.0 GPT 4.0e19 (29x) 117M 45.4 91.3 75.7 80.0 88.5 82.1 88.1 56.0 75.9 BERT-Base 6.4e19 (45x) 110M 52.1 93.5 84.8 85.8 89.2 84.6 90.5 66.4 80.9 BERT-Large 1.9e20 (135x) 335M 60.5 94.9 85.4 86.5 89.3 86.7 92.7 70.1 83.3 SpanBERT 7.1e20 (507x) 335M 64.3 94.8 87.9 89.9 89.5 87.7 94.3 79.0 85.9 ELECTRA-Small 1.4e18 (1x) 14M 54.6 89.1 83.7 80.3 88.0 79.7 87.7 60.8 78.0 ELECTRA-Small++ 3.3e19 (18x) 14M 55.6 91.1 84.9 84.6 88.0 81.6 88.3 63.6 79.7 ELECTRA-Base 6.4e19 (45x) 110M 59.7 93.4 86.7 87.7 89.1 85.8 92.7 73.1 83.5 ELECTRA-Base++ 3.3e20 (182x) 110M 64.6 96.0 88.1 90.2 89.5 88.5 93.1 75.2 85.7 ELECTRA-1.75M 3.1e21 (2200x) 330M 68.1 96.7 89.2 91.7 90.4 90.7 95.5 86.1 88.6 Table 8: Results for models on the GLUE test set. Only models"}, {"filename": "5.pdf", "page_number": 16, "content": "with single-task \ufb01netuning (no ensembling, task-speci\ufb01c tricks, etc.) are shown. E C OUNTING FLOP S We chose to measure compute usage in terms of \ufb02oating point operations (FLOPs) because it is a measure agnostic to the particular hardware, low-level optimizations, etc. However, it is worth not- ing that in some cases abstracting away hardware details is a drawback because hardware-centered optimizations can be key parts of a model\u2019s design, such as the speedup ALBERT (Lan et al., 2019) gets by tying weights and thus reducing communication overhead between TPU workers. We used TensorFlow\u2019s FLOP-counting capabilities9and checked the results with by-hand computation. We made the following assumptions: \u000fAn \u201coperation\u201d is a mathematical operation, not a machine instruction. For example, an exp is one op like an add, even though in practice the exp might be slower. We believe this assumption does not substantially change compute estimates because matrix-multiplies dominate the compute for"}, {"filename": "5.pdf", "page_number": 16, "content": "most models. Similarly, we count matrix-multiplies as 2\u0003m\u0003n FLOPs instead of m\u0003nas one might if considering fused multiply-add operations. \u000fThe backwards pass takes the same number of FLOPs as the forward pass. This assumption is not exactly right (e.g., for softmax cross entropy loss the backward pass is faster), but importantly, the forward/backward pass FLOPs really are the same for matrix-multiplies, which is most of the compute anyway. \u000fWe assume \u201cdense\u201d embedding lookups (i.e., multiplication by a one-hot vector). In prac- tice, sparse embedding lookups are much slower than constant time; on some hardware accelerators dense operations are actually faster than sparse lookups. F A DVERSARIAL TRAINING Here we detail attempts to adversarially train the generator instead of using maximum likelihood. In particular we train the generator Gto maximize the discriminator loss LDisc. As our discriminator isn\u2019t precisely the same as the discriminator of a GAN (see the discussion in Section"}, {"filename": "5.pdf", "page_number": 16, "content": "2), this method is really an instance of Adversarial Contrastive Estimation (Bose et al., 2018) rather than Generative Adversarial Training. It is not possible to adversarially train the generator by back-propagating through the discriminator (e.g., as in a GAN trained on images) due to the discrete sampling from the generator, so we use reinforcement learning instead. Our generator is different from most text generation models in that it is non-autogregressive: predic- tions are made independently. In other words, rather than taking a sequence of actions where each action generates a token, the generator takes a single giant action of generating all tokens simulta- neously, where the probability for the action factorizes as the product of generator probabilities for each token. To deal with this enormous action space, we make the following simplifying assumption: that the discriminator\u2019s prediction D(xcorrupt;t)depends only on the token xtand the non-replaced"}, {"filename": "5.pdf", "page_number": 16, "content": "9Seehttps://www.tensorflow.org/api_docs/python/tf/profiler 16"}, {"filename": "5.pdf", "page_number": 17, "content": "Published as a conference paper at ICLR 2020 tokensfxi:i62mg, i.e., it does not depend on other generated tokens f^xi:i2m^i6=tg. This isn\u2019t too bad of an assumption because a relatively small number of tokens are replaced, and it greatly simpli\ufb01es credit assignment when using reinforcement learning. Notationally, we show this assumption by (in a slight abuse of notation) by writing D(^xtjxmasked)for the discriminator predicting whether the generated token ^xtequals the original token xtgiven the masked context xmasked. A useful consequence of this assumption is that the discriminator score for non-replaced tokens (D(xtjxmasked)fort62m) is independent of pGbecause we are assuming it does not depend on any replaced token. Therefore these tokens can be ignored when training Gto maximizeLDisc. During training we seek to \ufb01nd arg max \u0012GLDisc= arg max \u0012GE x;m;^x nX t=1\u0000 1(xcorrupt t =xt) logD(xcorrupt;t)\u0000 1(xcorrupt t6=xt) log(1\u0000D(xcorrupt;t))! Using the simplifying assumption, we approximate"}, {"filename": "5.pdf", "page_number": 17, "content": "the above by \ufb01nding the argmax of E x;m;^x X t2m\u0000 1(^xt=xt) logD(^xjxmasked)\u0000 1(^xt6=xt) log(1\u0000D(^xjxmasked))! =E x;mX t2mE ^xt\u0018pGR(^xt;x) whereR(^xt;x) =\u001a\u0000logD(^xtjxmasked) if^xt=xt \u0000log(1\u0000D(^xtjxmasked))otherwise In short, the simplifying assumption allows us to decompose the loss over the individual generated tokens. We cannot directly \ufb01nd arg max\u0012Gusing gradient ascent because it is impossible to back- propagate through discrete sampling of ^x. Instead, we use policy gradient reinforcement learning (Williams, 1992). In particular, we use the REINFORCE gradient r\u0012GLDisc\u0019E x;mX t2mE ^xt\u0018pGr\u0012glogpG(^xtjxmasked)[R(^xt;x)\u0000b(xmasked;t)] Wherebis a learned baseline implemented as b(xmasked;t) =\u0000logsigmoid (wThG(xmasked)t) wherehG(xmasked)are the outputs of the generator\u2019s Transformer encoder. The baseline is trained with cross-entropy loss to match the reward for the corresponding position. We approximate the expectations with a single sample and learn \u0012Gwith gradient ascent. Despite"}, {"filename": "5.pdf", "page_number": 17, "content": "receiving no explicit feedback about which generated tokens are correct, we found the adversarial training resulted in a fairly accurate generator (for a 256-hidden-size generator, the adversarially trained one achieves 58% accuracy at masked language modeling while the same sized MLE generator gets 65%). How- ever, using this generator did not improve over the MLE-trained one on downstream tasks (see the right of Figure 3 in the main paper). G E VALUATING ELECTRA AS A MASKED LANGUAGE MODEL This sections details some initial experiments in evaluating ELECTRA as a masked language model. Using slightly different notation from the main paper, given a context cconsisting of a text sequence with one token xmasked-out, the discriminator loss can be written as LDisc=\u0000X x2vocab\u0010 (1\u0000pmask)pdata(xjc) logD(x;c) + //unmasked token pmaskpdata(xjc)pG(xjc) logD(x;c) + //generator samples correct token pmask(1\u0000pdata(xjc))pG(xjc) log(1\u0000D(x;c))\u0011 //generator samples incorrect token Finding the critical"}, {"filename": "5.pdf", "page_number": 17, "content": "points of this loss with respect to Dshows that for a \ufb01xed generator the optimal discriminator is D(x;c) =pdata(xjc)(a+pG(xjc))=(apdata(xjc) +pG(xjc)) 17"}, {"filename": "5.pdf", "page_number": 18, "content": "Published as a conference paper at ICLR 2020 which means pdata(xjc) =D(x;c)pG(xjc)=(a(1\u0000D(x;c)) +pG(xjc)) wherea= (1\u0000pmask)=pmask is the number of unmasked tokens for every masked token. We can use this expression to evaluate ELECTRA as a masked language model by selecting argmaxx2vocabD(x;c)pG(xjc)=(a(1\u0000D(x;c)) +pG(xjc))as the model\u2019s prediction for a given context. In practice, selecting over the whole vocabulary is very expensive, so we instead take the argmax over the top 100 predictions from the generator.10Using this method, we compared ELECTRA-Base and BERT-Base on the Wikipedia+BooksCorpus dataset. We found that BERT slightly outperformed ELECTRA at masked language modeling (77.9% vs 75.5% accuracy). It is possible that the assumption of an optimal discriminator, which is certainly far from correct, is harming ELECTRA\u2019s accuracy under this evaluation scheme. However, perhaps it is not too surpris- ing that a model like BERT that is trained speci\ufb01cally for generation performs"}, {"filename": "5.pdf", "page_number": 18, "content": "better at generation while a model with a discriminative objective like ELECTRA is better at being \ufb01ne-tuned on dis- criminative tasks. We think comparisons of BERT\u2019s and ELECTRA\u2019s MLM predictions might be an interesting way to uncover more about the differences between ELECTRA and BERT encoders in future work. H N EGATIVE RESULTS We brie\ufb02y describe a few ideas that did not look promising in our initial experiments: \u000fWe initially attempted to make BERT more ef\ufb01cient by strategically masking-out tokens (e.g., masking our rarer tokens more frequently, or training a model to guess which tokens BERT would struggle to predict if they were masked out). This resulted in fairly minor speedups over regular BERT. \u000fGiven that ELECTRA seemed to bene\ufb01t (up to a certain point) from having a weaker gener- ator (see Section 3.2), we explored raising the temperature of the generator\u2019s output softmax or disallowing the generator from sampling the correct token. Neither of these improved results. \u000fWe"}, {"filename": "5.pdf", "page_number": 18, "content": "tried adding a sentence-level contrastive objective. For this task, we kept 20% of input sentences unchanged rather than noising them with the generator. We then added a predic- tion head to the model that predicted if the entire input was corrupted or not. Surprisingly, this slightly decreased scores on downstream tasks. 10For ELECTRA-Base, this means the upper-bound for accuracy is around 95%. 18"}]
